{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPRcjfVUh+3HIf/4yOsnAC+"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"cjnSrJGZu3j9","executionInfo":{"status":"ok","timestamp":1710315334925,"user_tz":0,"elapsed":15499,"user":{"displayName":"yvapro","userId":"16744853191845421118"}}},"outputs":[],"source":["import tensorflow as tf\n","from tensorflow.keras import layers\n","from tensorflow.keras.models import Model\n","from transformers import BertTokenizer, TFBertModel\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import MinMaxScaler\n","import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt"]},{"cell_type":"code","source":["########### c'est cette version qu'il faut installer car toute les version que j'installe ici ne marche pas j;ai pris celle qui marche sur mon pc\n","########## Ne pas executer deux fois #############\n","\n","!pip install --upgrade tensorflow==2.12.0\n","!pip install transformers==4.37.1"],"metadata":{"id":"eQJuCpS7vK_o"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["########### c'est cette version qu'il faut installer car toute les version que j'installe ici ne marche pas j;ai pris celle qui marche sur mon pc\n","\n","\n","import tensorflow as tf\n","import transformers\n","\n","print(\"Version de TensorFlow :\", tf.__version__)\n","print(\"Version de Transformers :\", transformers.__version__)"],"metadata":{"id":"1P6GGlYUvL37"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["################ Bon Code ###############\n","#### charger et lire un dataset [.zip] sur coolab ####\n","\n","import zipfile\n","import os\n","import pandas as pd\n","\n","import shutil\n","\n","\n","chemin_zip_heavy_attacks = \"AttacksHeavy.zip\"\n","chemin_zip_heavy_benign = \"BenignHeavy.zip\"\n","\n","\n","chemin_zip_light_attacks = \"AttacksLight.zip\"\n","chemin_zip_light_benign = \"BenignLight.zip\"\n","\n","\n","# Fonction pour extraire les fichiers zip\n","def extraire_zip(chemin_zip):\n","    with zipfile.ZipFile(chemin_zip, 'r') as zip_ref:\n","        zip_ref.extractall(\"extraction_temp\")  # Extraire les fichiers zip dans un répertoire temporaire\n","\n","# Fonction pour charger les fichiers CSV d'un type spécifique (stateful ou stateless)\n","def charger_concatener_donnees(sous_dossier, prefixe):\n","    # Lister tous les fichiers CSV dans le sous-dossier\n","    fichiers_csv = [f for f in os.listdir(f\"extraction_temp/{sous_dossier}\") if f.startswith(prefixe) and f.endswith('.csv')]\n","    # Lire chaque fichier CSV et le stocker dans une liste de DataFrames\n","    dataframes = [pd.read_csv(f\"extraction_temp/{sous_dossier}/{f}\") for f in fichiers_csv]\n","    # Concaténer les DataFrames en un seul\n","    return pd.concat(dataframes, ignore_index=True)\n","\n","# Extraire les fichiers zip\n","extraire_zip(chemin_zip_heavy_attacks)\n","extraire_zip(chemin_zip_heavy_benign)\n","\n","extraire_zip(chemin_zip_light_attacks)\n","extraire_zip(chemin_zip_light_benign)\n","\n","########## Heavy ############\n","\n","# Charger et concaténer les données stateful\n","stateful_heavy_attack_data = charger_concatener_donnees(\"AttacksHeavy\", \"stateful\")\n","stateful_heavy_benign_data = charger_concatener_donnees(\"BenignHeavy\", \"stateful\")\n","\n","# Charger et concaténer les données stateless\n","stateless_heavy_attack_data = charger_concatener_donnees(\"AttacksHeavy\", \"stateless\")\n","stateless_heavy_benign_data = charger_concatener_donnees(\"BenignHeavy\", \"stateless\")\n","\n","\n","#### Light ###############\n","\n","# Charger et concaténer les données stateful\n","stateful_light_attack_data = charger_concatener_donnees(\"AttacksLight\", \"stateful\")\n","stateful_light_benign_data = charger_concatener_donnees(\"BenignLight\", \"stateful\")\n","\n","# Charger et concaténer les données stateless\n","stateless_light_attack_data = charger_concatener_donnees(\"AttacksLight\", \"stateless\")\n","stateless_light_benign_data = charger_concatener_donnees(\"BenignLight\", \"stateless\")\n","\n","\n","\n","# Supprimer le répertoire temporaire après avoir terminé\n","\n","# Vérifier si le répertoire temporaire existe\n","if os.path.exists(\"extraction_temp\"):\n","    # Supprimer le répertoire temporaire et son contenu\n","    shutil.rmtree(\"extraction_temp\")\n","\n","\n","# Maintenant, vous avez vos données prêtes à être utilisées\n"],"metadata":{"id":"OkYuic6_vPd5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","#### Sur-Echantillonage dataset  ####\n","\n","##  Heavy Attack   72,028(stateful)      251,670 (stateless)\n","\n","##  Heavy-Benign      156,014                  402,767\n","\n","### Light Attack      11,295                   42,683\n","\n","### Light-Benign      109,766                  281,164\n","\n","\n","#### Heavy ATTACK #########\n","stateful_heavy_attack_data = stateful_heavy_attack_data.sample(72028, random_state=42)\n","stateful_heavy_benign_data = stateful_heavy_benign_data.sample(156014, random_state=42,replace=True)\n","\n","stateless_heavy_attack_data = stateless_heavy_attack_data.sample(251670, random_state=42)\n","stateless_heavy_benign_data = stateless_heavy_benign_data.sample(402767, random_state=42,replace=True)\n","\n","#### Light ATTACK #########\n","\n","# Charger les données stateful\n","stateful_light_attack_data = stateful_light_attack_data.sample(11295, random_state=42)\n","stateful_light_benign_data = stateful_light_benign_data.sample(109766, random_state=42,replace=True)\n","# Charger les données stateless\n","stateless_light_attack_data = stateless_light_attack_data.sample(42683, random_state=42)\n","stateless_light_benign_data = stateless_light_benign_data.sample(281164, random_state=42,replace=True)\n","\n","############## taille apres re-echantillonage #############\n","print(\"Taille Stateful Heavy attack puis Stateless Heavy attack\")\n","print(stateful_heavy_attack_data.shape)\n","print(stateless_heavy_attack_data.shape)\n","\n","print(\"  \\n\")\n","\n","print(\"Taille Stateful Bengin attack puis Stateless Begnin attack \")\n","print(stateful_heavy_benign_data.shape)\n","print(stateless_heavy_benign_data.shape)\n","\n","print(\"  \\n\")\n","\n","\n","############## taille apres re-echantillonage #############\n","print(\"Taille Stateful Heavy Light puis Stateless Light\")\n","print(stateful_light_attack_data.shape)\n","print(stateless_light_attack_data.shape)\n","\n","print(\"  \\n\")\n","\n","print(\"Taille Stateful Bengin Light puis Stateless Begnin Light\")\n","print(stateful_light_benign_data.shape)\n","print(stateless_light_benign_data.shape)\n","\n","\n","#### concatenation  sur axis = 1 ########\n","\n","print(\" cocncatenantion sur axis = 0 sur les y \\n\")\n","print(\" Heavy attack\")\n","\n","heavy_attack = pd.concat([stateful_heavy_attack_data, stateless_heavy_attack_data], axis=0)\n","print(heavy_attack.shape)\n","\n","#### j'ajoute la classe / label ######\n","\n","heavy_attack['class'] = 'heavy_attacks'\n","print(heavy_attack.shape)\n","\n","print(\" \\n\")\n","print(\" Heavy Bengnin\")\n","\n","heavy_bengin = pd.concat([stateful_heavy_benign_data, stateless_heavy_benign_data], axis=0)\n","print(heavy_bengin.shape)\n","\n","#### j'ajoute la classe / label ######\n","\n","heavy_bengin['class'] = 'heavy_bengnin'\n","print(heavy_bengin.shape)\n","\n","print(\" \\n\")\n","print(\" Light attack\")\n","\n","light_attack = pd.concat([stateful_light_attack_data, stateless_light_attack_data], axis=0)\n","print(light_attack.shape)\n","\n","\n","#### j'ajoute la classe / label ######\n","light_attack['class'] = 'light_attacks'\n","print(light_attack.shape)\n","\n","\n","print(\" \\n\")\n","print(\" Light Bengnin\")\n","\n","light_bengin = pd.concat([stateful_light_benign_data, stateless_light_benign_data], axis=0)\n","print(light_bengin.shape)\n","\n","#### j'ajoute la classe / label ######\n","light_bengin['class'] = 'light_bengnin'\n","print(light_bengin.shape)\n","\n"],"metadata":{"id":"J1gPj6oCvSIZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["################################### CONCATENATION Final des donnee sur axis = 0 ########\n","########################################################################################\n","########################################################################################\n","\n","\n","# 2. Concaténer les données\n","\n","final_data = pd.concat([heavy_attack, heavy_bengin,light_attack,light_bengin], axis=0, ignore_index=True)\n","\n","# 3. Supprimer les colonnes redondantes dans les données catégorielles\n","final_data = final_data.loc[:, ~final_data.columns.duplicated()]\n","\n","# X_numerical = final_data.select_dtypes(include=['int64', 'float64'])\n","\n","# X_categorical = final_data.select_dtypes(exclude='number').drop('class', axis=1)\n","\n","X_numerical = final_data[['rr','A_frequency','FQDN_count','upper','lower','numeric','entropy','special', 'labels', 'labels_max','labels_average','len']]\n","\n","X_categorical = final_data[['rr_type','unique_ttl','timestamp', 'longest_word', 'sld']]\n","\n","y = final_data['class']\n","\n","print(X_numerical.shape)\n","print(X_categorical.shape)\n","\n","print(y.shape)\n","print(final_data.shape)\n","\n"],"metadata":{"id":"_HyGXC_YvX4x"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn.impute import SimpleImputer\n","\n","# Séparer les caractéristiques numériques et catégorielles\n","\n","# Imputation des valeurs manquantes pour les caractéristiques numériques\n","numerical_imputer = SimpleImputer(strategy='mean')\n","X_numerical_imputed = pd.DataFrame(numerical_imputer.fit_transform(X_numerical), columns=X_numerical.columns)\n","\n","\n","from sklearn.impute import SimpleImputer\n","from sklearn.preprocessing import MinMaxScaler\n","\n","# 2. Vérification des colonnes vides\n","if X_numerical.isnull().any().any():\n","    # Imputer les valeurs manquantes pour les caractéristiques numériques\n","    numerical_imputer = SimpleImputer(strategy='mean')\n","    X_numerical_imputed = pd.DataFrame(numerical_imputer.fit_transform(X_numerical), columns=X_numerical.columns)\n","\n","    #prepocessing des features numeriques soit  avec le  LabelEncoder soit le MinMaxScaler()\n","\n","    # Création d'un scaler\n","    scaler = MinMaxScaler()\n","\n","    # Ajustement du scaler aux données\n","    scaler.fit(X_numerical_imputed)\n","\n","    # Transformation des fonctionnalités numériques\n","    scaled_numeric_features = scaler.transform(X_numerical_imputed)\n","\n","    # print(scaled_numeric_features)\n","\n","    # Apres transformation Création d' un DataFrame à partir des valeurs transformées\n","\n","    scaled_df = pd.DataFrame(scaled_numeric_features, columns=X_numerical_imputed.columns)\n","\n","    # Afficher le DataFrame avec les valeurs transformées\n","    # print(\"Après transformation\")\n","    print(scaled_df)\n","\n","    total_size = scaled_df.shape\n","    print(\"Taille totale des caractéristiques numériques après transformations :\", total_size)\n","else:\n","    print(\"Pas de valeurs manquantes dans les caractéristiques numériques. Aucune imputation nécessaire.\")\n","\n","\n"],"metadata":{"id":"6UAVLbdPvYxe"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn.impute import SimpleImputer\n","\n","# Vérification des colonnes vides\n","if X_categorical.isnull().any().any():\n","    # Imputer les valeurs manquantes pour les caractéristiques catégorielles\n","    categorical_imputer = SimpleImputer(strategy='most_frequent')\n","    X_categorical_imputed = pd.DataFrame(categorical_imputer.fit_transform(X_categorical), columns=X_categorical.columns)\n","\n","    # Afficher le DataFrame avec les valeurs imputées\n","    print(\"Après imputation\")\n","    print(X_categorical_imputed)\n","\n","    total_size_categorical = X_categorical_imputed.shape\n","\n","    print(\"Taille totale des caractéristiques catégorielles après imputation :\", total_size_categorical)\n","\n","else:\n","    print(\"Pas de valeurs manquantes dans les caractéristiques catégorielles. Aucune imputation nécessaire.\")\n"],"metadata":{"id":"B5LEZZWsvfNJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from transformers import BertTokenizer\n","\n","# Créer un tokenizer BERT\n","tokenizer = BertTokenizer.from_pretrained('bert-base-uncased-small')\n","\n","# Convertir les données catégorielles imputées en listes\n","categorical_features = [X_categorical_imputed[col].astype(str).tolist() for col in X_categorical_imputed.columns]\n","\n","# Fusionner les textes catégoriels en une seule liste de textes\n","combined_texts = [' '.join([f\"{col_value}\" for col_value in row]) for row in zip(*categorical_features)]\n","\n","# Tokeniser les textes combinés\n","tokens = tokenizer(combined_texts, padding=True, truncation=True, return_tensors='tf', max_length=64)\n","\n","# Afficher les tokens\n","print(tokens)\n"],"metadata":{"id":"mCzkjjHDviAz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Obtenir les longueurs de séquence\n","sequence_lengths = [len(token_ids) for token_ids in tokens['input_ids'].numpy()]\n","\n","# Afficher l'histogramme des longueurs de séquence\n","plt.hist(sequence_lengths, bins=20)\n","plt.title('Histogramme des longueurs de séquence')\n","plt.xlabel('Longueur de séquence')\n","plt.ylabel('Fréquence')\n","plt.show()"],"metadata":{"id":"x84AU_USvn72"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Obtenir la longueur maximale\n","max_sequence_length = max(sequence_lengths)\n","\n","# Obtenir l'indice de la plus longue séquence\n","indice_plus_longue_sequence = sequence_lengths.index(max_sequence_length)\n","\n","# Longueur de la plus longue séquence\n","longueur_plus_longue_sequence = sequence_lengths[indice_plus_longue_sequence]\n","\n","print(f\"Longueur maximale du vecteur : {max_sequence_length}\")\n","print(f\"Longueur de la plus longue séquence : {longueur_plus_longue_sequence}\")\n"],"metadata":{"id":"quLkv0_bvozy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from sklearn.preprocessing import  LabelEncoder\n","\n","# Entrées pour l'entraînement\n","data_num = scaled_df\n","data_text = pad_sequences(tokens['input_ids'].numpy(), maxlen=max_sequence_length, padding='post')\n","\n","# verification les données numériques sont correctes\n","print(\"Shape of data_num:\", data_num.shape)\n","\n","# verification les données textuelles sont correctes après le rembourrage\n","print(\"Shape of data_text:\", data_text.shape)\n","\n","# verification les étiquettes sont correctes\n","print(\"Shape of labels:\", y.shape)\n","\n","# Encodage des étiquettes\n","label_encoder = LabelEncoder()\n","y_encoded = label_encoder.fit_transform(y)\n","\n","print(y_encoded)"],"metadata":{"id":"nyFIvxAqvrmR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["################# UN BERT PETIT ###########################\n","########## POUR EVEITER DE PLANTER MR RAM ##################\n","\n","\n","from transformers import TFBertModel, BertTokenizer\n","from tensorflow.keras import layers, Model\n","from keras_tuner.tuners import BayesianOptimization\n","\n","# Entrées\n","input_num = layers.Input(shape=(scaled_df.shape[1],))\n","input_text = layers.Input(shape=(data_text.shape[1],), dtype=tf.int32)\n","\n","# Branches du modèle\n","# Branche numérique - FFN\n","num_branch = layers.Dense(64, activation='relu')(input_num)\n","num_branch = layers.Dense(32, activation='relu')(input_num)\n","\n","# Branche textuelle - BERT\n","bert_model = TFBertModel.from_pretrained('bert-base-uncased-small')\n","text_branch = bert_model(input_text)['pooler_output']\n","\n","# Fusion des branches\n","merged = layers.concatenate([num_branch, text_branch])\n","\n","# Couches supplémentaires après la fusion\n","merged = layers.Dropout(0.2)(merged)\n","merged = layers.Dense(8, activation='relu')(merged)\n","output = layers.Dense(4, activation='softmax')(merged)\n","\n","# Création et compilation du modèle\n","model = Model(inputs=[input_num, input_text], outputs=output)\n","model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n"],"metadata":{"id":"SBMHMPyPvuLk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(model.summary())"],"metadata":{"id":"6VcBUzwuv-Ct"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn.model_selection import train_test_split\n","\n","# Diviser les données en ensembles d'entraînement et de test\n","text_train, text_test, num_train, num_test, labels_train, labels_test = train_test_split(\n","    data_text,  # Utilisation de data_text\n","    scaled_df,\n","    y_encoded,\n","    test_size=0.2,\n","    random_state=42\n",")\n","\n","\n","########### premier test #############################################################\n","############ Meme sur deux epoch j'ai toujours un probleme de ressource RAM ##########\n","history = model.fit([num_train, text_train], labels_train, epochs=2, batch_size=64, validation_data=([num_test, text_test], labels_test))\n"],"metadata":{"id":"FZvChuJov5qJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install keras-tuner\n"],"metadata":{"id":"f5PJ-mtsv_KJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import numpy as np\n","from sklearn.model_selection import train_test_split\n","from kerastuner.tuners import BayesianOptimization\n","from kerastuner.engine.hyperparameters import HyperParameters\n","from transformers import TFBertModel, BertTokenizer\n","from tensorflow.keras import layers, Model, Input\n","from sklearn.metrics import accuracy_score\n","\n","# Fonction pour construire le modèle\n","def build_model(hp):\n","    input_num = Input(shape=(scaled_df.shape[1],))\n","    input_text = Input(shape=(data_text.shape[1],), dtype=tf.int32)\n","\n","    # Branches du modèle\n","    # Branche numérique - FFN\n","    num_branch = layers.Dense(hp.Choice('num_units', values=[64, 128, 256]), activation='relu')(input_num)\n","    num_branch = layers.Dense(hp.Choice('num_units', values=[32, 64, 128]), activation='relu')(num_branch)\n","\n","    # Branche textuelle - BERT\n","    bert_model = TFBertModel.from_pretrained('bert-base-uncased-small')\n","    text_branch = bert_model(input_text)['pooler_output']\n","\n","    # Fusion des branches\n","    merged = layers.concatenate([num_branch, text_branch])\n","\n","    # Couches supplémentaires après la fusion\n","    merged = layers.Dropout(hp.Choice('dropout', values=[0.3, 0.5]))(merged)\n","    merged = layers.Dense(hp.Choice('dense_units', values=[16, 32, 64]), activation='relu')(merged)\n","\n","    output = layers.Dense(4, activation='softmax')(merged)\n","\n","    model = Model(inputs=[input_num, input_text], outputs=output)\n","\n","    batch_size = hp.Choice('batch_size', values=[16, 32, 64])\n","    model.compile(optimizer=\"adam\", loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n","\n","    return model\n","\n","# Division des données en ensembles d'entraînement et de test\n","text_train, text_test, num_train, num_test, labels_train, labels_test = train_test_split(\n","    data_text,  # Utilisation de data_text\n","    scaled_df,\n","    y_encoded,\n","    test_size=0.2,\n","    random_state=42\n",")\n","\n","# Recherche des meilleurs hyperparamètres\n","tuner = BayesianOptimization(\n","    build_model,\n","    objective='val_accuracy',\n","    max_trials=5,\n","    hyperparameters=HyperParameters(),\n","    directory='./my_dir/',\n","    project_name='bert_ffn_hyperparam_tuning'\n",")\n","\n","tuner.search([num_train, text_train], labels_train, epochs=2, validation_data=([num_test, text_test], labels_test))\n","\n","# Obtention des meilleurs hyperparamètres trouvés\n","best_hps = tuner.oracle.get_best_trials(num_trials=1)[0].hyperparameters\n","print(f\"Meilleurs hyperparamètres: {best_hps}\")\n"],"metadata":{"id":"cVjmqpHOwEbl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["best_hyperparameters = best_hps.get_config()\n","print(\"Best Hyperparameters:\")\n","for key, value in best_hyperparameters.items():\n","    print(f\"{key}: {value}\")"],"metadata":{"id":"ta1EWNAswLBa"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","# Réutiliser les meilleurs hyperparamètres pour construire le modèle final\n","model = build_model(best_hps)\n","\n","# Entraîner le modèle\n","history = model.fit([num_train, text_train], labels_train, epochs=40, batch_size=16, validation_data=([num_test, text_test], labels_test))\n"],"metadata":{"id":"uHOHtHMMwNYB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn.metrics import classification_report\n","\n","# Évaluation du modèle sur les données de test\n","evaluation_results = model.evaluate([num_test, text_test], labels_test)\n","\n","# Impression des résultats d'évaluation\n","print(\"Perte sur les données de test:\", evaluation_results[0])\n","print(\"Précision sur les données de test:\", evaluation_results[1])\n","\n","# Prédiction sur les données de test\n","y_pred = model.predict([num_test, text_test])\n","\n","print(y_pred)\n","\n","# Convertir les indices des classes prédites en étiquettes\n","y_pred_classes = np.argmax(y_pred, axis=1)\n","\n","# Afficher le rapport de classification\n","class_report = classification_report(labels_test, y_pred_classes)\n","print(\"Rapport de classification :\\n\", class_report)\n"],"metadata":{"id":"Qo3q4VjVwONu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn.metrics import accuracy_score\n","\n","# Convertir les indices des classes prédites en étiquettes\n","y_true_classes = np.argmax(labels_test, axis=1) if labels_test.ndim > 1 else labels_test\n","y_pred_classes = np.argmax(y_pred, axis=1) if y_pred.ndim > 1 else y_pred\n","\n","# Calculer l'accuracy pour chaque classe\n","accuracies = []\n","for class_label in range(4):  # Il y a 4 classes numérotées de 0 à 3\n","    y_true_class = (y_true_classes == class_label).astype(int)\n","    y_pred_class = (y_pred_classes == class_label).astype(int)\n","    class_accuracy = accuracy_score(y_true_class, y_pred_class)\n","    accuracies.append(class_accuracy)\n","\n","# Afficher les accuracies pour chaque classe\n","for class_label, accuracy in enumerate(accuracies):\n","    print(f\"Accuracy for class {class_label}: {accuracy}\")\n"],"metadata":{"id":"-S0WIGP8wSwm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Extraction of training metrics\n","training_loss = history.history['loss']\n","training_accuracy = history.history['accuracy']\n","validation_loss = history.history['val_loss']\n","validation_accuracy = history.history['val_accuracy']\n","\n","# Plotting loss and accuracy curves separately\n","epochs = range(1, len(training_loss) + 1)\n","\n","# Plotting training and validation curves\n","plt.figure(figsize=(12, 6))\n","\n","# Plotting training and validation loss\n","plt.subplot(1, 2, 1)\n","plt.plot(epochs, training_loss, label='Training loss')\n","plt.plot(epochs, validation_loss, label='Validation loss')\n","plt.title('Loss Curve')\n","plt.xlabel('Epochs')\n","plt.ylabel('Loss')\n","plt.legend()\n","\n","# Plotting training and validation accuracy\n","plt.subplot(1, 2, 2)\n","plt.plot(epochs, training_accuracy, label='Training accuracy')\n","plt.plot(epochs, validation_accuracy, label='Validation accuracy')\n","plt.title('Accuracy Curve')\n","plt.xlabel('Epochs')\n","plt.ylabel('Accuracy')\n","plt.legend()\n","\n","# Displaying both subplots\n","plt.tight_layout()\n","plt.show()\n"],"metadata":{"id":"h_KIT0-XwVbG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import matplotlib.pyplot as plt\n","import seaborn as sns\n","from sklearn.metrics import confusion_matrix\n","\n","# Matrice de confusion\n","conf_matrix = confusion_matrix(labels_test, y_pred_classes)\n","class_names = ['heavy_attacks', 'heavy_benign', 'light_attacks', 'light_benign']\n","\n","df_cm = pd.DataFrame(conf_matrix, index=class_names, columns=class_names)\n","\n","# Afficher la matrice de confusion avec seaborn\n","plt.figure(figsize=(8, 6))\n","sns.heatmap(df_cm, annot=True, fmt='g', cmap='Blues', cbar=False)\n","plt.title('Matrice de Confusion')\n","plt.xlabel('Prédictions')\n","plt.ylabel('Vraies valeurs')\n","plt.show()\n"],"metadata":{"id":"Z6OvY93uwYEO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Vérifier la forme des tableaux labels_test et y_pred_classes\n","print(\"Shape of labels_test:\", labels_test.shape)\n","print(\"Shape of y_pred_classes:\", y_pred_classes.shape)\n","\n","from sklearn.preprocessing import LabelBinarizer\n","\n","# Binariser labels_test\n","lb = LabelBinarizer()\n","labels_test_binary = lb.fit_transform(labels_test)\n","\n","\n","from sklearn.metrics import roc_curve, auc\n","import matplotlib.pyplot as plt\n","\n","# Calcul des courbes ROC et AUC pour chaque classe\n","fpr = dict()\n","tpr = dict()\n","roc_auc = dict()\n","for i in range(4):\n","    fpr[i], tpr[i], _ = roc_curve(labels_test_binary[:, i], y_pred[:, i])\n","    roc_auc[i] = auc(fpr[i], tpr[i])\n","\n","# Plotter les courbes ROC pour chaque classe\n","plt.figure(figsize=(8, 6))\n","for i in range(4):\n","    plt.plot(fpr[i], tpr[i], lw=2, label='Class %d (AUC = %0.2f)' % (i, roc_auc[i]))\n","\n","plt.plot([0, 1], [0, 1], color='black', lw=2, linestyle='--')\n","plt.xlabel('Taux de faux positifs (FPR)')\n","plt.ylabel('Taux de vrais positifs (TPR)')\n","plt.title('Courbes ROC pour chaque classe')\n","plt.legend(loc=\"lower right\")\n","plt.show()\n"],"metadata":{"id":"VlgpEv9Wwad2"},"execution_count":null,"outputs":[]}]}