{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Two roads diverged in a yellow wood,\n",
      "And sorry I could not travel both\n",
      "And be one traveler, long I stood\n",
      "And looked down one as far as I could\n",
      "To where it bent in the undergrowth;\n",
      "\n",
      "Then took the other, as just as fair,\n",
      "And having perhaps the better claim,\n",
      "Because it was grassy and wanted wear;\n",
      "Though as for that the passing there\n",
      "Had worn them really about the same,\n",
      "\n",
      "And both that morning equally lay\n",
      "In leaves no step had trodden black.\n",
      "Oh, I kept the first for another day!\n",
      "Yet knowing how way leads on to way,\n",
      "I doubted if I should ever come back.\n",
      "\n",
      "I shall be telling this with a sigh\n",
      "Somewhere ages and ages hence:\n",
      "Two roads diverged in a wood, and I—\n",
      "I took the one less traveled by,\n",
      "And that has made all the difference.\n",
      "\n",
      "Whose woods these are I think I know.   \n",
      "His house is in the village though;   \n",
      "He will not see me stopping here   \n",
      "To watch his woods fill up with snow.   \n",
      "\n",
      "My little horse must think it queer   \n",
      "To stop without a farmhouse near   \n",
      "Between the woods and frozen lake   \n",
      "The darkest evening of the year.   \n",
      "\n",
      "He gives his harness bells a shake   \n",
      "To ask if there is some mistake.   \n",
      "The only other sound’s the sweep   \n",
      "Of easy wind and downy flake.   \n",
      "\n",
      "The woods are lovely, dark and deep,   \n",
      "But I have promises to keep,   \n",
      "And miles to go before I sleep,   \n",
      "And miles to go before I sleep.\n",
      "\n",
      "When I see birches bend to left and right\n",
      "Across the lines of straighter darker trees,\n",
      "I like to think some boy's been swinging them.\n",
      "But swinging doesn't bend them down to stay\n",
      "As ice-storms do. Often you must have seen them\n",
      "Loaded with ice a sunny winter morning\n",
      "After a rain. They click upon themselves\n",
      "As the breeze rises, and turn many-colored\n",
      "As the stir cracks and crazes their enamel.\n",
      "Soon the sun's warmth makes them shed crystal shells\n",
      "Shattering and avalanching on the snow-crust—\n",
      "Such heaps of broken glass to sweep away\n",
      "You'd think the inner dome of heaven had fallen.\n",
      "They are dragged to the withered bracken by the load,\n",
      "And they seem not to break; though once they are bowed\n",
      "So low for long, they never right themselves:\n",
      "You may see their trunks arching in the woods\n",
      "Years afterwards, trailing their leaves on the ground\n",
      "Like girls on hands and knees that throw their hair\n",
      "Before them over their heads to dry in the sun.\n",
      "But I was going to say when Truth broke in\n",
      "With all her matter-of-fact about the ice-storm\n",
      "I should prefer to have some boy bend them\n",
      "As he went out and in to fetch the cows—\n",
      "Some boy too far from town to learn baseball,\n",
      "Whose only play was what he found himself,\n",
      "Summer or winter, and could play alone.\n",
      "One by one he subdued his father's trees\n",
      "By riding them down over and over again\n",
      "Until he took the stiffness out of them,\n",
      "And not one but hung limp, not one was left\n",
      "For him to conquer. He learned all there was\n",
      "To learn about not launching out too soon\n",
      "And so not carrying the tree away\n",
      "Clear to the ground. He always kept his poise\n",
      "To the top branches, climbing carefully\n",
      "With the same pains you use to fill a cup\n",
      "Up to the brim, and even above the brim.\n",
      "Then he flung outward, feet first, with a swish,\n",
      "Kicking his way down through the air to the ground.\n",
      "So was I once myself a swinger of birches.\n",
      "And so I dream of going back to be.\n",
      "It's when I'm weary of considerations,\n",
      "And life is too much like a pathless wood\n",
      "Where your face burns and tickles with the cobwebs\n",
      "Broken across it, and one eye is weeping\n",
      "From a twig's having lashed across it open.\n",
      "I'd like to get away from earth awhile\n",
      "And then come back to it and begin over.\n",
      "May no fate willfully misunderstand me\n",
      "And half grant what I wish and snatch me away\n",
      "Not to return. Earth's the right place for love:\n",
      "I don't know where it's likely to go better.\n",
      "I'd like to go by climbing a birch tree,\n",
      "And climb black branches up a snow-white trunk\n",
      "Toward heaven, till the tree could bear no more,\n",
      "But dipped its top and set me down again.\n",
      "That would be good both going and coming back.\n",
      "One could do worse than be a swinger of birches.\n",
      "\n",
      "Before man came to blow it right\n",
      "     The wind once blew itself untaught,\n",
      "And did its loudest day and night\n",
      "     In any rough place where it caught.\n",
      " \n",
      "Man came to tell it what was wrong:\n",
      "     It hadn’t found the place to blow;\n",
      "It blew too hard—the aim was song.\n",
      "     And listen—how it ought to go!\n",
      " \n",
      "He took a little in his mouth,\n",
      "     And held it long enough for north\n",
      "To be converted into south,\n",
      "     And then by measure blew it forth.\n",
      " \n",
      "By measure. It was word and note,\n",
      "     The wind the wind had meant to be—\n",
      "A little through the lips and throat.\n",
      "     The aim was song—the wind could see.\n",
      "\n",
      "The city had withdrawn into itself\n",
      "And left at last the country to the country;\n",
      "When between whirls of snow not come to lie\n",
      "And whirls of foliage not yet laid, there drove\n",
      "A stranger to our yard, who looked the city,\n",
      "Yet did in country fashion in that there\n",
      "He sat and waited till he drew us out\n",
      "A-buttoning coats to ask him who he was.\n",
      "He proved to be the city come again\n",
      "To look for something it had left behind\n",
      "And could not do without and keep its Christmas.\n",
      "He asked if I would sell my Christmas trees;\n",
      "My woods—the young fir balsams like a place\n",
      "Where houses all are churches and have spires.\n",
      "I hadn’t thought of them as Christmas Trees.\n",
      "I doubt if I was tempted for a moment\n",
      "To sell them off their feet to go in cars\n",
      "And leave the slope behind the house all bare,\n",
      "Where the sun shines now no warmer than the moon.\n",
      "I’d hate to have them know it if I was.\n",
      "Yet more I’d hate to hold my trees except\n",
      "As others hold theirs or refuse for them,\n",
      "Beyond the time of profitable growth,\n",
      "The trial by market everything must come to.\n",
      "I dallied so much with the thought of selling.\n",
      "Then whether from mistaken courtesy\n",
      "And fear of seeming short of speech, or whether\n",
      "From hope of hearing good of what was mine, I said,\n",
      "“There aren’t enough to be worth while.”\n",
      "“I could soon tell how many they would cut,\n",
      "You let me look them over.”\n",
      "\n",
      "                                                     “You could look.\n",
      "But don’t expect I’m going to let you have them.”\n",
      "Pasture they spring in, some in clumps too close\n",
      "That lop each other of boughs, but not a few\n",
      "Quite solitary and having equal boughs\n",
      "All round and round. The latter he nodded “Yes” to,\n",
      "Or paused to say beneath some lovelier one,\n",
      "With a buyer’s moderation, “That would do.”\n",
      "I thought so too, but wasn’t there to say so.\n",
      "We climbed the pasture on the south, crossed over,\n",
      "And came down on the north. He said, “A thousand.”\n",
      "\n",
      "“A thousand Christmas trees!—at what apiece?”\n",
      "\n",
      "He felt some need of softening that to me:\n",
      "“A thousand trees would come to thirty dollars.”\n",
      "\n",
      "Then I was certain I had never meant\n",
      "To let him have them. Never show surprise!\n",
      "But thirty dollars seemed so small beside\n",
      "The extent of pasture I should strip, three cents\n",
      "(For that was all they figured out apiece),\n",
      "Three cents so small beside the dollar friends\n",
      "I should be writing to within the hour\n",
      "Would pay in cities for good trees like those,\n",
      "Regular vestry-trees whole Sunday Schools\n",
      "Could hang enough on to pick off enough.\n",
      "A thousand Christmas trees I didn’t know I had!\n",
      "Worth three cents more to give away than sell,\n",
      "As may be shown by a simple calculation.\n",
      "Too bad I couldn’t lay one in a letter.\n",
      "I can’t help wishing I could send you one,\n",
      "In wishing you herewith a Merry Christmas.\n",
      "['two roads diverged in a yellow wood,', 'and sorry i could not travel both', 'and be one traveler, long i stood', 'and looked down one as far as i could', 'to where it bent in the undergrowth;', '', 'then took the other, as just as fair,', 'and having perhaps the better claim,', 'because it was grassy and wanted wear;', 'though as for that the passing there', 'had worn them really about the same,', '', 'and both that morning equally lay', 'in leaves no step had trodden black.', 'oh, i kept the first for another day!', 'yet knowing how way leads on to way,', 'i doubted if i should ever come back.', '', 'i shall be telling this with a sigh', 'somewhere ages and ages hence:', 'two roads diverged in a wood, and i—', 'i took the one less traveled by,', 'and that has made all the difference.', '', 'whose woods these are i think i know.   ', 'his house is in the village though;   ', 'he will not see me stopping here   ', 'to watch his woods fill up with snow.   ', '', 'my little horse must think it queer   ', 'to stop without a farmhouse near   ', 'between the woods and frozen lake   ', 'the darkest evening of the year.   ', '', 'he gives his harness bells a shake   ', 'to ask if there is some mistake.   ', 'the only other sound’s the sweep   ', 'of easy wind and downy flake.   ', '', 'the woods are lovely, dark and deep,   ', 'but i have promises to keep,   ', 'and miles to go before i sleep,   ', 'and miles to go before i sleep.', '', 'when i see birches bend to left and right', 'across the lines of straighter darker trees,', \"i like to think some boy's been swinging them.\", \"but swinging doesn't bend them down to stay\", 'as ice-storms do. often you must have seen them', 'loaded with ice a sunny winter morning', 'after a rain. they click upon themselves', 'as the breeze rises, and turn many-colored', 'as the stir cracks and crazes their enamel.', \"soon the sun's warmth makes them shed crystal shells\", 'shattering and avalanching on the snow-crust—', 'such heaps of broken glass to sweep away', \"you'd think the inner dome of heaven had fallen.\", 'they are dragged to the withered bracken by the load,', 'and they seem not to break; though once they are bowed', 'so low for long, they never right themselves:', 'you may see their trunks arching in the woods', 'years afterwards, trailing their leaves on the ground', 'like girls on hands and knees that throw their hair', 'before them over their heads to dry in the sun.', 'but i was going to say when truth broke in', 'with all her matter-of-fact about the ice-storm', 'i should prefer to have some boy bend them', 'as he went out and in to fetch the cows—', 'some boy too far from town to learn baseball,', 'whose only play was what he found himself,', 'summer or winter, and could play alone.', \"one by one he subdued his father's trees\", 'by riding them down over and over again', 'until he took the stiffness out of them,', 'and not one but hung limp, not one was left', 'for him to conquer. he learned all there was', 'to learn about not launching out too soon', 'and so not carrying the tree away', 'clear to the ground. he always kept his poise', 'to the top branches, climbing carefully', 'with the same pains you use to fill a cup', 'up to the brim, and even above the brim.', 'then he flung outward, feet first, with a swish,', 'kicking his way down through the air to the ground.', 'so was i once myself a swinger of birches.', 'and so i dream of going back to be.', \"it's when i'm weary of considerations,\", 'and life is too much like a pathless wood', 'where your face burns and tickles with the cobwebs', 'broken across it, and one eye is weeping', \"from a twig's having lashed across it open.\", \"i'd like to get away from earth awhile\", 'and then come back to it and begin over.', 'may no fate willfully misunderstand me', 'and half grant what i wish and snatch me away', \"not to return. earth's the right place for love:\", \"i don't know where it's likely to go better.\", \"i'd like to go by climbing a birch tree,\", 'and climb black branches up a snow-white trunk', 'toward heaven, till the tree could bear no more,', 'but dipped its top and set me down again.', 'that would be good both going and coming back.', 'one could do worse than be a swinger of birches.', '', 'before man came to blow it right', '     the wind once blew itself untaught,', 'and did its loudest day and night', '     in any rough place where it caught.', ' ', 'man came to tell it what was wrong:', '     it hadn’t found the place to blow;', 'it blew too hard—the aim was song.', '     and listen—how it ought to go!', ' ', 'he took a little in his mouth,', '     and held it long enough for north', 'to be converted into south,', '     and then by measure blew it forth.', ' ', 'by measure. it was word and note,', '     the wind the wind had meant to be—', 'a little through the lips and throat.', '     the aim was song—the wind could see.', '', 'the city had withdrawn into itself', 'and left at last the country to the country;', 'when between whirls of snow not come to lie', 'and whirls of foliage not yet laid, there drove', 'a stranger to our yard, who looked the city,', 'yet did in country fashion in that there', 'he sat and waited till he drew us out', 'a-buttoning coats to ask him who he was.', 'he proved to be the city come again', 'to look for something it had left behind', 'and could not do without and keep its christmas.', 'he asked if i would sell my christmas trees;', 'my woods—the young fir balsams like a place', 'where houses all are churches and have spires.', 'i hadn’t thought of them as christmas trees.', 'i doubt if i was tempted for a moment', 'to sell them off their feet to go in cars', 'and leave the slope behind the house all bare,', 'where the sun shines now no warmer than the moon.', 'i’d hate to have them know it if i was.', 'yet more i’d hate to hold my trees except', 'as others hold theirs or refuse for them,', 'beyond the time of profitable growth,', 'the trial by market everything must come to.', 'i dallied so much with the thought of selling.', 'then whether from mistaken courtesy', 'and fear of seeming short of speech, or whether', 'from hope of hearing good of what was mine, i said,', '“there aren’t enough to be worth while.”', '“i could soon tell how many they would cut,', 'you let me look them over.”', '', '                                                     “you could look.', 'but don’t expect i’m going to let you have them.”', 'pasture they spring in, some in clumps too close', 'that lop each other of boughs, but not a few', 'quite solitary and having equal boughs', 'all round and round. the latter he nodded “yes” to,', 'or paused to say beneath some lovelier one,', 'with a buyer’s moderation, “that would do.”', 'i thought so too, but wasn’t there to say so.', 'we climbed the pasture on the south, crossed over,', 'and came down on the north. he said, “a thousand.”', '', '“a thousand christmas trees!—at what apiece?”', '', 'he felt some need of softening that to me:', '“a thousand trees would come to thirty dollars.”', '', 'then i was certain i had never meant', 'to let him have them. never show surprise!', 'but thirty dollars seemed so small beside', 'the extent of pasture i should strip, three cents', '(for that was all they figured out apiece),', 'three cents so small beside the dollar friends', 'i should be writing to within the hour', 'would pay in cities for good trees like those,', 'regular vestry-trees whole sunday schools', 'could hang enough on to pick off enough.', 'a thousand christmas trees i didn’t know i had!', 'worth three cents more to give away than sell,', 'as may be shown by a simple calculation.', 'too bad i couldn’t lay one in a letter.', 'i can’t help wishing i could send you one,', 'in wishing you herewith a merry christmas.']\n",
      "\n",
      "two roads diverged in a yellow wood\n",
      "and sorry i could not travel both\n",
      "and be one traveler long i stood\n",
      "and looked down one as far as i could\n",
      "to where it bent in the undergrowth\n",
      "\n",
      "then took the other as just as fair\n",
      "and having perhaps the better claim\n",
      "because it was grassy and wanted wear\n",
      "though as for that the passing there\n",
      "had worn them really about the same\n",
      "\n",
      "and both that morning equally lay\n",
      "in leaves no step had trodden black\n",
      "oh i kept the first for another day!\n",
      "yet knowing how way leads on to way\n",
      "i doubted if i should ever come back\n",
      "\n",
      "i shall be telling this with a sigh\n",
      "somewhere ages and ages hence\n",
      "two roads diverged in a wood and i—\n",
      "i took the one less traveled by\n",
      "and that has made all the difference\n",
      "\n",
      "whose woods these are i think i know   \n",
      "his house is in the village though   \n",
      "he will not see me stopping here   \n",
      "to watch his woods fill up with snow   \n",
      "\n",
      "my little horse must think it queer   \n",
      "to stop without a farmhouse near   \n",
      "between the woods and frozen lake   \n",
      "the darkest evening of the year   \n",
      "\n",
      "he gives his harness bells a shake   \n",
      "to ask if there is some mistake   \n",
      "the only other sounds the sweep   \n",
      "of easy wind and downy flake   \n",
      "\n",
      "the woods are lovely dark and deep   \n",
      "but i have promises to keep   \n",
      "and miles to go before i sleep   \n",
      "and miles to go before i sleep\n",
      "\n",
      "when i see birches bend to left and right\n",
      "across the lines of straighter darker trees\n",
      "i like to think some boys been swinging them\n",
      "but swinging doesnt bend them down to stay\n",
      "as icestorms do often you must have seen them\n",
      "loaded with ice a sunny winter morning\n",
      "after a rain they click upon themselves\n",
      "as the breeze rises and turn manycolored\n",
      "as the stir cracks and crazes their enamel\n",
      "soon the suns warmth makes them shed crystal shells\n",
      "shattering and avalanching on the snowcrust—\n",
      "such heaps of broken glass to sweep away\n",
      "youd think the inner dome of heaven had fallen\n",
      "they are dragged to the withered bracken by the load\n",
      "and they seem not to break though once they are bowed\n",
      "so low for long they never right themselves\n",
      "you may see their trunks arching in the woods\n",
      "years afterwards trailing their leaves on the ground\n",
      "like girls on hands and knees that throw their hair\n",
      "before them over their heads to dry in the sun\n",
      "but i was going to say when truth broke in\n",
      "with all her matteroffact about the icestorm\n",
      "i should prefer to have some boy bend them\n",
      "as he went out and in to fetch the cows—\n",
      "some boy too far from town to learn baseball\n",
      "whose only play was what he found himself\n",
      "summer or winter and could play alone\n",
      "one by one he subdued his fathers trees\n",
      "by riding them down over and over again\n",
      "until he took the stiffness out of them\n",
      "and not one but hung limp not one was left\n",
      "for him to conquer he learned all there was\n",
      "to learn about not launching out too soon\n",
      "and so not carrying the tree away\n",
      "clear to the ground he always kept his poise\n",
      "to the top branches climbing carefully\n",
      "with the same pains you use to fill a cup\n",
      "up to the brim and even above the brim\n",
      "then he flung outward feet first with a swish\n",
      "kicking his way down through the air to the ground\n",
      "so was i once myself a swinger of birches\n",
      "and so i dream of going back to be\n",
      "its when im weary of considerations\n",
      "and life is too much like a pathless wood\n",
      "where your face burns and tickles with the cobwebs\n",
      "broken across it and one eye is weeping\n",
      "from a twigs having lashed across it open\n",
      "id like to get away from earth awhile\n",
      "and then come back to it and begin over\n",
      "may no fate willfully misunderstand me\n",
      "and half grant what i wish and snatch me away\n",
      "not to return earths the right place for love\n",
      "i dont know where its likely to go better\n",
      "id like to go by climbing a birch tree\n",
      "and climb black branches up a snowwhite trunk\n",
      "toward heaven till the tree could bear no more\n",
      "but dipped its top and set me down again\n",
      "that would be good both going and coming back\n",
      "one could do worse than be a swinger of birches\n",
      "\n",
      "before man came to blow it right\n",
      "     the wind once blew itself untaught\n",
      "and did its loudest day and night\n",
      "     in any rough place where it caught\n",
      " \n",
      "man came to tell it what was wrong\n",
      "     it hadnt found the place to blow\n",
      "it blew too hard—the aim was song\n",
      "     and listen—how it ought to go!\n",
      " \n",
      "he took a little in his mouth\n",
      "     and held it long enough for north\n",
      "to be converted into south\n",
      "     and then by measure blew it forth\n",
      " \n",
      "by measure it was word and note\n",
      "     the wind the wind had meant to be—\n",
      "a little through the lips and throat\n",
      "     the aim was song—the wind could see\n",
      "\n",
      "the city had withdrawn into itself\n",
      "and left at last the country to the country\n",
      "when between whirls of snow not come to lie\n",
      "and whirls of foliage not yet laid there drove\n",
      "a stranger to our yard who looked the city\n",
      "yet did in country fashion in that there\n",
      "he sat and waited till he drew us out\n",
      "abuttoning coats to ask him who he was\n",
      "he proved to be the city come again\n",
      "to look for something it had left behind\n",
      "and could not do without and keep its christmas\n",
      "he asked if i would sell my christmas trees\n",
      "my woods—the young fir balsams like a place\n",
      "where houses all are churches and have spires\n",
      "i hadnt thought of them as christmas trees\n",
      "i doubt if i was tempted for a moment\n",
      "to sell them off their feet to go in cars\n",
      "and leave the slope behind the house all bare\n",
      "where the sun shines now no warmer than the moon\n",
      "id hate to have them know it if i was\n",
      "yet more id hate to hold my trees except\n",
      "as others hold theirs or refuse for them\n",
      "beyond the time of profitable growth\n",
      "the trial by market everything must come to\n",
      "i dallied so much with the thought of selling\n",
      "then whether from mistaken courtesy\n",
      "and fear of seeming short of speech or whether\n",
      "from hope of hearing good of what was mine i said\n",
      "there arent enough to be worth while\n",
      "i could soon tell how many they would cut\n",
      "you let me look them over\n",
      "\n",
      "                                                     you could look\n",
      "but dont expect im going to let you have them\n",
      "pasture they spring in some in clumps too close\n",
      "that lop each other of boughs but not a few\n",
      "quite solitary and having equal boughs\n",
      "all round and round the latter he nodded yes to\n",
      "or paused to say beneath some lovelier one\n",
      "with a buyers moderation that would do\n",
      "i thought so too but wasnt there to say so\n",
      "we climbed the pasture on the south crossed over\n",
      "and came down on the north he said a thousand\n",
      "\n",
      "a thousand christmas trees!—at what apiece?\n",
      "\n",
      "he felt some need of softening that to me\n",
      "a thousand trees would come to thirty dollars\n",
      "\n",
      "then i was certain i had never meant\n",
      "to let him have them never show surprise!\n",
      "but thirty dollars seemed so small beside\n",
      "the extent of pasture i should strip three cents\n",
      "for that was all they figured out apiece\n",
      "three cents so small beside the dollar friends\n",
      "i should be writing to within the hour\n",
      "would pay in cities for good trees like those\n",
      "regular vestrytrees whole sunday schools\n",
      "could hang enough on to pick off enough\n",
      "a thousand christmas trees i didnt know i had!\n",
      "worth three cents more to give away than sell\n",
      "as may be shown by a simple calculation\n",
      "too bad i couldnt lay one in a letter\n",
      "i cant help wishing i could send you one\n",
      "in wishing you herewith a merry christmas\n",
      "['', 'two roads diverged in a yellow wood', 'and sorry i could not travel both', 'and be one traveler long i stood', 'and looked down one as far as i could', 'to where it bent in the undergrowth', '', 'then took the other as just as fair', 'and having perhaps the better claim', 'because it was grassy and wanted wear', 'though as for that the passing there', 'had worn them really about the same', '', 'and both that morning equally lay', 'in leaves no step had trodden black', 'oh i kept the first for another day!', 'yet knowing how way leads on to way', 'i doubted if i should ever come back', '', 'i shall be telling this with a sigh', 'somewhere ages and ages hence', 'two roads diverged in a wood and i—', 'i took the one less traveled by', 'and that has made all the difference', '', 'whose woods these are i think i know   ', 'his house is in the village though   ', 'he will not see me stopping here   ', 'to watch his woods fill up with snow   ', '', 'my little horse must think it queer   ', 'to stop without a farmhouse near   ', 'between the woods and frozen lake   ', 'the darkest evening of the year   ', '', 'he gives his harness bells a shake   ', 'to ask if there is some mistake   ', 'the only other sounds the sweep   ', 'of easy wind and downy flake   ', '', 'the woods are lovely dark and deep   ', 'but i have promises to keep   ', 'and miles to go before i sleep   ', 'and miles to go before i sleep', '', 'when i see birches bend to left and right', 'across the lines of straighter darker trees', 'i like to think some boys been swinging them', 'but swinging doesnt bend them down to stay', 'as icestorms do often you must have seen them', 'loaded with ice a sunny winter morning', 'after a rain they click upon themselves', 'as the breeze rises and turn manycolored', 'as the stir cracks and crazes their enamel', 'soon the suns warmth makes them shed crystal shells', 'shattering and avalanching on the snowcrust—', 'such heaps of broken glass to sweep away', 'youd think the inner dome of heaven had fallen', 'they are dragged to the withered bracken by the load', 'and they seem not to break though once they are bowed', 'so low for long they never right themselves', 'you may see their trunks arching in the woods', 'years afterwards trailing their leaves on the ground', 'like girls on hands and knees that throw their hair', 'before them over their heads to dry in the sun', 'but i was going to say when truth broke in', 'with all her matteroffact about the icestorm', 'i should prefer to have some boy bend them', 'as he went out and in to fetch the cows—', 'some boy too far from town to learn baseball', 'whose only play was what he found himself', 'summer or winter and could play alone', 'one by one he subdued his fathers trees', 'by riding them down over and over again', 'until he took the stiffness out of them', 'and not one but hung limp not one was left', 'for him to conquer he learned all there was', 'to learn about not launching out too soon', 'and so not carrying the tree away', 'clear to the ground he always kept his poise', 'to the top branches climbing carefully', 'with the same pains you use to fill a cup', 'up to the brim and even above the brim', 'then he flung outward feet first with a swish', 'kicking his way down through the air to the ground', 'so was i once myself a swinger of birches', 'and so i dream of going back to be', 'its when im weary of considerations', 'and life is too much like a pathless wood', 'where your face burns and tickles with the cobwebs', 'broken across it and one eye is weeping', 'from a twigs having lashed across it open', 'id like to get away from earth awhile', 'and then come back to it and begin over', 'may no fate willfully misunderstand me', 'and half grant what i wish and snatch me away', 'not to return earths the right place for love', 'i dont know where its likely to go better', 'id like to go by climbing a birch tree', 'and climb black branches up a snowwhite trunk', 'toward heaven till the tree could bear no more', 'but dipped its top and set me down again', 'that would be good both going and coming back', 'one could do worse than be a swinger of birches', '', 'before man came to blow it right', '     the wind once blew itself untaught', 'and did its loudest day and night', '     in any rough place where it caught', ' ', 'man came to tell it what was wrong', '     it hadnt found the place to blow', 'it blew too hard—the aim was song', '     and listen—how it ought to go!', ' ', 'he took a little in his mouth', '     and held it long enough for north', 'to be converted into south', '     and then by measure blew it forth', ' ', 'by measure it was word and note', '     the wind the wind had meant to be—', 'a little through the lips and throat', '     the aim was song—the wind could see', '', 'the city had withdrawn into itself', 'and left at last the country to the country', 'when between whirls of snow not come to lie', 'and whirls of foliage not yet laid there drove', 'a stranger to our yard who looked the city', 'yet did in country fashion in that there', 'he sat and waited till he drew us out', 'abuttoning coats to ask him who he was', 'he proved to be the city come again', 'to look for something it had left behind', 'and could not do without and keep its christmas', 'he asked if i would sell my christmas trees', 'my woods—the young fir balsams like a place', 'where houses all are churches and have spires', 'i hadnt thought of them as christmas trees', 'i doubt if i was tempted for a moment', 'to sell them off their feet to go in cars', 'and leave the slope behind the house all bare', 'where the sun shines now no warmer than the moon', 'id hate to have them know it if i was', 'yet more id hate to hold my trees except', 'as others hold theirs or refuse for them', 'beyond the time of profitable growth', 'the trial by market everything must come to', 'i dallied so much with the thought of selling', 'then whether from mistaken courtesy', 'and fear of seeming short of speech or whether', 'from hope of hearing good of what was mine i said', 'there arent enough to be worth while', 'i could soon tell how many they would cut', 'you let me look them over', '', '                                                     you could look', 'but dont expect im going to let you have them', 'pasture they spring in some in clumps too close', 'that lop each other of boughs but not a few', 'quite solitary and having equal boughs', 'all round and round the latter he nodded yes to', 'or paused to say beneath some lovelier one', 'with a buyers moderation that would do', 'i thought so too but wasnt there to say so', 'we climbed the pasture on the south crossed over', 'and came down on the north he said a thousand', '', 'a thousand christmas trees!—at what apiece?', '', 'he felt some need of softening that to me', 'a thousand trees would come to thirty dollars', '', 'then i was certain i had never meant', 'to let him have them never show surprise!', 'but thirty dollars seemed so small beside', 'the extent of pasture i should strip three cents', 'for that was all they figured out apiece', 'three cents so small beside the dollar friends', 'i should be writing to within the hour', 'would pay in cities for good trees like those', 'regular vestrytrees whole sunday schools', 'could hang enough on to pick off enough', 'a thousand christmas trees i didnt know i had!', 'worth three cents more to give away than sell', 'as may be shown by a simple calculation', 'too bad i couldnt lay one in a letter', 'i cant help wishing i could send you one', 'in wishing you herewith a merry christmas']\n",
      "544\n",
      "{'the': 1, 'to': 2, 'and': 3, 'i': 4, 'a': 5, 'of': 6, 'in': 7, 'he': 8, 'it': 9, 'was': 10, 'them': 11, 'not': 12, 'one': 13, 'as': 14, 'could': 15, 'for': 16, 'be': 17, 'that': 18, 'with': 19, 'by': 20, 'but': 21, 'trees': 22, 'so': 23, 'had': 24, 'you': 25, 'they': 26, 'there': 27, 'on': 28, 'all': 29, 'his': 30, 'some': 31, 'have': 32, 'like': 33, 'too': 34, 'down': 35, 'where': 36, 'then': 37, 'come': 38, 'me': 39, 'go': 40, 'their': 41, 'over': 42, 'would': 43, 'christmas': 44, 'if': 45, 'woods': 46, 'are': 47, 'wind': 48, 'away': 49, 'out': 50, 'from': 51, 'what': 52, 'its': 53, 'took': 54, 'no': 55, 'yet': 56, 'should': 57, 'back': 58, 'think': 59, 'know': 60, 'is': 61, 'see': 62, 'my': 63, 'before': 64, 'when': 65, 'left': 66, 'right': 67, 'do': 68, 'going': 69, 'or': 70, 'id': 71, 'place': 72, 'enough': 73, 'thousand': 74, 'wood': 75, 'both': 76, 'long': 77, 'other': 78, 'having': 79, 'though': 80, 'about': 81, 'way': 82, 'up': 83, 'little': 84, 'must': 85, 'birches': 86, 'bend': 87, 'across': 88, 'soon': 89, 'once': 90, 'never': 91, 'may': 92, 'ground': 93, 'say': 94, 'again': 95, 'him': 96, 'tree': 97, 'more': 98, 'good': 99, 'than': 100, 'came': 101, 'blew': 102, 'city': 103, 'country': 104, 'look': 105, 'sell': 106, 'thought': 107, 'let': 108, 'pasture': 109, 'three': 110, 'cents': 111, 'two': 112, 'roads': 113, 'diverged': 114, 'looked': 115, 'far': 116, 'better': 117, 'same': 118, 'morning': 119, 'lay': 120, 'leaves': 121, 'black': 122, 'kept': 123, 'first': 124, 'day': 125, 'how': 126, 'ages': 127, 'whose': 128, 'house': 129, 'fill': 130, 'snow': 131, 'without': 132, 'between': 133, 'ask': 134, 'only': 135, 'sweep': 136, 'keep': 137, 'miles': 138, 'sleep': 139, 'swinging': 140, 'winter': 141, 'themselves': 142, 'broken': 143, 'heaven': 144, 'sun': 145, 'boy': 146, 'learn': 147, 'play': 148, 'found': 149, 'top': 150, 'branches': 151, 'climbing': 152, 'brim': 153, 'feet': 154, 'through': 155, 'swinger': 156, 'im': 157, 'much': 158, 'dont': 159, 'till': 160, 'man': 161, 'blow': 162, 'itself': 163, 'did': 164, 'tell': 165, 'hadnt': 166, 'aim': 167, 'north': 168, 'into': 169, 'south': 170, 'measure': 171, 'meant': 172, 'whirls': 173, 'who': 174, 'behind': 175, 'off': 176, 'hate': 177, 'hold': 178, 'whether': 179, 'said': 180, 'worth': 181, 'boughs': 182, 'round': 183, 'apiece': 184, 'thirty': 185, 'dollars': 186, 'small': 187, 'beside': 188, 'wishing': 189, 'yellow': 190, 'sorry': 191, 'travel': 192, 'traveler': 193, 'stood': 194, 'bent': 195, 'undergrowth': 196, 'just': 197, 'fair': 198, 'perhaps': 199, 'claim': 200, 'because': 201, 'grassy': 202, 'wanted': 203, 'wear': 204, 'passing': 205, 'worn': 206, 'really': 207, 'equally': 208, 'step': 209, 'trodden': 210, 'oh': 211, 'another': 212, 'knowing': 213, 'leads': 214, 'doubted': 215, 'ever': 216, 'shall': 217, 'telling': 218, 'this': 219, 'sigh': 220, 'somewhere': 221, 'hence': 222, 'i—': 223, 'less': 224, 'traveled': 225, 'has': 226, 'made': 227, 'difference': 228, 'these': 229, 'village': 230, 'will': 231, 'stopping': 232, 'here': 233, 'watch': 234, 'horse': 235, 'queer': 236, 'stop': 237, 'farmhouse': 238, 'near': 239, 'frozen': 240, 'lake': 241, 'darkest': 242, 'evening': 243, 'year': 244, 'gives': 245, 'harness': 246, 'bells': 247, 'shake': 248, 'mistake': 249, 'sounds': 250, 'easy': 251, 'downy': 252, 'flake': 253, 'lovely': 254, 'dark': 255, 'deep': 256, 'promises': 257, 'lines': 258, 'straighter': 259, 'darker': 260, 'boys': 261, 'been': 262, 'doesnt': 263, 'stay': 264, 'icestorms': 265, 'often': 266, 'seen': 267, 'loaded': 268, 'ice': 269, 'sunny': 270, 'after': 271, 'rain': 272, 'click': 273, 'upon': 274, 'breeze': 275, 'rises': 276, 'turn': 277, 'manycolored': 278, 'stir': 279, 'cracks': 280, 'crazes': 281, 'enamel': 282, 'suns': 283, 'warmth': 284, 'makes': 285, 'shed': 286, 'crystal': 287, 'shells': 288, 'shattering': 289, 'avalanching': 290, 'snowcrust—': 291, 'such': 292, 'heaps': 293, 'glass': 294, 'youd': 295, 'inner': 296, 'dome': 297, 'fallen': 298, 'dragged': 299, 'withered': 300, 'bracken': 301, 'load': 302, 'seem': 303, 'break': 304, 'bowed': 305, 'low': 306, 'trunks': 307, 'arching': 308, 'years': 309, 'afterwards': 310, 'trailing': 311, 'girls': 312, 'hands': 313, 'knees': 314, 'throw': 315, 'hair': 316, 'heads': 317, 'dry': 318, 'truth': 319, 'broke': 320, 'her': 321, 'matteroffact': 322, 'icestorm': 323, 'prefer': 324, 'went': 325, 'fetch': 326, 'cows—': 327, 'town': 328, 'baseball': 329, 'himself': 330, 'summer': 331, 'alone': 332, 'subdued': 333, 'fathers': 334, 'riding': 335, 'until': 336, 'stiffness': 337, 'hung': 338, 'limp': 339, 'conquer': 340, 'learned': 341, 'launching': 342, 'carrying': 343, 'clear': 344, 'always': 345, 'poise': 346, 'carefully': 347, 'pains': 348, 'use': 349, 'cup': 350, 'even': 351, 'above': 352, 'flung': 353, 'outward': 354, 'swish': 355, 'kicking': 356, 'air': 357, 'myself': 358, 'dream': 359, 'weary': 360, 'considerations': 361, 'life': 362, 'pathless': 363, 'your': 364, 'face': 365, 'burns': 366, 'tickles': 367, 'cobwebs': 368, 'eye': 369, 'weeping': 370, 'twigs': 371, 'lashed': 372, 'open': 373, 'get': 374, 'earth': 375, 'awhile': 376, 'begin': 377, 'fate': 378, 'willfully': 379, 'misunderstand': 380, 'half': 381, 'grant': 382, 'wish': 383, 'snatch': 384, 'return': 385, 'earths': 386, 'love': 387, 'likely': 388, 'birch': 389, 'climb': 390, 'snowwhite': 391, 'trunk': 392, 'toward': 393, 'bear': 394, 'dipped': 395, 'set': 396, 'coming': 397, 'worse': 398, 'untaught': 399, 'loudest': 400, 'night': 401, 'any': 402, 'rough': 403, 'caught': 404, 'wrong': 405, 'hard—the': 406, 'song': 407, 'listen—how': 408, 'ought': 409, 'mouth': 410, 'held': 411, 'converted': 412, 'forth': 413, 'word': 414, 'note': 415, 'be—': 416, 'lips': 417, 'throat': 418, 'song—the': 419, 'withdrawn': 420, 'at': 421, 'last': 422, 'lie': 423, 'foliage': 424, 'laid': 425, 'drove': 426, 'stranger': 427, 'our': 428, 'yard': 429, 'fashion': 430, 'sat': 431, 'waited': 432, 'drew': 433, 'us': 434, 'abuttoning': 435, 'coats': 436, 'proved': 437, 'something': 438, 'asked': 439, 'woods—the': 440, 'young': 441, 'fir': 442, 'balsams': 443, 'houses': 444, 'churches': 445, 'spires': 446, 'doubt': 447, 'tempted': 448, 'moment': 449, 'cars': 450, 'leave': 451, 'slope': 452, 'bare': 453, 'shines': 454, 'now': 455, 'warmer': 456, 'moon': 457, 'except': 458, 'others': 459, 'theirs': 460, 'refuse': 461, 'beyond': 462, 'time': 463, 'profitable': 464, 'growth': 465, 'trial': 466, 'market': 467, 'everything': 468, 'dallied': 469, 'selling': 470, 'mistaken': 471, 'courtesy': 472, 'fear': 473, 'seeming': 474, 'short': 475, 'speech': 476, 'hope': 477, 'hearing': 478, 'mine': 479, 'arent': 480, 'while': 481, 'many': 482, 'cut': 483, 'expect': 484, 'spring': 485, 'clumps': 486, 'close': 487, 'lop': 488, 'each': 489, 'few': 490, 'quite': 491, 'solitary': 492, 'equal': 493, 'latter': 494, 'nodded': 495, 'yes': 496, 'paused': 497, 'beneath': 498, 'lovelier': 499, 'buyers': 500, 'moderation': 501, 'wasnt': 502, 'we': 503, 'climbed': 504, 'crossed': 505, '—at': 506, 'felt': 507, 'need': 508, 'softening': 509, 'certain': 510, 'show': 511, 'surprise': 512, 'seemed': 513, 'extent': 514, 'strip': 515, 'figured': 516, 'dollar': 517, 'friends': 518, 'writing': 519, 'within': 520, 'hour': 521, 'pay': 522, 'cities': 523, 'those': 524, 'regular': 525, 'vestrytrees': 526, 'whole': 527, 'sunday': 528, 'schools': 529, 'hang': 530, 'pick': 531, 'didnt': 532, 'give': 533, 'shown': 534, 'simple': 535, 'calculation': 536, 'bad': 537, 'couldnt': 538, 'letter': 539, 'cant': 540, 'help': 541, 'send': 542, 'herewith': 543, 'merry': 544}\n",
      "545\n",
      "[[112, 113], [112, 113, 114], [112, 113, 114, 7], [112, 113, 114, 7, 5], [112, 113, 114, 7, 5, 190], [112, 113, 114, 7, 5, 190, 75], [3, 191], [3, 191, 4], [3, 191, 4, 15], [3, 191, 4, 15, 12], [3, 191, 4, 15, 12, 192], [3, 191, 4, 15, 12, 192, 76], [3, 17], [3, 17, 13], [3, 17, 13, 193], [3, 17, 13, 193, 77], [3, 17, 13, 193, 77, 4], [3, 17, 13, 193, 77, 4, 194], [3, 115], [3, 115, 35], [3, 115, 35, 13], [3, 115, 35, 13, 14], [3, 115, 35, 13, 14, 116], [3, 115, 35, 13, 14, 116, 14], [3, 115, 35, 13, 14, 116, 14, 4], [3, 115, 35, 13, 14, 116, 14, 4, 15], [2, 36], [2, 36, 9], [2, 36, 9, 195], [2, 36, 9, 195, 7], [2, 36, 9, 195, 7, 1], [2, 36, 9, 195, 7, 1, 196], [37, 54], [37, 54, 1], [37, 54, 1, 78], [37, 54, 1, 78, 14], [37, 54, 1, 78, 14, 197], [37, 54, 1, 78, 14, 197, 14], [37, 54, 1, 78, 14, 197, 14, 198], [3, 79], [3, 79, 199], [3, 79, 199, 1], [3, 79, 199, 1, 117], [3, 79, 199, 1, 117, 200], [201, 9], [201, 9, 10], [201, 9, 10, 202], [201, 9, 10, 202, 3], [201, 9, 10, 202, 3, 203], [201, 9, 10, 202, 3, 203, 204], [80, 14], [80, 14, 16], [80, 14, 16, 18], [80, 14, 16, 18, 1], [80, 14, 16, 18, 1, 205], [80, 14, 16, 18, 1, 205, 27], [24, 206], [24, 206, 11], [24, 206, 11, 207], [24, 206, 11, 207, 81], [24, 206, 11, 207, 81, 1], [24, 206, 11, 207, 81, 1, 118], [3, 76], [3, 76, 18], [3, 76, 18, 119], [3, 76, 18, 119, 208], [3, 76, 18, 119, 208, 120], [7, 121], [7, 121, 55], [7, 121, 55, 209], [7, 121, 55, 209, 24], [7, 121, 55, 209, 24, 210], [7, 121, 55, 209, 24, 210, 122], [211, 4], [211, 4, 123], [211, 4, 123, 1], [211, 4, 123, 1, 124], [211, 4, 123, 1, 124, 16], [211, 4, 123, 1, 124, 16, 212], [211, 4, 123, 1, 124, 16, 212, 125], [56, 213], [56, 213, 126], [56, 213, 126, 82], [56, 213, 126, 82, 214], [56, 213, 126, 82, 214, 28], [56, 213, 126, 82, 214, 28, 2], [56, 213, 126, 82, 214, 28, 2, 82], [4, 215], [4, 215, 45], [4, 215, 45, 4], [4, 215, 45, 4, 57], [4, 215, 45, 4, 57, 216], [4, 215, 45, 4, 57, 216, 38], [4, 215, 45, 4, 57, 216, 38, 58], [4, 217], [4, 217, 17], [4, 217, 17, 218], [4, 217, 17, 218, 219], [4, 217, 17, 218, 219, 19], [4, 217, 17, 218, 219, 19, 5], [4, 217, 17, 218, 219, 19, 5, 220], [221, 127], [221, 127, 3], [221, 127, 3, 127], [221, 127, 3, 127, 222], [112, 113], [112, 113, 114], [112, 113, 114, 7], [112, 113, 114, 7, 5], [112, 113, 114, 7, 5, 75], [112, 113, 114, 7, 5, 75, 3], [112, 113, 114, 7, 5, 75, 3, 223], [4, 54], [4, 54, 1], [4, 54, 1, 13], [4, 54, 1, 13, 224], [4, 54, 1, 13, 224, 225], [4, 54, 1, 13, 224, 225, 20], [3, 18], [3, 18, 226], [3, 18, 226, 227], [3, 18, 226, 227, 29], [3, 18, 226, 227, 29, 1], [3, 18, 226, 227, 29, 1, 228], [128, 46], [128, 46, 229], [128, 46, 229, 47], [128, 46, 229, 47, 4], [128, 46, 229, 47, 4, 59], [128, 46, 229, 47, 4, 59, 4], [128, 46, 229, 47, 4, 59, 4, 60], [30, 129], [30, 129, 61], [30, 129, 61, 7], [30, 129, 61, 7, 1], [30, 129, 61, 7, 1, 230], [30, 129, 61, 7, 1, 230, 80], [8, 231], [8, 231, 12], [8, 231, 12, 62], [8, 231, 12, 62, 39], [8, 231, 12, 62, 39, 232], [8, 231, 12, 62, 39, 232, 233], [2, 234], [2, 234, 30], [2, 234, 30, 46], [2, 234, 30, 46, 130], [2, 234, 30, 46, 130, 83], [2, 234, 30, 46, 130, 83, 19], [2, 234, 30, 46, 130, 83, 19, 131], [63, 84], [63, 84, 235], [63, 84, 235, 85], [63, 84, 235, 85, 59], [63, 84, 235, 85, 59, 9], [63, 84, 235, 85, 59, 9, 236], [2, 237], [2, 237, 132], [2, 237, 132, 5], [2, 237, 132, 5, 238], [2, 237, 132, 5, 238, 239], [133, 1], [133, 1, 46], [133, 1, 46, 3], [133, 1, 46, 3, 240], [133, 1, 46, 3, 240, 241], [1, 242], [1, 242, 243], [1, 242, 243, 6], [1, 242, 243, 6, 1], [1, 242, 243, 6, 1, 244], [8, 245], [8, 245, 30], [8, 245, 30, 246], [8, 245, 30, 246, 247], [8, 245, 30, 246, 247, 5], [8, 245, 30, 246, 247, 5, 248], [2, 134], [2, 134, 45], [2, 134, 45, 27], [2, 134, 45, 27, 61], [2, 134, 45, 27, 61, 31], [2, 134, 45, 27, 61, 31, 249], [1, 135], [1, 135, 78], [1, 135, 78, 250], [1, 135, 78, 250, 1], [1, 135, 78, 250, 1, 136], [6, 251], [6, 251, 48], [6, 251, 48, 3], [6, 251, 48, 3, 252], [6, 251, 48, 3, 252, 253], [1, 46], [1, 46, 47], [1, 46, 47, 254], [1, 46, 47, 254, 255], [1, 46, 47, 254, 255, 3], [1, 46, 47, 254, 255, 3, 256], [21, 4], [21, 4, 32], [21, 4, 32, 257], [21, 4, 32, 257, 2], [21, 4, 32, 257, 2, 137], [3, 138], [3, 138, 2], [3, 138, 2, 40], [3, 138, 2, 40, 64], [3, 138, 2, 40, 64, 4], [3, 138, 2, 40, 64, 4, 139], [3, 138], [3, 138, 2], [3, 138, 2, 40], [3, 138, 2, 40, 64], [3, 138, 2, 40, 64, 4], [3, 138, 2, 40, 64, 4, 139], [65, 4], [65, 4, 62], [65, 4, 62, 86], [65, 4, 62, 86, 87], [65, 4, 62, 86, 87, 2], [65, 4, 62, 86, 87, 2, 66], [65, 4, 62, 86, 87, 2, 66, 3], [65, 4, 62, 86, 87, 2, 66, 3, 67], [88, 1], [88, 1, 258], [88, 1, 258, 6], [88, 1, 258, 6, 259], [88, 1, 258, 6, 259, 260], [88, 1, 258, 6, 259, 260, 22], [4, 33], [4, 33, 2], [4, 33, 2, 59], [4, 33, 2, 59, 31], [4, 33, 2, 59, 31, 261], [4, 33, 2, 59, 31, 261, 262], [4, 33, 2, 59, 31, 261, 262, 140], [4, 33, 2, 59, 31, 261, 262, 140, 11], [21, 140], [21, 140, 263], [21, 140, 263, 87], [21, 140, 263, 87, 11], [21, 140, 263, 87, 11, 35], [21, 140, 263, 87, 11, 35, 2], [21, 140, 263, 87, 11, 35, 2, 264], [14, 265], [14, 265, 68], [14, 265, 68, 266], [14, 265, 68, 266, 25], [14, 265, 68, 266, 25, 85], [14, 265, 68, 266, 25, 85, 32], [14, 265, 68, 266, 25, 85, 32, 267], [14, 265, 68, 266, 25, 85, 32, 267, 11], [268, 19], [268, 19, 269], [268, 19, 269, 5], [268, 19, 269, 5, 270], [268, 19, 269, 5, 270, 141], [268, 19, 269, 5, 270, 141, 119], [271, 5], [271, 5, 272], [271, 5, 272, 26], [271, 5, 272, 26, 273], [271, 5, 272, 26, 273, 274], [271, 5, 272, 26, 273, 274, 142], [14, 1], [14, 1, 275], [14, 1, 275, 276], [14, 1, 275, 276, 3], [14, 1, 275, 276, 3, 277], [14, 1, 275, 276, 3, 277, 278], [14, 1], [14, 1, 279], [14, 1, 279, 280], [14, 1, 279, 280, 3], [14, 1, 279, 280, 3, 281], [14, 1, 279, 280, 3, 281, 41], [14, 1, 279, 280, 3, 281, 41, 282], [89, 1], [89, 1, 283], [89, 1, 283, 284], [89, 1, 283, 284, 285], [89, 1, 283, 284, 285, 11], [89, 1, 283, 284, 285, 11, 286], [89, 1, 283, 284, 285, 11, 286, 287], [89, 1, 283, 284, 285, 11, 286, 287, 288], [289, 3], [289, 3, 290], [289, 3, 290, 28], [289, 3, 290, 28, 1], [289, 3, 290, 28, 1, 291], [292, 293], [292, 293, 6], [292, 293, 6, 143], [292, 293, 6, 143, 294], [292, 293, 6, 143, 294, 2], [292, 293, 6, 143, 294, 2, 136], [292, 293, 6, 143, 294, 2, 136, 49], [295, 59], [295, 59, 1], [295, 59, 1, 296], [295, 59, 1, 296, 297], [295, 59, 1, 296, 297, 6], [295, 59, 1, 296, 297, 6, 144], [295, 59, 1, 296, 297, 6, 144, 24], [295, 59, 1, 296, 297, 6, 144, 24, 298], [26, 47], [26, 47, 299], [26, 47, 299, 2], [26, 47, 299, 2, 1], [26, 47, 299, 2, 1, 300], [26, 47, 299, 2, 1, 300, 301], [26, 47, 299, 2, 1, 300, 301, 20], [26, 47, 299, 2, 1, 300, 301, 20, 1], [26, 47, 299, 2, 1, 300, 301, 20, 1, 302], [3, 26], [3, 26, 303], [3, 26, 303, 12], [3, 26, 303, 12, 2], [3, 26, 303, 12, 2, 304], [3, 26, 303, 12, 2, 304, 80], [3, 26, 303, 12, 2, 304, 80, 90], [3, 26, 303, 12, 2, 304, 80, 90, 26], [3, 26, 303, 12, 2, 304, 80, 90, 26, 47], [3, 26, 303, 12, 2, 304, 80, 90, 26, 47, 305], [23, 306], [23, 306, 16], [23, 306, 16, 77], [23, 306, 16, 77, 26], [23, 306, 16, 77, 26, 91], [23, 306, 16, 77, 26, 91, 67], [23, 306, 16, 77, 26, 91, 67, 142], [25, 92], [25, 92, 62], [25, 92, 62, 41], [25, 92, 62, 41, 307], [25, 92, 62, 41, 307, 308], [25, 92, 62, 41, 307, 308, 7], [25, 92, 62, 41, 307, 308, 7, 1], [25, 92, 62, 41, 307, 308, 7, 1, 46], [309, 310], [309, 310, 311], [309, 310, 311, 41], [309, 310, 311, 41, 121], [309, 310, 311, 41, 121, 28], [309, 310, 311, 41, 121, 28, 1], [309, 310, 311, 41, 121, 28, 1, 93], [33, 312], [33, 312, 28], [33, 312, 28, 313], [33, 312, 28, 313, 3], [33, 312, 28, 313, 3, 314], [33, 312, 28, 313, 3, 314, 18], [33, 312, 28, 313, 3, 314, 18, 315], [33, 312, 28, 313, 3, 314, 18, 315, 41], [33, 312, 28, 313, 3, 314, 18, 315, 41, 316], [64, 11], [64, 11, 42], [64, 11, 42, 41], [64, 11, 42, 41, 317], [64, 11, 42, 41, 317, 2], [64, 11, 42, 41, 317, 2, 318], [64, 11, 42, 41, 317, 2, 318, 7], [64, 11, 42, 41, 317, 2, 318, 7, 1], [64, 11, 42, 41, 317, 2, 318, 7, 1, 145], [21, 4], [21, 4, 10], [21, 4, 10, 69], [21, 4, 10, 69, 2], [21, 4, 10, 69, 2, 94], [21, 4, 10, 69, 2, 94, 65], [21, 4, 10, 69, 2, 94, 65, 319], [21, 4, 10, 69, 2, 94, 65, 319, 320], [21, 4, 10, 69, 2, 94, 65, 319, 320, 7], [19, 29], [19, 29, 321], [19, 29, 321, 322], [19, 29, 321, 322, 81], [19, 29, 321, 322, 81, 1], [19, 29, 321, 322, 81, 1, 323], [4, 57], [4, 57, 324], [4, 57, 324, 2], [4, 57, 324, 2, 32], [4, 57, 324, 2, 32, 31], [4, 57, 324, 2, 32, 31, 146], [4, 57, 324, 2, 32, 31, 146, 87], [4, 57, 324, 2, 32, 31, 146, 87, 11], [14, 8], [14, 8, 325], [14, 8, 325, 50], [14, 8, 325, 50, 3], [14, 8, 325, 50, 3, 7], [14, 8, 325, 50, 3, 7, 2], [14, 8, 325, 50, 3, 7, 2, 326], [14, 8, 325, 50, 3, 7, 2, 326, 1], [14, 8, 325, 50, 3, 7, 2, 326, 1, 327], [31, 146], [31, 146, 34], [31, 146, 34, 116], [31, 146, 34, 116, 51], [31, 146, 34, 116, 51, 328], [31, 146, 34, 116, 51, 328, 2], [31, 146, 34, 116, 51, 328, 2, 147], [31, 146, 34, 116, 51, 328, 2, 147, 329], [128, 135], [128, 135, 148], [128, 135, 148, 10], [128, 135, 148, 10, 52], [128, 135, 148, 10, 52, 8], [128, 135, 148, 10, 52, 8, 149], [128, 135, 148, 10, 52, 8, 149, 330], [331, 70], [331, 70, 141], [331, 70, 141, 3], [331, 70, 141, 3, 15], [331, 70, 141, 3, 15, 148], [331, 70, 141, 3, 15, 148, 332], [13, 20], [13, 20, 13], [13, 20, 13, 8], [13, 20, 13, 8, 333], [13, 20, 13, 8, 333, 30], [13, 20, 13, 8, 333, 30, 334], [13, 20, 13, 8, 333, 30, 334, 22], [20, 335], [20, 335, 11], [20, 335, 11, 35], [20, 335, 11, 35, 42], [20, 335, 11, 35, 42, 3], [20, 335, 11, 35, 42, 3, 42], [20, 335, 11, 35, 42, 3, 42, 95], [336, 8], [336, 8, 54], [336, 8, 54, 1], [336, 8, 54, 1, 337], [336, 8, 54, 1, 337, 50], [336, 8, 54, 1, 337, 50, 6], [336, 8, 54, 1, 337, 50, 6, 11], [3, 12], [3, 12, 13], [3, 12, 13, 21], [3, 12, 13, 21, 338], [3, 12, 13, 21, 338, 339], [3, 12, 13, 21, 338, 339, 12], [3, 12, 13, 21, 338, 339, 12, 13], [3, 12, 13, 21, 338, 339, 12, 13, 10], [3, 12, 13, 21, 338, 339, 12, 13, 10, 66], [16, 96], [16, 96, 2], [16, 96, 2, 340], [16, 96, 2, 340, 8], [16, 96, 2, 340, 8, 341], [16, 96, 2, 340, 8, 341, 29], [16, 96, 2, 340, 8, 341, 29, 27], [16, 96, 2, 340, 8, 341, 29, 27, 10], [2, 147], [2, 147, 81], [2, 147, 81, 12], [2, 147, 81, 12, 342], [2, 147, 81, 12, 342, 50], [2, 147, 81, 12, 342, 50, 34], [2, 147, 81, 12, 342, 50, 34, 89], [3, 23], [3, 23, 12], [3, 23, 12, 343], [3, 23, 12, 343, 1], [3, 23, 12, 343, 1, 97], [3, 23, 12, 343, 1, 97, 49], [344, 2], [344, 2, 1], [344, 2, 1, 93], [344, 2, 1, 93, 8], [344, 2, 1, 93, 8, 345], [344, 2, 1, 93, 8, 345, 123], [344, 2, 1, 93, 8, 345, 123, 30], [344, 2, 1, 93, 8, 345, 123, 30, 346], [2, 1], [2, 1, 150], [2, 1, 150, 151], [2, 1, 150, 151, 152], [2, 1, 150, 151, 152, 347], [19, 1], [19, 1, 118], [19, 1, 118, 348], [19, 1, 118, 348, 25], [19, 1, 118, 348, 25, 349], [19, 1, 118, 348, 25, 349, 2], [19, 1, 118, 348, 25, 349, 2, 130], [19, 1, 118, 348, 25, 349, 2, 130, 5], [19, 1, 118, 348, 25, 349, 2, 130, 5, 350], [83, 2], [83, 2, 1], [83, 2, 1, 153], [83, 2, 1, 153, 3], [83, 2, 1, 153, 3, 351], [83, 2, 1, 153, 3, 351, 352], [83, 2, 1, 153, 3, 351, 352, 1], [83, 2, 1, 153, 3, 351, 352, 1, 153], [37, 8], [37, 8, 353], [37, 8, 353, 354], [37, 8, 353, 354, 154], [37, 8, 353, 354, 154, 124], [37, 8, 353, 354, 154, 124, 19], [37, 8, 353, 354, 154, 124, 19, 5], [37, 8, 353, 354, 154, 124, 19, 5, 355], [356, 30], [356, 30, 82], [356, 30, 82, 35], [356, 30, 82, 35, 155], [356, 30, 82, 35, 155, 1], [356, 30, 82, 35, 155, 1, 357], [356, 30, 82, 35, 155, 1, 357, 2], [356, 30, 82, 35, 155, 1, 357, 2, 1], [356, 30, 82, 35, 155, 1, 357, 2, 1, 93], [23, 10], [23, 10, 4], [23, 10, 4, 90], [23, 10, 4, 90, 358], [23, 10, 4, 90, 358, 5], [23, 10, 4, 90, 358, 5, 156], [23, 10, 4, 90, 358, 5, 156, 6], [23, 10, 4, 90, 358, 5, 156, 6, 86], [3, 23], [3, 23, 4], [3, 23, 4, 359], [3, 23, 4, 359, 6], [3, 23, 4, 359, 6, 69], [3, 23, 4, 359, 6, 69, 58], [3, 23, 4, 359, 6, 69, 58, 2], [3, 23, 4, 359, 6, 69, 58, 2, 17], [53, 65], [53, 65, 157], [53, 65, 157, 360], [53, 65, 157, 360, 6], [53, 65, 157, 360, 6, 361], [3, 362], [3, 362, 61], [3, 362, 61, 34], [3, 362, 61, 34, 158], [3, 362, 61, 34, 158, 33], [3, 362, 61, 34, 158, 33, 5], [3, 362, 61, 34, 158, 33, 5, 363], [3, 362, 61, 34, 158, 33, 5, 363, 75], [36, 364], [36, 364, 365], [36, 364, 365, 366], [36, 364, 365, 366, 3], [36, 364, 365, 366, 3, 367], [36, 364, 365, 366, 3, 367, 19], [36, 364, 365, 366, 3, 367, 19, 1], [36, 364, 365, 366, 3, 367, 19, 1, 368], [143, 88], [143, 88, 9], [143, 88, 9, 3], [143, 88, 9, 3, 13], [143, 88, 9, 3, 13, 369], [143, 88, 9, 3, 13, 369, 61], [143, 88, 9, 3, 13, 369, 61, 370], [51, 5], [51, 5, 371], [51, 5, 371, 79], [51, 5, 371, 79, 372], [51, 5, 371, 79, 372, 88], [51, 5, 371, 79, 372, 88, 9], [51, 5, 371, 79, 372, 88, 9, 373], [71, 33], [71, 33, 2], [71, 33, 2, 374], [71, 33, 2, 374, 49], [71, 33, 2, 374, 49, 51], [71, 33, 2, 374, 49, 51, 375], [71, 33, 2, 374, 49, 51, 375, 376], [3, 37], [3, 37, 38], [3, 37, 38, 58], [3, 37, 38, 58, 2], [3, 37, 38, 58, 2, 9], [3, 37, 38, 58, 2, 9, 3], [3, 37, 38, 58, 2, 9, 3, 377], [3, 37, 38, 58, 2, 9, 3, 377, 42], [92, 55], [92, 55, 378], [92, 55, 378, 379], [92, 55, 378, 379, 380], [92, 55, 378, 379, 380, 39], [3, 381], [3, 381, 382], [3, 381, 382, 52], [3, 381, 382, 52, 4], [3, 381, 382, 52, 4, 383], [3, 381, 382, 52, 4, 383, 3], [3, 381, 382, 52, 4, 383, 3, 384], [3, 381, 382, 52, 4, 383, 3, 384, 39], [3, 381, 382, 52, 4, 383, 3, 384, 39, 49], [12, 2], [12, 2, 385], [12, 2, 385, 386], [12, 2, 385, 386, 1], [12, 2, 385, 386, 1, 67], [12, 2, 385, 386, 1, 67, 72], [12, 2, 385, 386, 1, 67, 72, 16], [12, 2, 385, 386, 1, 67, 72, 16, 387], [4, 159], [4, 159, 60], [4, 159, 60, 36], [4, 159, 60, 36, 53], [4, 159, 60, 36, 53, 388], [4, 159, 60, 36, 53, 388, 2], [4, 159, 60, 36, 53, 388, 2, 40], [4, 159, 60, 36, 53, 388, 2, 40, 117], [71, 33], [71, 33, 2], [71, 33, 2, 40], [71, 33, 2, 40, 20], [71, 33, 2, 40, 20, 152], [71, 33, 2, 40, 20, 152, 5], [71, 33, 2, 40, 20, 152, 5, 389], [71, 33, 2, 40, 20, 152, 5, 389, 97], [3, 390], [3, 390, 122], [3, 390, 122, 151], [3, 390, 122, 151, 83], [3, 390, 122, 151, 83, 5], [3, 390, 122, 151, 83, 5, 391], [3, 390, 122, 151, 83, 5, 391, 392], [393, 144], [393, 144, 160], [393, 144, 160, 1], [393, 144, 160, 1, 97], [393, 144, 160, 1, 97, 15], [393, 144, 160, 1, 97, 15, 394], [393, 144, 160, 1, 97, 15, 394, 55], [393, 144, 160, 1, 97, 15, 394, 55, 98], [21, 395], [21, 395, 53], [21, 395, 53, 150], [21, 395, 53, 150, 3], [21, 395, 53, 150, 3, 396], [21, 395, 53, 150, 3, 396, 39], [21, 395, 53, 150, 3, 396, 39, 35], [21, 395, 53, 150, 3, 396, 39, 35, 95], [18, 43], [18, 43, 17], [18, 43, 17, 99], [18, 43, 17, 99, 76], [18, 43, 17, 99, 76, 69], [18, 43, 17, 99, 76, 69, 3], [18, 43, 17, 99, 76, 69, 3, 397], [18, 43, 17, 99, 76, 69, 3, 397, 58], [13, 15], [13, 15, 68], [13, 15, 68, 398], [13, 15, 68, 398, 100], [13, 15, 68, 398, 100, 17], [13, 15, 68, 398, 100, 17, 5], [13, 15, 68, 398, 100, 17, 5, 156], [13, 15, 68, 398, 100, 17, 5, 156, 6], [13, 15, 68, 398, 100, 17, 5, 156, 6, 86], [64, 161], [64, 161, 101], [64, 161, 101, 2], [64, 161, 101, 2, 162], [64, 161, 101, 2, 162, 9], [64, 161, 101, 2, 162, 9, 67], [1, 48], [1, 48, 90], [1, 48, 90, 102], [1, 48, 90, 102, 163], [1, 48, 90, 102, 163, 399], [3, 164], [3, 164, 53], [3, 164, 53, 400], [3, 164, 53, 400, 125], [3, 164, 53, 400, 125, 3], [3, 164, 53, 400, 125, 3, 401], [7, 402], [7, 402, 403], [7, 402, 403, 72], [7, 402, 403, 72, 36], [7, 402, 403, 72, 36, 9], [7, 402, 403, 72, 36, 9, 404], [161, 101], [161, 101, 2], [161, 101, 2, 165], [161, 101, 2, 165, 9], [161, 101, 2, 165, 9, 52], [161, 101, 2, 165, 9, 52, 10], [161, 101, 2, 165, 9, 52, 10, 405], [9, 166], [9, 166, 149], [9, 166, 149, 1], [9, 166, 149, 1, 72], [9, 166, 149, 1, 72, 2], [9, 166, 149, 1, 72, 2, 162], [9, 102], [9, 102, 34], [9, 102, 34, 406], [9, 102, 34, 406, 167], [9, 102, 34, 406, 167, 10], [9, 102, 34, 406, 167, 10, 407], [3, 408], [3, 408, 9], [3, 408, 9, 409], [3, 408, 9, 409, 2], [3, 408, 9, 409, 2, 40], [8, 54], [8, 54, 5], [8, 54, 5, 84], [8, 54, 5, 84, 7], [8, 54, 5, 84, 7, 30], [8, 54, 5, 84, 7, 30, 410], [3, 411], [3, 411, 9], [3, 411, 9, 77], [3, 411, 9, 77, 73], [3, 411, 9, 77, 73, 16], [3, 411, 9, 77, 73, 16, 168], [2, 17], [2, 17, 412], [2, 17, 412, 169], [2, 17, 412, 169, 170], [3, 37], [3, 37, 20], [3, 37, 20, 171], [3, 37, 20, 171, 102], [3, 37, 20, 171, 102, 9], [3, 37, 20, 171, 102, 9, 413], [20, 171], [20, 171, 9], [20, 171, 9, 10], [20, 171, 9, 10, 414], [20, 171, 9, 10, 414, 3], [20, 171, 9, 10, 414, 3, 415], [1, 48], [1, 48, 1], [1, 48, 1, 48], [1, 48, 1, 48, 24], [1, 48, 1, 48, 24, 172], [1, 48, 1, 48, 24, 172, 2], [1, 48, 1, 48, 24, 172, 2, 416], [5, 84], [5, 84, 155], [5, 84, 155, 1], [5, 84, 155, 1, 417], [5, 84, 155, 1, 417, 3], [5, 84, 155, 1, 417, 3, 418], [1, 167], [1, 167, 10], [1, 167, 10, 419], [1, 167, 10, 419, 48], [1, 167, 10, 419, 48, 15], [1, 167, 10, 419, 48, 15, 62], [1, 103], [1, 103, 24], [1, 103, 24, 420], [1, 103, 24, 420, 169], [1, 103, 24, 420, 169, 163], [3, 66], [3, 66, 421], [3, 66, 421, 422], [3, 66, 421, 422, 1], [3, 66, 421, 422, 1, 104], [3, 66, 421, 422, 1, 104, 2], [3, 66, 421, 422, 1, 104, 2, 1], [3, 66, 421, 422, 1, 104, 2, 1, 104], [65, 133], [65, 133, 173], [65, 133, 173, 6], [65, 133, 173, 6, 131], [65, 133, 173, 6, 131, 12], [65, 133, 173, 6, 131, 12, 38], [65, 133, 173, 6, 131, 12, 38, 2], [65, 133, 173, 6, 131, 12, 38, 2, 423], [3, 173], [3, 173, 6], [3, 173, 6, 424], [3, 173, 6, 424, 12], [3, 173, 6, 424, 12, 56], [3, 173, 6, 424, 12, 56, 425], [3, 173, 6, 424, 12, 56, 425, 27], [3, 173, 6, 424, 12, 56, 425, 27, 426], [5, 427], [5, 427, 2], [5, 427, 2, 428], [5, 427, 2, 428, 429], [5, 427, 2, 428, 429, 174], [5, 427, 2, 428, 429, 174, 115], [5, 427, 2, 428, 429, 174, 115, 1], [5, 427, 2, 428, 429, 174, 115, 1, 103], [56, 164], [56, 164, 7], [56, 164, 7, 104], [56, 164, 7, 104, 430], [56, 164, 7, 104, 430, 7], [56, 164, 7, 104, 430, 7, 18], [56, 164, 7, 104, 430, 7, 18, 27], [8, 431], [8, 431, 3], [8, 431, 3, 432], [8, 431, 3, 432, 160], [8, 431, 3, 432, 160, 8], [8, 431, 3, 432, 160, 8, 433], [8, 431, 3, 432, 160, 8, 433, 434], [8, 431, 3, 432, 160, 8, 433, 434, 50], [435, 436], [435, 436, 2], [435, 436, 2, 134], [435, 436, 2, 134, 96], [435, 436, 2, 134, 96, 174], [435, 436, 2, 134, 96, 174, 8], [435, 436, 2, 134, 96, 174, 8, 10], [8, 437], [8, 437, 2], [8, 437, 2, 17], [8, 437, 2, 17, 1], [8, 437, 2, 17, 1, 103], [8, 437, 2, 17, 1, 103, 38], [8, 437, 2, 17, 1, 103, 38, 95], [2, 105], [2, 105, 16], [2, 105, 16, 438], [2, 105, 16, 438, 9], [2, 105, 16, 438, 9, 24], [2, 105, 16, 438, 9, 24, 66], [2, 105, 16, 438, 9, 24, 66, 175], [3, 15], [3, 15, 12], [3, 15, 12, 68], [3, 15, 12, 68, 132], [3, 15, 12, 68, 132, 3], [3, 15, 12, 68, 132, 3, 137], [3, 15, 12, 68, 132, 3, 137, 53], [3, 15, 12, 68, 132, 3, 137, 53, 44], [8, 439], [8, 439, 45], [8, 439, 45, 4], [8, 439, 45, 4, 43], [8, 439, 45, 4, 43, 106], [8, 439, 45, 4, 43, 106, 63], [8, 439, 45, 4, 43, 106, 63, 44], [8, 439, 45, 4, 43, 106, 63, 44, 22], [63, 440], [63, 440, 441], [63, 440, 441, 442], [63, 440, 441, 442, 443], [63, 440, 441, 442, 443, 33], [63, 440, 441, 442, 443, 33, 5], [63, 440, 441, 442, 443, 33, 5, 72], [36, 444], [36, 444, 29], [36, 444, 29, 47], [36, 444, 29, 47, 445], [36, 444, 29, 47, 445, 3], [36, 444, 29, 47, 445, 3, 32], [36, 444, 29, 47, 445, 3, 32, 446], [4, 166], [4, 166, 107], [4, 166, 107, 6], [4, 166, 107, 6, 11], [4, 166, 107, 6, 11, 14], [4, 166, 107, 6, 11, 14, 44], [4, 166, 107, 6, 11, 14, 44, 22], [4, 447], [4, 447, 45], [4, 447, 45, 4], [4, 447, 45, 4, 10], [4, 447, 45, 4, 10, 448], [4, 447, 45, 4, 10, 448, 16], [4, 447, 45, 4, 10, 448, 16, 5], [4, 447, 45, 4, 10, 448, 16, 5, 449], [2, 106], [2, 106, 11], [2, 106, 11, 176], [2, 106, 11, 176, 41], [2, 106, 11, 176, 41, 154], [2, 106, 11, 176, 41, 154, 2], [2, 106, 11, 176, 41, 154, 2, 40], [2, 106, 11, 176, 41, 154, 2, 40, 7], [2, 106, 11, 176, 41, 154, 2, 40, 7, 450], [3, 451], [3, 451, 1], [3, 451, 1, 452], [3, 451, 1, 452, 175], [3, 451, 1, 452, 175, 1], [3, 451, 1, 452, 175, 1, 129], [3, 451, 1, 452, 175, 1, 129, 29], [3, 451, 1, 452, 175, 1, 129, 29, 453], [36, 1], [36, 1, 145], [36, 1, 145, 454], [36, 1, 145, 454, 455], [36, 1, 145, 454, 455, 55], [36, 1, 145, 454, 455, 55, 456], [36, 1, 145, 454, 455, 55, 456, 100], [36, 1, 145, 454, 455, 55, 456, 100, 1], [36, 1, 145, 454, 455, 55, 456, 100, 1, 457], [71, 177], [71, 177, 2], [71, 177, 2, 32], [71, 177, 2, 32, 11], [71, 177, 2, 32, 11, 60], [71, 177, 2, 32, 11, 60, 9], [71, 177, 2, 32, 11, 60, 9, 45], [71, 177, 2, 32, 11, 60, 9, 45, 4], [71, 177, 2, 32, 11, 60, 9, 45, 4, 10], [56, 98], [56, 98, 71], [56, 98, 71, 177], [56, 98, 71, 177, 2], [56, 98, 71, 177, 2, 178], [56, 98, 71, 177, 2, 178, 63], [56, 98, 71, 177, 2, 178, 63, 22], [56, 98, 71, 177, 2, 178, 63, 22, 458], [14, 459], [14, 459, 178], [14, 459, 178, 460], [14, 459, 178, 460, 70], [14, 459, 178, 460, 70, 461], [14, 459, 178, 460, 70, 461, 16], [14, 459, 178, 460, 70, 461, 16, 11], [462, 1], [462, 1, 463], [462, 1, 463, 6], [462, 1, 463, 6, 464], [462, 1, 463, 6, 464, 465], [1, 466], [1, 466, 20], [1, 466, 20, 467], [1, 466, 20, 467, 468], [1, 466, 20, 467, 468, 85], [1, 466, 20, 467, 468, 85, 38], [1, 466, 20, 467, 468, 85, 38, 2], [4, 469], [4, 469, 23], [4, 469, 23, 158], [4, 469, 23, 158, 19], [4, 469, 23, 158, 19, 1], [4, 469, 23, 158, 19, 1, 107], [4, 469, 23, 158, 19, 1, 107, 6], [4, 469, 23, 158, 19, 1, 107, 6, 470], [37, 179], [37, 179, 51], [37, 179, 51, 471], [37, 179, 51, 471, 472], [3, 473], [3, 473, 6], [3, 473, 6, 474], [3, 473, 6, 474, 475], [3, 473, 6, 474, 475, 6], [3, 473, 6, 474, 475, 6, 476], [3, 473, 6, 474, 475, 6, 476, 70], [3, 473, 6, 474, 475, 6, 476, 70, 179], [51, 477], [51, 477, 6], [51, 477, 6, 478], [51, 477, 6, 478, 99], [51, 477, 6, 478, 99, 6], [51, 477, 6, 478, 99, 6, 52], [51, 477, 6, 478, 99, 6, 52, 10], [51, 477, 6, 478, 99, 6, 52, 10, 479], [51, 477, 6, 478, 99, 6, 52, 10, 479, 4], [51, 477, 6, 478, 99, 6, 52, 10, 479, 4, 180], [27, 480], [27, 480, 73], [27, 480, 73, 2], [27, 480, 73, 2, 17], [27, 480, 73, 2, 17, 181], [27, 480, 73, 2, 17, 181, 481], [4, 15], [4, 15, 89], [4, 15, 89, 165], [4, 15, 89, 165, 126], [4, 15, 89, 165, 126, 482], [4, 15, 89, 165, 126, 482, 26], [4, 15, 89, 165, 126, 482, 26, 43], [4, 15, 89, 165, 126, 482, 26, 43, 483], [25, 108], [25, 108, 39], [25, 108, 39, 105], [25, 108, 39, 105, 11], [25, 108, 39, 105, 11, 42], [25, 15], [25, 15, 105], [21, 159], [21, 159, 484], [21, 159, 484, 157], [21, 159, 484, 157, 69], [21, 159, 484, 157, 69, 2], [21, 159, 484, 157, 69, 2, 108], [21, 159, 484, 157, 69, 2, 108, 25], [21, 159, 484, 157, 69, 2, 108, 25, 32], [21, 159, 484, 157, 69, 2, 108, 25, 32, 11], [109, 26], [109, 26, 485], [109, 26, 485, 7], [109, 26, 485, 7, 31], [109, 26, 485, 7, 31, 7], [109, 26, 485, 7, 31, 7, 486], [109, 26, 485, 7, 31, 7, 486, 34], [109, 26, 485, 7, 31, 7, 486, 34, 487], [18, 488], [18, 488, 489], [18, 488, 489, 78], [18, 488, 489, 78, 6], [18, 488, 489, 78, 6, 182], [18, 488, 489, 78, 6, 182, 21], [18, 488, 489, 78, 6, 182, 21, 12], [18, 488, 489, 78, 6, 182, 21, 12, 5], [18, 488, 489, 78, 6, 182, 21, 12, 5, 490], [491, 492], [491, 492, 3], [491, 492, 3, 79], [491, 492, 3, 79, 493], [491, 492, 3, 79, 493, 182], [29, 183], [29, 183, 3], [29, 183, 3, 183], [29, 183, 3, 183, 1], [29, 183, 3, 183, 1, 494], [29, 183, 3, 183, 1, 494, 8], [29, 183, 3, 183, 1, 494, 8, 495], [29, 183, 3, 183, 1, 494, 8, 495, 496], [29, 183, 3, 183, 1, 494, 8, 495, 496, 2], [70, 497], [70, 497, 2], [70, 497, 2, 94], [70, 497, 2, 94, 498], [70, 497, 2, 94, 498, 31], [70, 497, 2, 94, 498, 31, 499], [70, 497, 2, 94, 498, 31, 499, 13], [19, 5], [19, 5, 500], [19, 5, 500, 501], [19, 5, 500, 501, 18], [19, 5, 500, 501, 18, 43], [19, 5, 500, 501, 18, 43, 68], [4, 107], [4, 107, 23], [4, 107, 23, 34], [4, 107, 23, 34, 21], [4, 107, 23, 34, 21, 502], [4, 107, 23, 34, 21, 502, 27], [4, 107, 23, 34, 21, 502, 27, 2], [4, 107, 23, 34, 21, 502, 27, 2, 94], [4, 107, 23, 34, 21, 502, 27, 2, 94, 23], [503, 504], [503, 504, 1], [503, 504, 1, 109], [503, 504, 1, 109, 28], [503, 504, 1, 109, 28, 1], [503, 504, 1, 109, 28, 1, 170], [503, 504, 1, 109, 28, 1, 170, 505], [503, 504, 1, 109, 28, 1, 170, 505, 42], [3, 101], [3, 101, 35], [3, 101, 35, 28], [3, 101, 35, 28, 1], [3, 101, 35, 28, 1, 168], [3, 101, 35, 28, 1, 168, 8], [3, 101, 35, 28, 1, 168, 8, 180], [3, 101, 35, 28, 1, 168, 8, 180, 5], [3, 101, 35, 28, 1, 168, 8, 180, 5, 74], [5, 74], [5, 74, 44], [5, 74, 44, 22], [5, 74, 44, 22, 506], [5, 74, 44, 22, 506, 52], [5, 74, 44, 22, 506, 52, 184], [8, 507], [8, 507, 31], [8, 507, 31, 508], [8, 507, 31, 508, 6], [8, 507, 31, 508, 6, 509], [8, 507, 31, 508, 6, 509, 18], [8, 507, 31, 508, 6, 509, 18, 2], [8, 507, 31, 508, 6, 509, 18, 2, 39], [5, 74], [5, 74, 22], [5, 74, 22, 43], [5, 74, 22, 43, 38], [5, 74, 22, 43, 38, 2], [5, 74, 22, 43, 38, 2, 185], [5, 74, 22, 43, 38, 2, 185, 186], [37, 4], [37, 4, 10], [37, 4, 10, 510], [37, 4, 10, 510, 4], [37, 4, 10, 510, 4, 24], [37, 4, 10, 510, 4, 24, 91], [37, 4, 10, 510, 4, 24, 91, 172], [2, 108], [2, 108, 96], [2, 108, 96, 32], [2, 108, 96, 32, 11], [2, 108, 96, 32, 11, 91], [2, 108, 96, 32, 11, 91, 511], [2, 108, 96, 32, 11, 91, 511, 512], [21, 185], [21, 185, 186], [21, 185, 186, 513], [21, 185, 186, 513, 23], [21, 185, 186, 513, 23, 187], [21, 185, 186, 513, 23, 187, 188], [1, 514], [1, 514, 6], [1, 514, 6, 109], [1, 514, 6, 109, 4], [1, 514, 6, 109, 4, 57], [1, 514, 6, 109, 4, 57, 515], [1, 514, 6, 109, 4, 57, 515, 110], [1, 514, 6, 109, 4, 57, 515, 110, 111], [16, 18], [16, 18, 10], [16, 18, 10, 29], [16, 18, 10, 29, 26], [16, 18, 10, 29, 26, 516], [16, 18, 10, 29, 26, 516, 50], [16, 18, 10, 29, 26, 516, 50, 184], [110, 111], [110, 111, 23], [110, 111, 23, 187], [110, 111, 23, 187, 188], [110, 111, 23, 187, 188, 1], [110, 111, 23, 187, 188, 1, 517], [110, 111, 23, 187, 188, 1, 517, 518], [4, 57], [4, 57, 17], [4, 57, 17, 519], [4, 57, 17, 519, 2], [4, 57, 17, 519, 2, 520], [4, 57, 17, 519, 2, 520, 1], [4, 57, 17, 519, 2, 520, 1, 521], [43, 522], [43, 522, 7], [43, 522, 7, 523], [43, 522, 7, 523, 16], [43, 522, 7, 523, 16, 99], [43, 522, 7, 523, 16, 99, 22], [43, 522, 7, 523, 16, 99, 22, 33], [43, 522, 7, 523, 16, 99, 22, 33, 524], [525, 526], [525, 526, 527], [525, 526, 527, 528], [525, 526, 527, 528, 529], [15, 530], [15, 530, 73], [15, 530, 73, 28], [15, 530, 73, 28, 2], [15, 530, 73, 28, 2, 531], [15, 530, 73, 28, 2, 531, 176], [15, 530, 73, 28, 2, 531, 176, 73], [5, 74], [5, 74, 44], [5, 74, 44, 22], [5, 74, 44, 22, 4], [5, 74, 44, 22, 4, 532], [5, 74, 44, 22, 4, 532, 60], [5, 74, 44, 22, 4, 532, 60, 4], [5, 74, 44, 22, 4, 532, 60, 4, 24], [181, 110], [181, 110, 111], [181, 110, 111, 98], [181, 110, 111, 98, 2], [181, 110, 111, 98, 2, 533], [181, 110, 111, 98, 2, 533, 49], [181, 110, 111, 98, 2, 533, 49, 100], [181, 110, 111, 98, 2, 533, 49, 100, 106], [14, 92], [14, 92, 17], [14, 92, 17, 534], [14, 92, 17, 534, 20], [14, 92, 17, 534, 20, 5], [14, 92, 17, 534, 20, 5, 535], [14, 92, 17, 534, 20, 5, 535, 536], [34, 537], [34, 537, 4], [34, 537, 4, 538], [34, 537, 4, 538, 120], [34, 537, 4, 538, 120, 13], [34, 537, 4, 538, 120, 13, 7], [34, 537, 4, 538, 120, 13, 7, 5], [34, 537, 4, 538, 120, 13, 7, 5, 539], [4, 540], [4, 540, 541], [4, 540, 541, 189], [4, 540, 541, 189, 4], [4, 540, 541, 189, 4, 15], [4, 540, 541, 189, 4, 15, 542], [4, 540, 541, 189, 4, 15, 542, 25], [4, 540, 541, 189, 4, 15, 542, 25, 13], [7, 189], [7, 189, 25], [7, 189, 25, 543], [7, 189, 25, 543, 5], [7, 189, 25, 543, 5, 544], [7, 189, 25, 543, 5, 544, 44]]\n",
      "11\n",
      "[[  0   0   0 ...   0 112 113]\n",
      " [  0   0   0 ... 112 113 114]\n",
      " [  0   0   0 ... 113 114   7]\n",
      " ...\n",
      " [  0   0   0 ...  25 543   5]\n",
      " [  0   0   0 ... 543   5 544]\n",
      " [  0   0   0 ...   5 544  44]]\n",
      "xs:  [[  0   0   0 ...   0   0 112]\n",
      " [  0   0   0 ...   0 112 113]\n",
      " [  0   0   0 ... 112 113 114]\n",
      " ...\n",
      " [  0   0   0 ... 189  25 543]\n",
      " [  0   0   0 ...  25 543   5]\n",
      " [  0   0   0 ... 543   5 544]]\n",
      "labels: [113 114   7 ...   5 544  44]\n",
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 1.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "38/38 [==============================] - 27s 483ms/step - loss: 6.1292 - accuracy: 0.0367\n",
      "Epoch 2/100\n",
      "38/38 [==============================] - 16s 423ms/step - loss: 5.8096 - accuracy: 0.0467\n",
      "Epoch 3/100\n",
      "38/38 [==============================] - 17s 444ms/step - loss: 5.7534 - accuracy: 0.0376\n",
      "Epoch 4/100\n",
      "38/38 [==============================] - 19s 510ms/step - loss: 5.7297 - accuracy: 0.0451\n",
      "Epoch 5/100\n",
      "38/38 [==============================] - 20s 526ms/step - loss: 5.6978 - accuracy: 0.0392\n",
      "Epoch 6/100\n",
      "38/38 [==============================] - 20s 533ms/step - loss: 5.6592 - accuracy: 0.0401\n",
      "Epoch 7/100\n",
      "38/38 [==============================] - 19s 496ms/step - loss: 5.5014 - accuracy: 0.0467\n",
      "Epoch 8/100\n",
      "38/38 [==============================] - 17s 447ms/step - loss: 5.3250 - accuracy: 0.0518\n",
      "Epoch 9/100\n",
      "38/38 [==============================] - 17s 445ms/step - loss: 5.2026 - accuracy: 0.0509\n",
      "Epoch 10/100\n",
      "38/38 [==============================] - 19s 489ms/step - loss: 5.0914 - accuracy: 0.0509\n",
      "Epoch 11/100\n",
      "38/38 [==============================] - 21s 548ms/step - loss: 4.9725 - accuracy: 0.0559\n",
      "Epoch 12/100\n",
      "38/38 [==============================] - 18s 461ms/step - loss: 4.8285 - accuracy: 0.0651\n",
      "Epoch 13/100\n",
      "38/38 [==============================] - 19s 488ms/step - loss: 4.6362 - accuracy: 0.0810\n",
      "Epoch 14/100\n",
      "38/38 [==============================] - 19s 510ms/step - loss: 4.4488 - accuracy: 0.0927\n",
      "Epoch 15/100\n",
      "38/38 [==============================] - 19s 496ms/step - loss: 4.2565 - accuracy: 0.0960\n",
      "Epoch 16/100\n",
      "38/38 [==============================] - 18s 458ms/step - loss: 4.1014 - accuracy: 0.1119\n",
      "Epoch 17/100\n",
      "38/38 [==============================] - 18s 463ms/step - loss: 3.8833 - accuracy: 0.1377\n",
      "Epoch 18/100\n",
      "38/38 [==============================] - 16s 424ms/step - loss: 3.6255 - accuracy: 0.1561\n",
      "Epoch 19/100\n",
      "38/38 [==============================] - 18s 467ms/step - loss: 3.4304 - accuracy: 0.1886\n",
      "Epoch 20/100\n",
      "38/38 [==============================] - 19s 490ms/step - loss: 3.1809 - accuracy: 0.2254\n",
      "Epoch 21/100\n",
      "38/38 [==============================] - 18s 476ms/step - loss: 2.9566 - accuracy: 0.2479\n",
      "Epoch 22/100\n",
      "38/38 [==============================] - 18s 484ms/step - loss: 2.6976 - accuracy: 0.2980\n",
      "Epoch 23/100\n",
      "38/38 [==============================] - 20s 527ms/step - loss: 2.3983 - accuracy: 0.3556\n",
      "Epoch 24/100\n",
      "38/38 [==============================] - 20s 514ms/step - loss: 2.2369 - accuracy: 0.3873\n",
      "Epoch 25/100\n",
      "38/38 [==============================] - 20s 514ms/step - loss: 2.0087 - accuracy: 0.4399\n",
      "Epoch 26/100\n",
      "38/38 [==============================] - 19s 497ms/step - loss: 1.8073 - accuracy: 0.5000\n",
      "Epoch 27/100\n",
      "38/38 [==============================] - 20s 535ms/step - loss: 1.5919 - accuracy: 0.5426\n",
      "Epoch 28/100\n",
      "38/38 [==============================] - 18s 467ms/step - loss: 1.3987 - accuracy: 0.6035\n",
      "Epoch 29/100\n",
      "38/38 [==============================] - 21s 544ms/step - loss: 1.2777 - accuracy: 0.6344\n",
      "Epoch 30/100\n",
      "38/38 [==============================] - 21s 541ms/step - loss: 1.1737 - accuracy: 0.6553\n",
      "Epoch 31/100\n",
      "38/38 [==============================] - 20s 531ms/step - loss: 1.0593 - accuracy: 0.6920\n",
      "Epoch 32/100\n",
      "38/38 [==============================] - 20s 537ms/step - loss: 0.9358 - accuracy: 0.7529\n",
      "Epoch 33/100\n",
      "38/38 [==============================] - 20s 520ms/step - loss: 0.8340 - accuracy: 0.7596\n",
      "Epoch 34/100\n",
      "38/38 [==============================] - 21s 541ms/step - loss: 0.7672 - accuracy: 0.7763\n",
      "Epoch 35/100\n",
      "38/38 [==============================] - 20s 528ms/step - loss: 0.7206 - accuracy: 0.7938\n",
      "Epoch 36/100\n",
      "38/38 [==============================] - 20s 514ms/step - loss: 0.6506 - accuracy: 0.8122\n",
      "Epoch 37/100\n",
      "38/38 [==============================] - 19s 491ms/step - loss: 0.6352 - accuracy: 0.8155\n",
      "Epoch 38/100\n",
      "38/38 [==============================] - 19s 511ms/step - loss: 0.6005 - accuracy: 0.8422\n",
      "Epoch 39/100\n",
      "38/38 [==============================] - 20s 518ms/step - loss: 0.5387 - accuracy: 0.8447\n",
      "Epoch 40/100\n",
      "38/38 [==============================] - 19s 493ms/step - loss: 0.5152 - accuracy: 0.8581\n",
      "Epoch 41/100\n",
      "38/38 [==============================] - 19s 509ms/step - loss: 0.4943 - accuracy: 0.8556\n",
      "Epoch 42/100\n",
      "38/38 [==============================] - 19s 497ms/step - loss: 0.4930 - accuracy: 0.8548\n",
      "Epoch 43/100\n",
      "38/38 [==============================] - 19s 505ms/step - loss: 0.4469 - accuracy: 0.8631\n",
      "Epoch 44/100\n",
      "38/38 [==============================] - 19s 491ms/step - loss: 0.4699 - accuracy: 0.8681\n",
      "Epoch 45/100\n",
      "38/38 [==============================] - 19s 511ms/step - loss: 0.4533 - accuracy: 0.8656\n",
      "Epoch 46/100\n",
      "38/38 [==============================] - 24s 624ms/step - loss: 0.4627 - accuracy: 0.8631\n",
      "Epoch 47/100\n",
      "38/38 [==============================] - 20s 517ms/step - loss: 0.5314 - accuracy: 0.8464\n",
      "Epoch 48/100\n",
      "38/38 [==============================] - 21s 540ms/step - loss: 0.4854 - accuracy: 0.8539\n",
      "Epoch 49/100\n",
      "38/38 [==============================] - 19s 504ms/step - loss: 0.4398 - accuracy: 0.8623\n",
      "Epoch 50/100\n",
      "38/38 [==============================] - 19s 479ms/step - loss: 0.4275 - accuracy: 0.8664\n",
      "Epoch 51/100\n",
      "38/38 [==============================] - 19s 495ms/step - loss: 0.4115 - accuracy: 0.8748\n",
      "Epoch 52/100\n",
      "38/38 [==============================] - 20s 509ms/step - loss: 0.3731 - accuracy: 0.8823\n",
      "Epoch 53/100\n",
      "38/38 [==============================] - 20s 518ms/step - loss: 0.3619 - accuracy: 0.8865\n",
      "Epoch 54/100\n",
      "38/38 [==============================] - 18s 480ms/step - loss: 0.3586 - accuracy: 0.8923\n",
      "Epoch 55/100\n",
      "38/38 [==============================] - 18s 476ms/step - loss: 0.3511 - accuracy: 0.8840\n",
      "Epoch 56/100\n",
      "38/38 [==============================] - 16s 413ms/step - loss: 0.3711 - accuracy: 0.8798\n",
      "Epoch 57/100\n",
      "38/38 [==============================] - 16s 411ms/step - loss: 0.3378 - accuracy: 0.8873\n",
      "Epoch 58/100\n",
      "38/38 [==============================] - 16s 413ms/step - loss: 0.3211 - accuracy: 0.8957\n",
      "Epoch 59/100\n",
      "38/38 [==============================] - 19s 508ms/step - loss: 0.3074 - accuracy: 0.8940\n",
      "Epoch 60/100\n",
      "38/38 [==============================] - 20s 520ms/step - loss: 0.2997 - accuracy: 0.8990\n",
      "Epoch 61/100\n",
      "38/38 [==============================] - 17s 453ms/step - loss: 0.2971 - accuracy: 0.8932\n",
      "Epoch 62/100\n",
      "38/38 [==============================] - 17s 461ms/step - loss: 0.3001 - accuracy: 0.8990\n",
      "Epoch 63/100\n",
      "38/38 [==============================] - 16s 429ms/step - loss: 0.2918 - accuracy: 0.8982\n",
      "Epoch 64/100\n",
      "38/38 [==============================] - 16s 417ms/step - loss: 0.2891 - accuracy: 0.8965\n",
      "Epoch 65/100\n",
      "38/38 [==============================] - 16s 421ms/step - loss: 0.2863 - accuracy: 0.8973\n",
      "Epoch 66/100\n",
      "38/38 [==============================] - 16s 420ms/step - loss: 0.2836 - accuracy: 0.8998\n",
      "Epoch 67/100\n",
      "38/38 [==============================] - 16s 426ms/step - loss: 0.2845 - accuracy: 0.8982\n",
      "Epoch 68/100\n",
      "38/38 [==============================] - 16s 422ms/step - loss: 0.2896 - accuracy: 0.8948\n",
      "Epoch 69/100\n",
      "38/38 [==============================] - 16s 428ms/step - loss: 0.2897 - accuracy: 0.8982\n",
      "Epoch 70/100\n",
      "38/38 [==============================] - 16s 415ms/step - loss: 0.2790 - accuracy: 0.8965\n",
      "Epoch 71/100\n",
      "38/38 [==============================] - 16s 420ms/step - loss: 0.2782 - accuracy: 0.8965\n",
      "Epoch 72/100\n",
      "38/38 [==============================] - 16s 418ms/step - loss: 0.2732 - accuracy: 0.8998\n",
      "Epoch 73/100\n",
      "38/38 [==============================] - 16s 421ms/step - loss: 0.2744 - accuracy: 0.8973\n",
      "Epoch 74/100\n",
      "38/38 [==============================] - 16s 420ms/step - loss: 0.2906 - accuracy: 0.8973\n",
      "Epoch 75/100\n",
      "38/38 [==============================] - 16s 421ms/step - loss: 0.3308 - accuracy: 0.8856\n",
      "Epoch 76/100\n",
      "38/38 [==============================] - 16s 421ms/step - loss: 0.3465 - accuracy: 0.8823\n",
      "Epoch 77/100\n",
      "38/38 [==============================] - 16s 429ms/step - loss: 0.4224 - accuracy: 0.8656\n",
      "Epoch 78/100\n",
      "38/38 [==============================] - 17s 438ms/step - loss: 0.4068 - accuracy: 0.8606\n",
      "Epoch 79/100\n",
      "38/38 [==============================] - 16s 430ms/step - loss: 0.4326 - accuracy: 0.8539\n",
      "Epoch 80/100\n",
      "38/38 [==============================] - 16s 425ms/step - loss: 0.4740 - accuracy: 0.8472\n",
      "Epoch 81/100\n",
      "38/38 [==============================] - 16s 426ms/step - loss: 0.4428 - accuracy: 0.8472\n",
      "Epoch 82/100\n",
      "38/38 [==============================] - 16s 419ms/step - loss: 0.3961 - accuracy: 0.8614\n",
      "Epoch 83/100\n",
      "38/38 [==============================] - 16s 421ms/step - loss: 0.3558 - accuracy: 0.8781\n",
      "Epoch 84/100\n",
      "38/38 [==============================] - 16s 418ms/step - loss: 0.3269 - accuracy: 0.8881\n",
      "Epoch 85/100\n",
      "38/38 [==============================] - 16s 416ms/step - loss: 0.3262 - accuracy: 0.8881\n",
      "Epoch 86/100\n",
      "38/38 [==============================] - 16s 421ms/step - loss: 0.2859 - accuracy: 0.8965\n",
      "Epoch 87/100\n",
      "38/38 [==============================] - 16s 425ms/step - loss: 0.2718 - accuracy: 0.8973\n",
      "Epoch 88/100\n",
      "38/38 [==============================] - 16s 425ms/step - loss: 0.2738 - accuracy: 0.8965\n",
      "Epoch 89/100\n",
      "38/38 [==============================] - 16s 425ms/step - loss: 0.2739 - accuracy: 0.8957\n",
      "Epoch 90/100\n",
      "38/38 [==============================] - 16s 425ms/step - loss: 0.2660 - accuracy: 0.9032\n",
      "Epoch 91/100\n",
      "38/38 [==============================] - 16s 428ms/step - loss: 0.2643 - accuracy: 0.8957\n",
      "Epoch 92/100\n",
      "38/38 [==============================] - 16s 424ms/step - loss: 0.2538 - accuracy: 0.8973\n",
      "Epoch 93/100\n",
      "38/38 [==============================] - 16s 416ms/step - loss: 0.2610 - accuracy: 0.9007\n",
      "Epoch 94/100\n",
      "38/38 [==============================] - 16s 418ms/step - loss: 0.2578 - accuracy: 0.8973\n",
      "Epoch 95/100\n",
      "38/38 [==============================] - 16s 422ms/step - loss: 0.2585 - accuracy: 0.8982\n",
      "Epoch 96/100\n",
      "38/38 [==============================] - 16s 420ms/step - loss: 0.2581 - accuracy: 0.8982\n",
      "Epoch 97/100\n",
      "38/38 [==============================] - 16s 418ms/step - loss: 0.2565 - accuracy: 0.8973\n",
      "Epoch 98/100\n",
      "38/38 [==============================] - 16s 418ms/step - loss: 0.2596 - accuracy: 0.8990\n",
      "Epoch 99/100\n",
      "38/38 [==============================] - 16s 420ms/step - loss: 0.2545 - accuracy: 0.9032\n",
      "Epoch 100/100\n",
      "38/38 [==============================] - 16s 419ms/step - loss: 0.2597 - accuracy: 0.8982\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "i am feeling good today so much with the thought of selling was said a thousand again said a few his cup again said a\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAiMAAAGdCAYAAADAAnMpAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAAA+gklEQVR4nO3deXxU9b3/8fdMkpkkJJmEhCQkBALIvkMgBhd6aypUq2K1RauCXKs/rbYov94qLnhbq2hbubSVyk9v0d7rAtW61QWqcUWQJRBkTdgTAtkIyWSfZOb8/ggMRrYEkpxZXs/HYx6P5uScmU++pDlvv9uxGIZhCAAAwCRWswsAAADBjTACAABMRRgBAACmIowAAABTEUYAAICpCCMAAMBUhBEAAGAqwggAADBVqNkFtIfH49GhQ4cUHR0ti8VidjkAAKAdDMNQTU2NUlJSZLWevv/DL8LIoUOHlJaWZnYZAADgHBQVFalPnz6n/b5fhJHo6GhJrT9MTEyMydUAAID2cDqdSktL897HT8cvwsjxoZmYmBjCCAAAfuZsUyyYwAoAAExFGAEAAKYijAAAAFMRRgAAgKkIIwAAwFSEEQAAYCrCCAAAMBVhBAAAmIowAgAATEUYAQAApiKMAAAAUxFGAACAqQgjAAB0glJno/6cs0ubi6rMLuWUPB5D6/dX6s85u/RJfpk8HsPskrz84qm9ABBMDMNQcVWDDlc3KjU2Qskx4bJaz/zU0/PR2OzW9sNOJceEq7cj/KxPWEVbHo+hZeuLtOCDHappbNHCjwp06+R0/fLyIephP/NttqiyXp/ml6m4qlET0+OUNTBekbbOvTUXlNborU3FejvvkIqrGrzH+yf00Mysfrp+Qh9Fh4d16md2lMUwDN+JRqfhdDrlcDhUXV2tmJgYs8sBEGTqXS16Y2Ox/r6hSGEhVt1+SX9dPjy5TUAwDENf7KrQ4k92a1dZrRwRYXJEhCkuMkyxkTalxkaoX3yk+if0UL/4HurZw6aaxmZV1TerqqFZlXVN2n7IqbyiKuUVVauitsn73rZQq/r1jFR6Qg+NSnXoO0N6aWSKo83nN7s9yj1wVF/sKle9y630+B5KT+ih/vE9lBIbrtCQth3hB47U6bOCcn2aX641e46oodktSUqMtmtMWqzGpsVqUGKUQr7xGVarRRPTeyrqLDdYf2QYhmqaWlRdf/zfxCWLLIqNbP13jI0MU5Q99KSgtqe8VvPe2KJ1+yolSamxEd4bfmpshH577Uj925BESVKL26PiqgbtKa/Vql1H9GlBmfaW17V5P1uoVZn9e2rK4F7qFW1vreVYPbWNLfr2DTs8zKrYCJu3zghbiA5VNWj/kXrtr6jT/oo6Hapu9J4fZQ/VhQPitXbvEdU0tUiSethCdN2EPvrpxQPUNz6yM5u13fdvwgiAoFfb1KL//mKvjta51C++h9ITItUvvocskl5ZW6jlG4pU09jS5pqhydGac9kgTR2RrC92V2jRRwXaVFjVaTWFWi1KdoSrpLpRLafoTo/vYdOlg3tpREqMNuw/qi93V3hvLt9mtUih1hNhxJChZnfb94yLDJOzsUXus3Td94mL0Ku3X6i0np170zJLY7Nbv1uRr5fWHpCrxXPGc0OtJ8JJXKRNUeGhWr3niFwtHkXaQvTLy4do1uR0fbm7Qg++uUUHj7aGknF9Y1VV36yiyvqT/i1DrBZN6BunfvGRWr3nSJuei84SFmLRlMGJmj4uRdnDkhQeFqK6pha9salYf1u9X7vLaiVJ//Pvk3Tp4F6d+tmEEQABz9Xi0eo9FRrdJ1Y9e9jO6T02FR7VvcvzdOBI/RnP6xcfqZlZ6aqqd+mFL/er9tiNPy4yTEfrmyVJ9lCrbsrsp2vHpaqh2a2qepeq6ptVWe9SUWW99h+p0/6Keh2qbtDxv7xR9lDvf3kP7BXl7ZUYkRKj8LAQtbg9OlTVqH1H6rS3vFZf7T2iL3cf8X7+Nx0PKAlRNh040vp5B47Uq+kUN9lQq0UT+sXpO0MS9Z0hvTQ0OVqNzR5tO1StvKIqbSqq8t5Mjys+Wq+KWpdSYyO07A7/DyQ7S5ya82qe8ktrvMeO9zQ4IlqHLaoaXDpa33zGoDJlcC/9dvrINu1R72rRwn8VaOmX+/TN/GEPtapffKTGpcXpO0N66aJBCYo5NkRiGIb2lNfq0/xyrdpdIVeL51j4ae35iA4PldXyzd641jBV3dCsqvrWOuuaWtQ7NkLp8a2BOj0+UoOSor0/z7cZhqHVe47og62H9ZurR3b6cCBhBEDAampx67UNB/Xsp3tUXNWg+B42/f5Ho/XdoUntfg+3x9BfPtmtRTm75PYYSo2N0JWje6uosl77Klpv4g3Nbk0Z3Eu3Tk7XlMG9vH+oq+pdWrpqn174cr9qmloUHmbVzZn9dMeUAUqMDm9X/TWNLXJEhCkspOPrCFwtHm0sPKpP88tVUFqjMX1i9Z0hvTQq1XHSzcTjMVRR13RSj0dMeNhZ5zN8W0l1o258/ivtq6jz60Di8Rh6cfV+Pblip1wtHiVE2fTkD0fr4kEJCg8LOeU1jc1uHT0WLqvqm1V9LKT0doRryuBep51ns7PEqa8PVqtPXITS43t0+fwfX0MYARBwmlrc+vuGg3r2k93ecfAQq8V7o52Z1U8PXjHstDeU44oq6zX373lav/+oJOmqMSn67fSRbf7r0TAMNbV4zvhe1fXNyi2s1KjUWPWKtp/vj+cX/C2QvJ1XrNdzD7YJY5V1Lu0sae0NuWxoop66frQSooLj36+7EUYABATDMJR74KjeyivWe18f9g6JJMXYddeUgbp2fB/9KWeX/rpqnyRpUGKUFt0wViNSHCe9l9tj6IUv9+npfxWoodmtKHuoHps+QtPHprKCpANKnY268bmvtLeiTr0d4br5wn76zpBeGt47xmfa0eMx9LuV+Vry2Z5Tft8eatXDPxiumzP7+kzNgYgwAsCvVda1DoW8lVfcZu5Ccky47vrOQM2YmNam1+KzgnL98rXNKq9pktUiTR6YoGvGpmjayGRFh4dp+yGn5r3xtTYfrJYkZfbvqT/8aIxP/1e9L/tmIDmuV7RdUwb30qysdI3qc3IY7C4NLrfuW56nFdtKJEk/vbj/SfVM6BenPnH823c1wggAv2QYht7OO6TfvLtdlXUuSa1LD6eOTNb0samaPDD+pGWqxx2pbdKDb27Rym2l3mP2UKsm9e+pNXuOqMVjKDo8VA9dMUw/zkgLqrH7ruBsbNbbeYf0WX6Zvtx9YnlwpC1E7//iEqUn9Oj2msqcjfrp/2zQ1werZQux6qnrR+nacX26vQ60IowA8DsHj9broTe36rOCckmty2fv/rcLlD0sSRG2M88D+abCI/V6O69Yb+UVa8839nH4/shk/frqEUqMOfskU3RMU4tb6/cd1dMf5mtTYZXGpsXq9TuzThscu8Lh6gZd/+waFVc1KC4yTP/vlgxN6t+z2z4fJyOMAPAbHo+hv63Zr9+vzFe9yy1bqFVzLhukOy4dcE6rTY4zDEPbDjn1WUG5hqfEeDefQtcprmrQtEWfq6axRfdmD9K92YO75XM9HkM3/fdardl7RP0TeuiFWyea0jODttp7/w68bfQA+JVSZ6N++dpmfbGrQpI0qX9PLfjhKA3sFXXe722xWDQy1aGRqebNXwg2qbER+u30kZqzLE9//ni3Lh3cS+P7xnX55/73qr1as/eIIsJCtJQg4nd4UB4A06zcVqJpiz7XF7sqFB5m1WPXjNCy2y/slCAC81wzNlVXj0mR22PovuV5qjvNzrCdZfshp36/Ml+SNP+q4epPEPE79IwAQa6x2a1/bj6kl9YWqqS6QSNTHN5dQMekxZ5258bzcbTOpd+tzNer6wolSSNSYvTHG8bpgkRCSKB47JqR2rC/UgeO1Ouxd7fryetGd8nnNDa7de/yTWp2G8oelqQbJqZ1yeegaxFGgCB1uLpBL311QK+uK/KuWpGkUmeZcnaWeb8ekNDDG0zGpsVqaO9o2UPbP5n0uAaXWx/tKNXbecX6rKBczW5DFot0x6UD9H+/N0S2UDpqA4kjMkx/+PEY3fTfa7VsfZFcLR7delG6RveJ7dTPeWrFThWU1iohyq6nrhvFniF+igmsQJBpdnv055xdWvzpHu+ulCmOcN2c1U8T03tqy8FqbT5YpbyiqlM+r6VXtF0v3ZapIcnR7fq8qnqXnlqRr3fyilXncnuPj0iJ0UNXDNPkCxI65weDT1r4YYH+lLPL+/W4vrG6dXK6vj+y93kH0M8LyjVz6TpJ0gu3TtS/DWWCsq9hNQ0QJFwtHlksateqk/0VdZqzPE+bi6oktW78NfuidGUPSzrlEszKOldrMCms0uaDVdpUWKXqhmYNTY7W2/dcdNYektV7KjR3+WaVOFu3bu8TF6HpY1N1zdgUDUpqX5iB/8srqtLfVu/Xu18f8j4tODU2Qvd/f6iuGt37nHozDh6t1/TFX6qi1qVbLuynx6aP7Oyy0QkII0AQ2F1Woxue+0o1jS0amerwDqeMTnUoMcauiLAQWSwWGYah13IP6j/f2aZ6l1sx4aF6/NpRumpMSoc+r7ymSdMWfa4jdS7dcekAPXjFsFOe52rx6OkP8/Xc53tlGFL/hB56/NqRyhoQTzd6ECuradSra4v00toDKq9pkiSN7xurR34wXOM6sOKmrqlF1y9Zox2HnRrWO0Zv3DW5Q/vQoPsQRoAAV+Zs1LV/Wa3iqobTnmMLscoRGaaIsBAVVrYOuWT276n/mjFWKbER5/S5H24v1e3/s0EWi/TybZknDbPsLqvRvcvztLXYKUm6YWKaHvnB8A4/IRaBq7HZrec/36tnP9uj+mNDd9PHpuhX04ae9ffS4zF018u5WrmtVAlRNr19z8VKPcffZXQ9wggQwOqaWjTjuTXaWuxU/4Qe+uMNY7W7rFZ5Ra1zPXYerpHL7WlzTajVov97+RDdcekAhZznNujz3tiiV9cVqrcjXCvmXCpHZJhcLR4t+WyPnvl4t1xuj2Ijw/TkD0dr2sjk8/osBK5SZ6N+vzJfr+celCSFh1l1x6UDdeeUAYq0nTq8LvxXvv708W7ZQqx69Y5MTejHDqu+jDACBKgWt0c//Z8N+jS/XPE9bHrjZ5PVL77tvgqGYaje5VZVQ7Oq6l2qqm9W356RnfZQuHpXi6780yrtq6jTD0b31uyL+mveG1+roLRWkvSdIb301HWjlcS262iHLQer9di727Vuf6Wk1icy3z9tqKaPTW3z/KB/bj6kn7+6SZL0hx+N0fUTeOaMryOMAAHIMAzNe2OLlq0vUniYVcvuyNLYtFhTaskrqtJ1z672rsiRpPgeNs2/ariuHpPC3BB0iGEY+mBriRZ8sENFla1DjwlRdtm/seKmrKZRzW5D/+fSAZp3mvlK8C1sBw8EAMMwVOJsVF5hlfIOVmnD/qPKPXBUVov0zI3jTQsikjQ2LVZzLhukhR8WSJKuG99HD185THE9bKbVBP9lsVh0xaje+u7QRL24er+e+Xi3KmqbTjrv8uFJ+tW0oSZUiK5Ezwjgo8pqGnXD//tKeyvq2hy3WFp3t7z5wn4mVXaC22Po9dwipcf3UOaAeLPLQQCpbmjW/m/97tvDrBqSFE2vmx+hZwTwcy9+uV97K+oUYrVoSFK0xvaN1dg+sZrYv6fPPHsjxGrRjIl9zS4DAcgREaYxJvb8oXsRRgAf1OBy65Vjz21Z/JPxrEgBENB4GATgg/6x8aB3Bcz3hieZXQ4AdCnCCOBjPB5DL3y5T5J06+T0894TBAB8HWEE8DGf7SrXnvI6RdlD9aMM9lEAEPgII4CPWbqqtVdkxsQ0RYeHmVwNAHQ9wgjgQwpKa/TFrgpZLa1DNAAQDAgjgA853ity+fDkTtu6HQB8HWEE8BFHapv0xqZiSdJtl/Q3uRoA6D7sMwL4gKLKei3+ZLdcLR6NSnUoo1+c2SUBQLchjAAmWbv3iFZuK9WnBWXaW35i2+ufXtKf7a4BBBXCCGCCv68v0q/+8bX36xCrRRP6xunqsSm6ekyKiZUBQPc7pzkjixcvVnp6usLDw5WZmal169ad8fxFixZpyJAhioiIUFpamu677z41NjaeU8GAvytzNuqx97ZLkqaNSNZfbhqvjY98T3+/M0s3X9iPXhEAQafDPSPLly/X3LlztWTJEmVmZmrRokWaOnWq8vPzlZiYeNL5r7zyih544AEtXbpUkydPVkFBgW699VZZLBYtXLiwU34IwJ88+s421TS2aEwfhxbfNJ4dVgEEvQ73jCxcuFC33367Zs+ereHDh2vJkiWKjIzU0qVLT3n+6tWrddFFF+knP/mJ0tPTdfnll+vGG288a28KEIhWbivRB1tLFGK1aMEPRxNEAEAdDCMul0u5ubnKzs4+8QZWq7Kzs7VmzZpTXjN58mTl5uZ6w8fevXv1/vvv64orrjjt5zQ1NcnpdLZ5Af7O2dis+W9vlST9n0sHaHhKjMkVAYBv6NAwTUVFhdxut5KS2j5FNCkpSTt37jzlNT/5yU9UUVGhiy++WIZhqKWlRXfeeacefPDB037OggUL9Otf/7ojpQE+76kPdqrU2aT+CT30i8sGmV0OAPiMLt/07NNPP9UTTzyhv/zlL9q4caPeeOMNvffee3rsscdOe828efNUXV3tfRUVFXV1mUCXWrevUi+vLZQkLfjhKIWHhZhcEQD4jg71jCQkJCgkJESlpaVtjpeWlio5OfmU1zzyyCO65ZZb9NOf/lSSNGrUKNXV1emOO+7QQw89JKv15Dxkt9tlt9s7Uhrgsxqb3XrgjdZlvDdMTNOFA+JNrggAfEuHekZsNpsmTJignJwc7zGPx6OcnBxlZWWd8pr6+vqTAkdISOt/FRqG0dF6Ab+z8MMC7S2vU2K0XfO+P8zscgDA53R4ae/cuXM1a9YsZWRkaNKkSVq0aJHq6uo0e/ZsSdLMmTOVmpqqBQsWSJKuuuoqLVy4UOPGjVNmZqZ2796tRx55RFdddZU3lACBKvdApZ7/Yq+k1uEZR2SYyRUBgO/pcBiZMWOGysvLNX/+fJWUlGjs2LFasWKFd1JrYWFhm56Qhx9+WBaLRQ8//LCKi4vVq1cvXXXVVXr88cc776cAfFCDy61fvva1DEO6bnwfXTYs6ewXAUAQshh+MFbidDrlcDhUXV2tmBiWQ8I/PPbudv111T4lxdj1r/umyBFBrwiA4NLe+3eXr6YBgtG6fZVa+uU+SdKT140miADAGRBGgE5W72rRr17fLMOQfpzRR/825OTHJAAATiCMAJ3syQ92av+RevV2hOvhHww3uxwA8HmEEaATfbKzTP+z5oAk6anrRismnOEZADgbwgjQSSpqm/Qfr2+WJM2+KF2XDu5lckUA4B8II0AnMAxDD/xjiypqXRqcFKX7pw01uyQA8BuEEaATLFtfpI92lMoWYtWiGeN49gwAdABhBDhP+yrq9Jt/bpck/XLqYA1PYS8cAOgIwghwHrYWV+vO/81VQ7NbWQPi9dOLB5hdEgD4nQ5vBw9AKnM26vcr8/X6xoMyDCkuMkxP/3iMrFaL2aUBgN8hjAAd0Oz26LnP92rxJ7tV73JLkq4Zm6L7pw1VSmyEydUBgH8ijAAd8Nzne/X7lfmSpHF9Y/XID4ZrfN84k6sCAP9GGAE64POCcknSLy4bpPuyB8liYVgGAM4XE1iBdvJ4DG0/5JQkTRuRTBABgE5CGAHaqehovWqaWmQLsWpQUpTZ5QBAwCCMAO207VivyJDkaIWF8H8dAOgs/EUF2mnboWpJ0gg2NQOATkUYAdppa3FrzwhhBAA6F2EEaKfjwzQjUh0mVwIAgYUwArRDmbNRFbVNslqkYcn0jABAZyKMAO2w9dh8kQG9ohRh44m8ANCZCCNAO2w7Nl9kJPNFAKDTEUaAdvDOF0lhvggAdDbCCNAOW1nWCwBdhjACnEV1fbMOHm2QRM8IAHQFwghwFtsOt/aK9ImLkCMyzORqACDwEEaAszgxeZVeEQDoCoQR4CzYBh4AuhZhBDiLEzuvEkYAoCsQRoAzaHC5tae8VhLDNADQVQgjwBnsKHHKY0gJUXYlxoSbXQ4ABCTCCHAGJzY7Y4gGALoKYQQ4g23FrZNXRzJfBAC6DGEEOAO2gQeArkcYAU6j2e1RfkmNJIZpAKArEUaA09hcVCWX2yNHRJjS4iLNLgcAAhZhBDiNFVtLJEnfHZooq9VicjUAELgII8ApGIahFdtaw8jUEckmVwMAgY0wApzCtkNOHTzaoPAwq6YM7mV2OQAQ0AgjwCn861ivyJTBvRRhCzG5GgAIbIQR4BSOD9FMG8kQDQB0NcII8C17ymtVUFqrUKtF3x2aZHY5ABDwCCPAt6w81isy+YIEOSLCTK4GAAIfYQT4lpXHlvROYxUNAHQLwgjwDcVVDdp8sFoWi/S94QzRAEB3IIwA33B8FU1Gvzj1irabXA0ABAfCCPANx3ddZaMzAOg+hBHgmCO1TVq/v1ISYQQAuhNhBDjmox2l8hjSyNQYpfXkwXgA0F0II4Ban0XzyroiSdLU4fSKAEB3IowAkv759WFtLqpSpC1EMyammV0OAAQVwgiCXmOzW099sFOSdOeUgUqMCTe5IgAILoQRBL2lX+5TcVWDejvCdfslA8wuBwCCDmEEQa28pkl/+WSPJOlX04bwhF4AMAFhBEHtvz4qUG1Ti0b3ceiaMalmlwMAQYkwgqCVX1KjZesKJUkPXzlcVqvF5IoAIDgRRhC0Hn9/hzyG9P2RyZrUv6fZ5QBA0CKMICit31+pzwvKZQux6oHvDzW7HAAIaoQRBKXl61s3OJs+LkX94nuYXA0ABDfCCIJOXVOL3t9yWJL04ww2OAMAsxFGEHTe23JY9S63BiT00IR+cWaXAwBBjzCCoPPahtYhmusz+shiYQUNAJiNMIKgsre8Vuv3H5XVIl03vo/Z5QAARBhBkHk996Ak6dLBvZTEM2gAwCcQRhA03B5Db2wslsTEVQDwJYQRBI0vdpWrxNmo2MgwXTYs0exyAADHEEYQNF7b0DpEM31squyhPBAPAHzFOYWRxYsXKz09XeHh4crMzNS6devOeH5VVZXuvvtu9e7dW3a7XYMHD9b7779/TgUD56Kq3qUPt5dKkn6UwcRVAPAloR29YPny5Zo7d66WLFmizMxMLVq0SFOnTlV+fr4SE0/u+na5XPre976nxMREvf7660pNTdWBAwcUGxvbGfUD7fJ23iG53B4N7x2jESkOs8sBAHxDh8PIwoULdfvtt2v27NmSpCVLlui9997T0qVL9cADD5x0/tKlS1VZWanVq1crLCxMkpSenn5+VQMd9M7mQ5LoFQEAX9ShYRqXy6Xc3FxlZ2efeAOrVdnZ2VqzZs0pr3nnnXeUlZWlu+++W0lJSRo5cqSeeOIJud3u035OU1OTnE5nmxdwrlrcHm0trpYkTRncy+RqAADf1qEwUlFRIbfbraSkpDbHk5KSVFJScspr9u7dq9dff11ut1vvv/++HnnkET399NP67W9/e9rPWbBggRwOh/eVlsYyTJy7PeV1amrxqIctROk8FA8AfE6Xr6bxeDxKTEzUc889pwkTJmjGjBl66KGHtGTJktNeM2/ePFVXV3tfRUVFXV0mAti2Q629IsNTYmS1sv07APiaDs0ZSUhIUEhIiEpLS9scLy0tVXJy8imv6d27t8LCwhQScmIp5bBhw1RSUiKXyyWbzXbSNXa7XXa7vSOlAae1tbh1mI+JqwDgmzrUM2Kz2TRhwgTl5OR4j3k8HuXk5CgrK+uU11x00UXavXu3PB6P91hBQYF69+59yiACdLbjPSMjUmJMrgQAcCodHqaZO3eunn/+ef3tb3/Tjh07dNddd6murs67umbmzJmaN2+e9/y77rpLlZWVmjNnjgoKCvTee+/piSee0N133915PwVwGh6Poe2H6BkBAF/W4aW9M2bMUHl5uebPn6+SkhKNHTtWK1as8E5qLSwslNV6IuOkpaVp5cqVuu+++zR69GilpqZqzpw5uv/++zvvpwBOo+hovWqaWmQLsWpQUpTZ5QAATsFiGIZhdhFn43Q65XA4VF1drZgYutrRfu9vOayfvbxRo1Id+ufPLza7HAAIKu29f/NsGgQ05osAgO8jjCCgnVhJQxgBAF9FGEHAMgzjRM9IKpNXAcBXEUYQsMpqmlRR65LVIg1LpmcEAHwVYQQB63ivyMBeUYqwhZzlbACAWQgjCFjbmC8CAH6BMIKAtdW7kob5IgDgywgjCFjbju+8mkrPCAD4MsIIAlJ1fbMOHm2QJI3oTc8IAPgywggC0vHJq33iIuSIDDO5GgDAmRBGEJCOD9GMZL4IAPg8wggCEtvAA4D/IIwgIG1l8ioA+A3CCAJOg8utveW1khimAQB/QBhBwNlR4pTHkBKi7EqMCTe7HADAWRBGEHBy9x+VJI1iiAYA/AJhBAHn813lkqSLLkgwuRIAQHsQRhBQGpvdWrevUpI0ZXAvk6sBALQHYQQBZe2+SjW1eJQcE64LEqPMLgcA0A6EEQSULwpah2guHZwgi8VicjUAgPYgjCCgHJ8vcskghmgAwF8QRhAwSqobVVBaK4tFupjJqwDgNwgjCBjHe0VG94lVXA+bydUAANqLMIKA8fnx+SKD6BUBAH9CGEFAcHsMrdpdIUm6lCW9AOBXCCMICFuLq1VV36xoe6jGpsWaXQ4AoAMIIwgIx4dosgbGKyyEX2sA8Cf81UZA+GIXQzQA4K8II/B7NY3N2ljY+nA8toAHAP9DGIHfW7PniFo8htLjI5XWM9LscgAAHUQYgd87vr8IQzQA4J8II/B7q47NF2ELeADwT4QR+LXq+mbtP1IvSZqU3tPkagAA54IwAr+27VC1JCmtZ4QckWEmVwMAOBeEEfi1rcfCyMgUh8mVAADOFWEEfm1rsVOSNDKVMAIA/oowAr/m7RkhjACA3yKMwG/VNrVoX0WdJGlESozJ1QAAzhVhBH5rx2GnDEPq7QhXQpTd7HIAAOeIMAK/tbW4dYhmBJNXAcCvEUbgt05MXmWIBgD8GWEEfmsby3oBICAQRuCXGpvd2lVWK4mVNADg7wgj8Es7S2rk9hhKiLIpKYbJqwDgzwgj8EvfnLxqsVhMrgYAcD4II/BL3vkiTF4FAL9HGIFf8q6kYfIqAPg9wgj8jqvFo/ySGklMXgWAQEAYgd8pKK2Ry+1RTHio+sRFmF0OAOA8EUbgd7Z94+F4TF4FAP9HGIHfObHzKkM0ABAICCPwO1sPHV/Wy0oaAAgEhBH4lRa3RzsO0zMCAIGEMAK/sreiTo3NHvWwhah/fA+zywEAdALCCPzK8Z1Xh6fEyGpl8ioABALCCPzK5qIqSQzRAEAgIYzAr2wsrJIkje8bZ24hAIBOQxiB36h3tWj7scmr4/sRRgAgUBBG4De+Plgtt8dQcky4UhzhZpcDAOgkhBH4jY2FRyVJ4/vFsvMqAAQQwgj8xsYDVZKYLwIAgYYwAr9gGIY2HesZGUcYAYCAQhiBXyisrNeROpdsIVaNTGUbeAAIJIQR+IXcA629IiNTY2QPDTG5GgBAZyKMwC94J68yRAMAAYcwAr/gnbzK/iIAEHDOKYwsXrxY6enpCg8PV2ZmptatW9eu65YtWyaLxaLp06efy8ciSNU1tWhnybHNzugZAYCA0+Ewsnz5cs2dO1ePPvqoNm7cqDFjxmjq1KkqKys743X79+/XL3/5S11yySXnXCyC0+aDVfIYUoojXMlsdgYAAafDYWThwoW6/fbbNXv2bA0fPlxLlixRZGSkli5detpr3G63brrpJv3617/WgAEDzqtgBJ9Nx55HM44hGgAISB0KIy6XS7m5ucrOzj7xBlarsrOztWbNmtNe95vf/EaJiYm67bbbzr1SBK3jK2kmMEQDAAEptCMnV1RUyO12Kykpqc3xpKQk7dy585TXrFq1Sn/961+Vl5fX7s9pampSU1OT92un09mRMhFAvrnZGZNXASAwdelqmpqaGt1yyy16/vnnlZCQ0O7rFixYIIfD4X2lpaV1YZXwZfsq6nS0vln2UKuG92azMwAIRB3qGUlISFBISIhKS0vbHC8tLVVycvJJ5+/Zs0f79+/XVVdd5T3m8XhaPzg0VPn5+Ro4cOBJ182bN09z5871fu10OgkkQWrjsfkio1IdsoWyEh0AAlGHwojNZtOECROUk5PjXZ7r8XiUk5Oje+6556Tzhw4dqi1btrQ59vDDD6umpkZ//OMfTxsw7Ha77HZ7R0pDgNrIEA0ABLwOhRFJmjt3rmbNmqWMjAxNmjRJixYtUl1dnWbPni1JmjlzplJTU7VgwQKFh4dr5MiRba6PjY2VpJOOA6ey8QA7rwJAoOtwGJkxY4bKy8s1f/58lZSUaOzYsVqxYoV3UmthYaGsVrrTcf7KnI3KL62RJI3vF2tuMQCALmMxDMMwu4izcTqdcjgcqq6uVkwMkxiDxXOf79ET7+/U+L6xeuNnF5ldDgCgg9p7/6YLAz7JMAz9I7dYknTdhD4mVwMA6EqEEfikbYecyi+tkS3Uqh+MTjG7HABAFyKMwCf9Y+NBSdL3hifJERFmcjUAgK5EGIHPaXZ79E7eIUnS9eMZogGAQEcYgc/5NL9cR+pcSoiy65JB7d+5FwDgnwgj8Dn/yG0dopk+NkWhIfyKAkCg4y89fEpVvUs5O1sfN8AqGgAIDoQR+JR/bj6kZreh4b1jNIwH4wFAUCCMwKe8vpG9RQAg2BBG4DN2l9Vqc1GVQqwWXT2GvUUAIFgQRuAz3tncupz3O4N7qVc0T20GgGBBGIHPyD1QKUnKHp5kciUAgO5EGIFPMAxDXx+sliSN7uMwuRoAQHcijMAnHDhSr5rGFtlCrRqcFG12OQCAbkQYgU/4uri1V2R47xiFsdEZAAQV/urDJ2w5WCWJIRoACEaEEfiEzd75IrHmFgIA6HaEEZjO7TG0rZjJqwAQrAgjMN2+ilrVudyKCAvRwF5RZpcDAOhmhBGY7viS3pGpMQqxWkyuBgDQ3QgjMN3xMDIqNdbcQgAApiCMwHRfs5IGAIIaYQSmanF7tO2QUxJhBACCFWEEptpVVqumFo+i7aFKj+9hdjkAABMQRmCqLd7Jqw5ZmbwKAEGJMAJTfV1cJYkhGgAIZoQRmMq7koYwAgBBizAC0zS1uLXjcOvk1TFsAw8AQYswAtMUlNSq2W0oNjJMfeIizC4HAGASwghMc3y+yKhUhywWJq8CQLAijMA0Ww7ycDwAAGEEJtrMNvAAABFGYJLGZrcKSmsk0TMCAMGOMAJTbCmulttjqFe0Xb0d4WaXAwAwEWEEpli/v1KSNDE9jsmrABDkCCMwxYb9RyVJGf16mlwJAMBshBF0O4/H0AZvzwhhBACCHWEE3a6grEbOxhZF2kI0rHe02eUAAExGGEG3W39siGZ83ziFhvArCADBjjsBut3xIZqM9DiTKwEA+ALCCLrd8cmrzBcBAEiEEXSz4qoGFVc1KMRq0di0WLPLAQD4AMIIutXxIZoRKTHqYQ81uRoAgC8gjKBbsb8IAODbCCPoVt/ceRUAAIkwgm5U3dCs/GMPx5tAGAEAHEMYQbfZWHhUhiGlx0cqMZqH4wEAWhFG0G1O7C/CfBEAwAmEEXSb9d79RRiiAQCcQBhBt2hqcWtzUZUkekYAAG0RRtAtthZXq6nFo549bBqQ0MPscgAAPoQwgm6x3ru/SJwsFovJ1QAAfAlhBF3OMAyt2FoiiefRAABORhhBl/t4Z5nyiqoUHmbVNWNTzC4HAOBjCCPoUh6Pod+vzJckzZqcrsQY9hcBALRFGEGXem/LYe0sqVG0PVR3XjrQ7HIAAD6IMIIu0+L26L8+LJAk/fSSAYrrYTO5IgCALyKMoMu8sbFYeyvq1LOHTbdd0t/scgAAPoowgi7R1OLWH3N2SZLumjJQUfZQkysCAPgqwgi6xLJ1RSqualBSjF23ZPUzuxwAgA8jjKDTNbjc+vPHuyVJP//uIIWHhZhcEQDAlxFG0On+tb1EFbVNSo2N0I8z0swuBwDg4wgj6HQrt7Xutjp9XIpsofyKAQDOjDsFOlVjs1uf7CyXJE0b0dvkagAA/oAwgk71eUG5GprdSo2N0MjUGLPLAQD4AcIIOtXKbaWSpKkjknk6LwCgXQgj6DTNbo8+2nE8jCSZXA0AwF+cUxhZvHix0tPTFR4erszMTK1bt+605z7//PO65JJLFBcXp7i4OGVnZ5/xfPivtXsrVd3QrPgeNmWk9zS7HACAn+hwGFm+fLnmzp2rRx99VBs3btSYMWM0depUlZWVnfL8Tz/9VDfeeKM++eQTrVmzRmlpabr88stVXFx83sXDt6zYdliSdPmIJIVYGaIBALSPxTAMoyMXZGZmauLEiXrmmWckSR6PR2lpafr5z3+uBx544KzXu91uxcXF6ZlnntHMmTPb9ZlOp1MOh0PV1dWKiWFSpC/yeAxlLshReU2TXpw9Ud8Zkmh2SQAAk7X3/t2hnhGXy6Xc3FxlZ2efeAOrVdnZ2VqzZk273qO+vl7Nzc3q2fP03fhNTU1yOp1tXvBtm4qOqrymSdH2UE0emGB2OQAAP9KhMFJRUSG3262kpLaTE5OSklRSUtKu97j//vuVkpLSJtB824IFC+RwOLyvtDR28fR1x1fRfHdYIhudAQA6pFvvGk8++aSWLVumN998U+Hh4ac9b968eaqurva+ioqKurFKdJRhGFqxtTWMThuRbHI1AAB/06HnuickJCgkJESlpaVtjpeWlio5+cw3oT/84Q968skn9dFHH2n06NFnPNdut8tut3ekNJhox+EaFVbWyx5q1ZQhvcwuBwDgZzrUM2Kz2TRhwgTl5OR4j3k8HuXk5CgrK+u01/3ud7/TY489phUrVigjI+Pcq4VPWnHsWTRTBvdSpK1D+RYAgI71jEjS3LlzNWvWLGVkZGjSpElatGiR6urqNHv2bEnSzJkzlZqaqgULFkiSnnrqKc2fP1+vvPKK0tPTvXNLoqKiFBUV1Yk/CsxgGIbe39K6pHcqQzQAgHPQ4TAyY8YMlZeXa/78+SopKdHYsWO1YsUK76TWwsJCWa0nOlyeffZZuVwuXX/99W3e59FHH9V//ud/nl/1MN22Q07tLquVPdSq77HrKgDgHHR4nxEzsM+I73rs3e3666p9unJ0by3+yXizywEA+JAu2WcE+KYWt0dv5x2SJP1wXKrJ1QAA/BVhBOfsyz1HVFHbpLjIMF06mFU0AIBzQxjBOXtrU+vzha4ak6KwEH6VAADnhjsIzkldU4t3o7NrGaIBAJwHwgjOyb+2l6ih2a30+EiNTYs1uxwAgB8jjOCcvLmpdeLq9HGpslgsJlcDAPBnhBF0WJmzUat2lUtiiAYAcP4II+iwdzYfkseQxveNVb/4HmaXAwDwc4QRdNhbea2raK4d38fkSgAAgYAwgg7ZWeLU1mKnQq0W/WBUb7PLAQAEAMII2s0wDP3mn9slSdnDkhTXw2ZyRQCAQEAYQbu9lntQq/ccUXiYVfOuGGp2OQCAAEEYQbuU1zTp8fd2SJLmfm8wE1cBAJ2GMIJ2+fU/t6m6oVkjU2P07xf1N7scAEAAIYzgrD7aXqp3vz6sEKtFT/5wtEJ5Dg0AoBNxV8EZ1TQ265G3t0qSfnpJf41MdZhcEQAg0BBGcEZ/WJmvw9WN6hcfqXsvG2x2OQCAAEQYwWkdqmrQS2sLJUlPXDtKEbYQkysCAAQiwghO64Uv98ntMZQ1IF4XXZBgdjkAgABFGMEpORub9eq6IknSHZcOMLkaAEAgI4zglJatK1RtU4sGJUZpyuBeZpcDAAhghBGcxNXi0dJV+yVJt18yQFarxdyCAAABjTCCk7z79SGVOBvVK9qua8almF0OACDAEUbQhmEYeu7zvZKkWyenyx7KChoAQNcijKCNVbsrtLOkRpG2EN2U2dfscgAAQYAwgjaO94r8OCNNsZE2k6sBAAQDwgi8th2q1he7KmS1SLddzMPwAADdgzACSVJTi1u/ev1rSdIVo3orrWekyRUBAIIFYQSSpAXv79S2Q07FRYbpoSuHmV0OACCIEEagldtK9OLq/ZKkp388Rr0dEeYWBAAIKoSRIHfwaL3+47XNkqTbL+mv7w5NMrkiAECwIYwEsWa3R794dZOcjS0akxar/5g61OySAABBKNTsAtD9DMPQnvI6Pff5Hm0srFJ0eKieuXGcbKFkUwBA9yOMBAm3x9Cn+WX6JL9Mn+aX6+DRBu/3nrpuNKtnAACmIYwEAVeLRz97eaM+2lHqPWYLsSpzQE9dP6GPrhjV28TqAADBjjAS4Jpa3Lr75Y36aEeZbKFWzchI03eG9FLWwHhF2vjnBwCYj7tRAGtqcetnL21Uzs4y2UOten5mhi4d3MvssgAAaIMwEqCaWty666WN+vhYEPnvWRm6ZBBBBADgewgjAajF7WkTRP46a6IuHpRgdlkAAJwSYSQA/Slnlz7eWabwsNYgctEFBBEAgO9iY4kAs3p3hf78yW5JrUt2CSIAAF9HGAkg5TVNmrM8T4Yh3TAxTdeMTTW7JAAAzoowEiA8HkNz/56n8pomDU6K0qNXjTC7JAAA2oUwEiCWfL5HX+yqUHiYVc/8ZLwibCFmlwQAQLsQRgLAV3uP6Ol/FUiSfn31CA1Oija5IgAA2o8w4ueWrSvUzL+uk9tj6JqxKfpxRprZJQEA0CEs7fVTjc1uPfr2Ni3fUCRJ+t7wJD1x7ShZLBaTKwMAoGMII36oqLJeP3t5o7YUV8tqkf7v5UN015SBsloJIgAA/0MY8SNFlfX6368O6NV1happbFFcZJj+fON4dlcFAPg1woiJthys1q6yGjkiwhQbGSZHhE2OiDDZQtpO5dlSXK0XV+9Xzs5SGUbrsTF9HPrLzROUGhthQuUAAHQewkg3c7V49P6Ww3px9X7lFVV1+PpLBiVoVla6/m1ookIYlgEABADCyDcYhqGV20qVnhCpockxnfre1fXNemH1Pr28tlDlNU2SJFuIVeP7xarB5VZVQ7Oq6pvlbGz29n4cF2UP1Q/Hp2pmVrouSIzq1LoAADAbYeSYxma3fvX613pn8yGFhVj0yA+G65YL+520OqWosl5Prdip8LAQ3Tll4FnDQYvbo1fWFeq/PizQ0fpmSVJSjF03Z/bTDZP6qle0vc35Ho8hz7fSiNViYXIqACBgWQzj2/8d7nucTqccDoeqq6sVE9O5PRaSdKS2SXf8b65yDxxtc/zacal64tpRirCFqMXt0Yur9+vpfxWoodktSbJYpKvHpOjn3x10ylDyaX6ZHn9vh3aV1UqSBiVG6ReXDdK0kckKC2GLFwBAYGvv/Tvow8jushrNfnG9iiobFBMeqmdvnqAdh51a8MFOuT2GhiZH65eXD9GfPt6lrw9WS5Iy+/dUTESYPtxeKqk1lEwdnixHRJiO1rtU1dCsitom7S2vkyTFRYZp7uVDdOPENIUSQgAAQYIw0g5f7q7QnS/lqqaxRX17RmrprRO9PRxf7T2ie17ZqIpal/f86PBQPXTFMP04I01Wq0Vbi6v1p5xd+texUPJtYSEWzcpK188vGyRHRFin1Q0AgD8gjJxFvatFl/7uE1XUupTRL07PzcxQzx62NueUVDfqZy/namNhlb4/Mlm/vnqEEmPCT3qv7YecWrH1sOxhId5lurERNl2QGKVkx8nnAwAQDAgj7bB6T4X+kVusx68dqfCwUz/l1uMxdNjZyH4eAAB0UHvv30G9mmbywARNHnjm3UutVgtBBACALsRsSgAAYCrCCAAAMBVhBAAAmIowAgAATEUYAQAApiKMAAAAUxFGAACAqQgjAADAVOcURhYvXqz09HSFh4crMzNT69atO+P5r732moYOHarw8HCNGjVK77///jkVCwAAAk+Hw8jy5cs1d+5cPfroo9q4caPGjBmjqVOnqqys7JTnr169WjfeeKNuu+02bdq0SdOnT9f06dO1devW8y4eAAD4vw4/myYzM1MTJ07UM888I0nyeDxKS0vTz3/+cz3wwAMnnT9jxgzV1dXp3Xff9R678MILNXbsWC1ZsqRdn9lVz6YBAABdp7337w71jLhcLuXm5io7O/vEG1itys7O1po1a055zZo1a9qcL0lTp0497fmS1NTUJKfT2eYFAAACU4fCSEVFhdxut5KSktocT0pKUklJySmvKSkp6dD5krRgwQI5HA7vKy0trSNlAgAAP+KTT+2dN2+e5s6d6/26urpaffv2pYcEAAA/cvy+fbYZIR0KIwkJCQoJCVFpaWmb46WlpUpOTj7lNcnJyR06X5Lsdrvsdrv36+M/DD0kAAD4n5qaGjkcjtN+v0NhxGazacKECcrJydH06dMltU5gzcnJ0T333HPKa7KyspSTk6N7773Xe+zDDz9UVlZWuz83JSVFRUVFio6OlsVi6UjJZ+R0OpWWlqaioiImxnYx2rr70Nbdi/buPrR19+mstjYMQzU1NUpJSTnjeR0eppk7d65mzZqljIwMTZo0SYsWLVJdXZ1mz54tSZo5c6ZSU1O1YMECSdKcOXM0ZcoUPf3007ryyiu1bNkybdiwQc8991y7P9NqtapPnz4dLbXdYmJi+MXuJrR196Gtuxft3X1o6+7TGW19ph6R4zocRmbMmKHy8nLNnz9fJSUlGjt2rFasWOGdpFpYWCir9cS82MmTJ+uVV17Rww8/rAcffFCDBg3SW2+9pZEjR3b0owEAQADq8D4jgYT9S7oPbd19aOvuRXt3H9q6+3R3Wwf1s2nsdrseffTRNpNl0TVo6+5DW3cv2rv70Nbdp7vbOqh7RgAAgPmCumcEAACYjzACAABMRRgBAACmIowAAABTBXUYWbx4sdLT0xUeHq7MzEytW7fO7JL83oIFCzRx4kRFR0crMTFR06dPV35+fptzGhsbdffddys+Pl5RUVG67rrrTnpkADrmySeflMViabPTMe3cuYqLi3XzzTcrPj5eERERGjVqlDZs2OD9vmEYmj9/vnr37q2IiAhlZ2dr165dJlbsn9xutx555BH1799fERERGjhwoB577LE2zzahrc/N559/rquuukopKSmyWCx666232ny/Pe1aWVmpm266STExMYqNjdVtt92m2tra8y/OCFLLli0zbDabsXTpUmPbtm3G7bffbsTGxhqlpaVml+bXpk6darzwwgvG1q1bjby8POOKK64w+vbta9TW1nrPufPOO420tDQjJyfH2LBhg3HhhRcakydPNrFq/7Zu3TojPT3dGD16tDFnzhzvcdq581RWVhr9+vUzbr31VmPt2rXG3r17jZUrVxq7d+/2nvPkk08aDofDeOutt4zNmzcbV199tdG/f3+joaHBxMr9z+OPP27Ex8cb7777rrFv3z7jtddeM6Kioow//vGP3nNo63Pz/vvvGw899JDxxhtvGJKMN998s83329Ou06ZNM8aMGWN89dVXxhdffGFccMEFxo033njetQVtGJk0aZJx9913e792u91GSkqKsWDBAhOrCjxlZWWGJOOzzz4zDMMwqqqqjLCwMOO1117znrNjxw5DkrFmzRqzyvRbNTU1xqBBg4wPP/zQmDJlijeM0M6d6/777zcuvvji037f4/EYycnJxu9//3vvsaqqKsNutxuvvvpqd5QYMK688krj3//939sc++EPf2jcdNNNhmHQ1p3l22GkPe26fft2Q5Kxfv167zkffPCBYbFYjOLi4vOqJyiHaVwul3Jzc5Wdne09ZrValZ2drTVr1phYWeCprq6WJPXs2VOSlJubq+bm5jZtP3ToUPXt25e2Pwd33323rrzyyjbtKdHOne2dd95RRkaGfvSjHykxMVHjxo3T888/7/3+vn37VFJS0qa9HQ6HMjMzae8Omjx5snJyclRQUCBJ2rx5s1atWqXvf//7kmjrrtKedl2zZo1iY2OVkZHhPSc7O1tWq1Vr1649r8/v8LNpAkFFRYXcbrf3eTrHJSUlaefOnSZVFXg8Ho/uvfdeXXTRRd5nEZWUlMhmsyk2NrbNuUlJSSopKTGhSv+1bNkybdy4UevXrz/pe7Rz59q7d6+effZZzZ07Vw8++KDWr1+vX/ziF7LZbJo1a5a3TU/1N4X27pgHHnhATqdTQ4cOVUhIiNxutx5//HHddNNNkkRbd5H2tGtJSYkSExPbfD80NFQ9e/Y877YPyjCC7nH33Xdr69atWrVqldmlBJyioiLNmTNHH374ocLDw80uJ+B5PB5lZGToiSeekCSNGzdOW7du1ZIlSzRr1iyTqwssf//73/Xyyy/rlVde0YgRI5SXl6d7771XKSkptHUAC8phmoSEBIWEhJy0sqC0tFTJyckmVRVY7rnnHr377rv65JNP1KdPH+/x5ORkuVwuVVVVtTmftu+Y3NxclZWVafz48QoNDVVoaKg+++wz/elPf1JoaKiSkpJo507Uu3dvDR8+vM2xYcOGqbCwUJK8bcrflPP3H//xH3rggQd0ww03aNSoUbrlllt03333acGCBZJo667SnnZNTk5WWVlZm++3tLSosrLyvNs+KMOIzWbThAkTlJOT4z3m8XiUk5OjrKwsEyvzf4Zh6J577tGbb76pjz/+WP3792/z/QkTJigsLKxN2+fn56uwsJC274DLLrtMW7ZsUV5enveVkZGhm266yfu/aefOc9FFF520RL2goED9+vWTJPXv31/Jyclt2tvpdGrt2rW0dwfV19fLam17awoJCZHH45FEW3eV9rRrVlaWqqqqlJub6z3n448/lsfjUWZm5vkVcF7TX/3YsmXLDLvdbrz44ovG9u3bjTvuuMOIjY01SkpKzC7Nr911112Gw+EwPv30U+Pw4cPeV319vfecO++80+jbt6/x8ccfGxs2bDCysrKMrKwsE6sODN9cTWMYtHNnWrdunREaGmo8/vjjxq5du4yXX37ZiIyMNF566SXvOU8++aQRGxtrvP3228bXX39tXHPNNSw3PQezZs0yUlNTvUt733jjDSMhIcH41a9+5T2Htj43NTU1xqZNm4xNmzYZkoyFCxcamzZtMg4cOGAYRvvaddq0aca4ceOMtWvXGqtWrTIGDRrE0t7z9ec//9no27evYbPZjEmTJhlfffWV2SX5PUmnfL3wwgvecxoaGoyf/exnRlxcnBEZGWlce+21xuHDh80rOkB8O4zQzp3rn//8pzFy5EjDbrcbQ4cONZ577rk23/d4PMYjjzxiJCUlGXa73bjsssuM/Px8k6r1X06n05gzZ47Rt29fIzw83BgwYIDx0EMPGU1NTd5zaOtz88knn5zy7/OsWbMMw2hfux45csS48cYbjaioKCMmJsaYPXu2UVNTc961WQzjG9vaAQAAdLOgnDMCAAB8B2EEAACYijACAABMRRgBAACmIowAAABTEUYAAICpCCMAAMBUhBEAAGAqwggAADAVYQQAAJiKMAIAAExFGAEAAKb6/5bw9uMxiKKrAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"Poem text generator.ipynb\n",
    "\n",
    "Automatically generated by Colaboratory.\n",
    "\n",
    "# Text Generation using LSTM\n",
    "\n",
    "##Two types of text generation\n",
    "\n",
    "\n",
    "1.   Character based text generation\n",
    "* Each character of the text is used to train the model and prediction will also result in generation of new characters. \n",
    "\n",
    "2.   Word based text generation\n",
    "* Words are converted into tokens which are used to train the model. The model will generate words instead of characters in the prediction stage.\n",
    "\n",
    "## Mechanics of the text generation model: \n",
    "\n",
    "1. The next word of the sequence is predicted using the words that are already present in the sequence. \n",
    "\n",
    "2. It is a simple model where splitting the data into training and testing sets is not required. This is because the model will use all the words in the sequence to predict the next word. Just like forecasting.\n",
    "\n",
    "#The flow of program:\n",
    "\n",
    "1. Loading data\n",
    "2. Preprocessing the data and Tokenizing \n",
    "3. Building and fitting the model on data\n",
    "4. Evaluate the model\n",
    "5. Predicting(Generating the text)\n",
    "6. Saving the model for future applicaitons\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# reading the data from local google drive \n",
    "# with open('robert_frost.txt') as story:\n",
    "#   story_data = story.read().decode(encoding='utf-8')\n",
    "\n",
    "story_data = open('robert_frost.txt', 'rb').read().decode(encoding='utf-8') #New buy mee\n",
    "\n",
    "print(story_data)\n",
    "\n",
    "# data cleaning process\n",
    "import re                                # Regular expressions to use sub function for replacing the useless text from the data\n",
    "\n",
    "def clean_text(text):\n",
    "  text = re.sub(r',', '', text)\n",
    "  text = re.sub(r'\\'', '',  text)\n",
    "  text = re.sub(r'\\\"', '', text)\n",
    "  text = re.sub(r'\\(', '', text)\n",
    "  text = re.sub(r'\\)', '', text)\n",
    "  text = re.sub(r'\\n', '', text)\n",
    "  text = re.sub(r'“', '', text)\n",
    "  text = re.sub(r'”', '', text)\n",
    "  text = re.sub(r'’', '', text)\n",
    "  text = re.sub(r'\\.', '', text)\n",
    "  text = re.sub(r';', '', text)\n",
    "  text = re.sub(r':', '', text)\n",
    "  text = re.sub(r'\\-', '', text)\n",
    "\n",
    "  return text\n",
    "\n",
    "# cleaning the data\n",
    "lower_data = story_data.lower()           # Converting the string to lower case to get uniformity\n",
    "\n",
    "split_data = lower_data.splitlines()      # Splitting the data to get every line seperately but this will give the list of uncleaned data\n",
    "\n",
    "print(split_data)                         \n",
    "\n",
    "final = ''                                # initiating a argument with blank string to hold the values of final cleaned data\n",
    "\n",
    "for line in split_data:\n",
    "  line = clean_text(line)\n",
    "  final += '\\n' + line\n",
    "\n",
    "print(final)\n",
    "\n",
    "final_data = final.split('\\n')       # splitting again to get list of cleaned and splitted data ready to be processed\n",
    "print(final_data)\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Instantiating the Tokenizer\n",
    "max_vocab = 1000000\n",
    "tokenizer = Tokenizer(num_words=max_vocab)\n",
    "tokenizer.fit_on_texts(final_data)\n",
    "\n",
    "# Getting the total number of words of the data.\n",
    "word2idx = tokenizer.word_index\n",
    "print(len(word2idx))\n",
    "print(word2idx)\n",
    "vocab_size = len(word2idx) + 1        # Adding 1 to the vocab_size because the index starts from 1 not 0. This will make it uniform when using it further\n",
    "print(vocab_size)\n",
    "\n",
    "\"\"\"## Creating n-gram sequences from the sentences\n",
    "\n",
    "* Consider this sentence : ['two roads diverged in a yellow wood']. Here we will use ['two roads diverged in a yellow'] to predict ['wood']. This is the basic concept of forecasting which can be applied here to generate text.\n",
    "\n",
    "* An advacement of this will be to use single word or every combination words possible from the sentence to predict the next word. And this is loosely termed as n_gram sequences\n",
    "\n",
    "* The sentence ['two roads diverged in a yellow wood'] will have sequence as [112, 113, 114, 7, 5, 190, 75]\n",
    "\n",
    "* so we will use combinations of words to make our model better\n",
    "\n",
    "* [112, 113], \n",
    "* [112, 113, 114], \n",
    "* [112, 113, 114, 7], \n",
    "* [112, 113, 114, 7, 5], \n",
    "* [112, 113, 114, 7, 5, 190], \n",
    "* [112, 113, 114, 7, 5, 190, 75]\n",
    "\n",
    "* we train our model that if 112 comes then it has to predict 113.\n",
    "* if combination of 112, 113, comes then it has to predict 114 and so on.\n",
    "\"\"\"\n",
    "\n",
    "# We will turn the sentences to sequences line by line and create n_gram sequences\n",
    "\n",
    "input_seq = []\n",
    "\n",
    "for line in final_data:\n",
    "  token_list = tokenizer.texts_to_sequences([line])[0]\n",
    "  for i in range(1, len(token_list)):\n",
    "    n_gram_seq = token_list[:i+1]\n",
    "    input_seq.append(n_gram_seq)\n",
    "\n",
    "print(input_seq)\n",
    "\n",
    "# Getting the maximum length of sequence for padding purpose\n",
    "max_seq_length = max(len(x) for x in input_seq)\n",
    "print(max_seq_length)\n",
    "\n",
    "# Padding the sequences and converting them to array\n",
    "input_seq = np.array(pad_sequences(input_seq, maxlen=max_seq_length, padding='pre'))\n",
    "print(input_seq)\n",
    "\n",
    "# Taking xs and labels to train the model.\n",
    "\n",
    "xs = input_seq[:, :-1]        # xs contains every word in sentence except the last one because we are using this value to predict the y value\n",
    "labels = input_seq[:, -1]     # labels contains only the last word of the sentence which will help in hot encoding the y value in next step\n",
    "print(\"xs: \",xs)\n",
    "print(\"labels:\",labels)\n",
    "\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# one-hot encoding the labels according to the vocab size\n",
    "\n",
    "# The matrix is square matrix of the size of vocab_size. Each row will denote a label and it will have \n",
    "# a single +ve value(i.e 1) for that label and other values will be zero. \n",
    "\n",
    "ys = to_categorical(labels, num_classes=vocab_size)\n",
    "print(ys)\n",
    "\n",
    "from tensorflow.keras.layers import Input, Dense, Embedding, LSTM, Dropout, Bidirectional, GlobalMaxPooling1D\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "# using the functional APIs of keras to define the model\n",
    "\n",
    "i = Input(shape=(max_seq_length - 1, ))                           # using 1 less value becasuse we are preserving the last value for predicted word \n",
    "x = Embedding(vocab_size, 124)(i)\n",
    "x = Dropout(0.2)(x)\n",
    "x = LSTM(520, return_sequences=True)(x)\n",
    "x = Bidirectional(layer=LSTM(340, return_sequences=True))(x)\n",
    "x = GlobalMaxPooling1D()(x)\n",
    "x = Dense(1024, activation='relu')(x)\n",
    "x = Dense(vocab_size, activation='softmax')(x)\n",
    "\n",
    "model = Model(i,x)\n",
    "\n",
    "# using the pipeline method of sequential to define a model\n",
    "\n",
    "# model = Sequential()\n",
    "# model.add(Embedding(vocab_size, 124, input_length=max_seq_length-1))\n",
    "# model.add(Dropout(0.2))\n",
    "# model.add(LSTM(520, return_sequences=True))\n",
    "# model.add(Bidirectional(LSTM(340, return_sequences=True)))\n",
    "# model.add(GlobalMaxPooling1D())\n",
    "# model.add(Dense(1024, activation='relu'))\n",
    "# model.add(Dense(vocab_size, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer=Adam(learning_rate=0.001),\n",
    "              loss = 'categorical_crossentropy',\n",
    "              metrics=['accuracy'])   ## New je change 'lr' en \"learning_rate\"\n",
    "\n",
    "# model.summary()                                       # We can know about the shape of the model\n",
    "\n",
    "r = model.fit(xs,ys,epochs=100)\n",
    "# r = model.fit(xs,ys,epochs=10)\n",
    "\n",
    "\n",
    "# Evaluating the model on accuracy\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(r.history['accuracy'])\n",
    "\n",
    "# Defining a function to take input of seed text from user and no. of words to be predicted\n",
    "\n",
    "def predict_words(seed, no_words):\n",
    "  for i in range(no_words):\n",
    "    token_list = tokenizer.texts_to_sequences([seed])[0]\n",
    "    token_list = pad_sequences([token_list], maxlen=max_seq_length-1, padding='pre')\n",
    "    predicted = np.argmax(model.predict(token_list), axis=1)\n",
    "\n",
    "    new_word = ''\n",
    "\n",
    "    for word, index in tokenizer.word_index.items():\n",
    "      if predicted == index:\n",
    "        new_word = word\n",
    "        break\n",
    "    seed += \" \" + new_word\n",
    "  print(seed)\n",
    "\n",
    "# predicting or generating the poem with the seed text\n",
    "\n",
    "seed_text = 'i am feeling good today'\n",
    "next_words = 20\n",
    "\n",
    "predict_words(seed_text, next_words)\n",
    "\n",
    "# saving the model\n",
    "\n",
    "model.save('poem_generator.h5') # Will create a HDF5 file of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 230ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 64ms/step\n",
      "1/1 [==============================] - 0s 114ms/step\n",
      "1/1 [==============================] - 0s 52ms/step\n",
      "1/1 [==============================] - 0s 55ms/step\n",
      "1/1 [==============================] - 0s 59ms/step\n",
      "1/1 [==============================] - 0s 51ms/step\n",
      "1/1 [==============================] - 0s 51ms/step\n",
      "1/1 [==============================] - 0s 55ms/step\n",
      "1/1 [==============================] - 0s 83ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "1/1 [==============================] - 0s 58ms/step\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "two roads diverged in a yellow wood away i— sell their bowed his moon again said a few again said the few again\n"
     ]
    }
   ],
   "source": [
    "seed_text = 'two roads diverged'\n",
    "next_words = 20\n",
    "\n",
    "predict_words(seed_text, next_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yvanol fotso\\AppData\\Local\\Temp\\ipykernel_9056\\2952996871.py:33: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  len(characters)), dtype=np.bool)\n",
      "C:\\Users\\yvanol fotso\\AppData\\Local\\Temp\\ipykernel_9056\\2952996871.py:35: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  len(characters)), dtype=np.bool)\n",
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.RMSprop.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n",
      "105/651 [===>..........................] - ETA: 31:29 - loss: 3.1110"
     ]
    }
   ],
   "source": [
    "####### Tutos et projet net #######\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "####### Meme sans connexion le jeu de donnée est acccessible #######\n",
    "\n",
    "filepath = tf.keras.utils.get_file('shakespeare.txt',\n",
    "        'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')\n",
    "text = open(filepath, 'rb').read().decode(encoding='utf-8')\n",
    "\n",
    "text = open(filepath, 'rb').read().decode(encoding='utf-8').lower()\n",
    "\n",
    "\n",
    "\n",
    "#text = open('shakespeare.txt', 'rb').read().decode(encoding='utf-8')  # New change by mee but i don't want to download dataset anyway when i'want to execute this code\n",
    "\n",
    "#text = open('shakespeare.txt', 'rb').read().decode(encoding='utf-8').lower()  # New change by mee but i don't want to download dataset anyway when i'want to execute this code\n",
    "\n",
    "\n",
    "text = text[300000:800000]\n",
    "\n",
    "characters = sorted(set(text))\n",
    "\n",
    "char_to_index = dict((c, i) for i, c in enumerate(characters))\n",
    "index_to_char = dict((i, c) for i, c in enumerate(characters))\n",
    "\n",
    "SEQ_LENGTH = 40\n",
    "STEP_SIZE = 3\n",
    "\n",
    "sentences = []\n",
    "next_char = []\n",
    "\n",
    "for i in range(0, len(text) - SEQ_LENGTH, STEP_SIZE):\n",
    "    sentences.append(text[i: i + SEQ_LENGTH])\n",
    "    next_char.append(text[i + SEQ_LENGTH])\n",
    "\n",
    "\n",
    "\n",
    "x = np.zeros((len(sentences), SEQ_LENGTH,\n",
    "              len(characters)), dtype=np.bool)\n",
    "y = np.zeros((len(sentences),\n",
    "              len(characters)), dtype=np.bool)\n",
    "\n",
    "for i, satz in enumerate(sentences):\n",
    "    for t, char in enumerate(satz):\n",
    "        x[i, t, char_to_index[char]] = 1\n",
    "    y[i, char_to_index[next_char[i]]] = 1\n",
    "\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "from tensorflow.keras.layers import Activation, Dense, LSTM\n",
    "\n",
    "model = Sequential()\n",
    "model.add(LSTM(128,\n",
    "               input_shape=(SEQ_LENGTH,\n",
    "                            len(characters)))) ### New je change 128 en 520 \"Mais l'entrainement devient  10 fois plus lent\"\n",
    "model.add(Dense(len(characters)))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=RMSprop(learning_rate=0.01)) ## New je change 'lr' en \"learning_rate\"\n",
    "\n",
    "model.fit(x, y, batch_size=256, epochs=4)\n",
    "\n",
    "\n",
    "def sample(preds, temperature=1.0):\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds) / temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, preds, 1)\n",
    "    return np.argmax(probas)\n",
    "\n",
    "def generate_text(length, temperature):\n",
    "    start_index = random.randint(0, len(text) - SEQ_LENGTH - 1)\n",
    "    generated = ''\n",
    "    sentence = text[start_index: start_index + SEQ_LENGTH]\n",
    "    generated += sentence\n",
    "    for i in range(length):\n",
    "        x_predictions = np.zeros((1, SEQ_LENGTH, len(characters)))\n",
    "        for t, char in enumerate(sentence):\n",
    "            x_predictions[0, t, char_to_index[char]] = 1\n",
    "\n",
    "        predictions = model.predict(x_predictions, verbose=0)[0]\n",
    "        next_index = sample(predictions,\n",
    "                                 temperature)\n",
    "        next_character = index_to_char[next_index]\n",
    "\n",
    "        generated += next_character\n",
    "        sentence = sentence[1:] + next_character\n",
    "    return generated\n",
    "\n",
    "\n",
    "\n",
    "print(generate_text(300, 0.2))\n",
    "print(generate_text(300, 0.4))\n",
    "print(generate_text(300, 0.5))\n",
    "print(generate_text(300, 0.6))\n",
    "print(generate_text(300, 0.7))\n",
    "print(generate_text(300, 0.8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluating the model on accuracy\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(model.history['accuracy'])\n",
    "plt.plot(model.history['val_accuracy'])\n",
    "plt.plot(model.history['val_loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########## Tutos [thibo73800] Generation des poeme de [Victor Hugo] Trois tutos and youtube and code in github access ########\n",
    "########## RNN  ############"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################## begin le Tutos ##########################\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "#assert hasattr(tf, \"function\") # Be sure to use tensorflow 2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "130231\n",
      "Parce que, jargonnant vÃªpres, jeÃ»ne et vigile,\n",
      "Exploitant Dieu qui rÃªve au fond du firmament,\n",
      "Vous avez, au milieu du divin Ã©vangile,\n",
      "Ouvert boutique effrontÃ©ment ;\n",
      "\n",
      "Parce que vous feriez prendre Ã  JÃ©sus la verge,\n",
      "Cyniques brocanteurs sortis on ne sait d'oÃ¹ ;\n",
      "Parce que vous allez vendant la sainte vierge\n",
      "Dix sous avec miracle, et sans miracle un sou ;\n",
      "\n",
      "Parce que vous contez d'effroyables sornettes\n",
      "Qui font des temples saints trembler les vieux piliers ;\n",
      "Parce que votre style Ã©blouit les lunettes\n",
      "Des duÃ¨gnes et des marguilliers ;\n",
      "\n",
      "Parce que la soutane est sous vos redingotes,\n",
      "Parce que vous sentez la crasse et non l'Å“illet,\n",
      "Parce que vous bÃ¢clez un journal de bigotes\n",
      "PensÃ© par Escobar, Ã©crit par Patouillet ;\n",
      "\n",
      "Parce qu'en balayant leurs portes, les concierges\n",
      "Poussent dans le ruisseau ce pamphlet mÃ©prisÃ© ;\n",
      "Parce que vous mÃªlez Ã  la cire des cierges\n",
      "Votre affreux suif vert-de-grisÃ© ;\n",
      "\n",
      "Parce qu'Ã  vous tout seuls vous faites une espÃ¨ce\n",
      "Parce qu'enfin, blanchis dehors et\n"
     ]
    }
   ],
   "source": [
    "# You can used your own dataset with english text\n",
    "\n",
    "with open(\"victorhugo.txt\", \"r\") as f:\n",
    "    text = f.read()\n",
    "\n",
    "print(len(text))\n",
    "\n",
    "print(text[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40 {'/', 'n', '.', 's', 'i', 'p', 'q', '(', 'b', '%', 'z', 'g', 'm', 'd', '|', ';', 'j', 'y', 'l', 't', ')', 'x', 'u', \"'\", 'w', ',', '\\n', 'v', 'o', '\"', 'e', ' ', 'f', 'h', '0', 'r', 'c', ':', 'k', 'a'}\n",
      "parce que, jargonnant vaapres, jeane et vigile,\n",
      "exploitant dieu qui raave au fond du firmament,\n",
      "vous avez, au milieu du divin a(c)vangile,\n",
      "ouvert boutique effronta(c)ment ;\n",
      "\n",
      "parce que vous feriez prendre a  ja(c)sus la verge,\n",
      "cyniques brocanteurs sortis on ne sait d'oa ;\n",
      "parce que vous allez vendant la sainte vierge\n",
      "dix sous avec miracle, et sans miracle un sou ;\n",
      "\n",
      "parce que vous contez d'effroyables sornettes\n",
      "qui font des temples saints trembler les vieux piliers ;\n",
      "parce que votre style a(c)blouit les lunettes\n",
      "des dua\"gnes et des marguilliers ;\n",
      "\n",
      "parce que la soutane est sous vos redingotes,\n",
      "parce que vous sentez la crasse et non l'a\"illet,\n",
      "parce que vous bac/clez un journal de bigotes\n",
      "pensa(c) par escobar, a(c)crit par patouillet ;\n",
      "\n",
      "parce qu'en balayant leurs portes, les concierges\n",
      "poussent dans le ruisseau ce pamphlet ma(c)prisa(c) ;\n",
      "parce que vous maalez a  la cire des cierges\n",
      "votre affreux suif vertdegrisa(c) ;\n",
      "\n",
      "parce qu'a  vous tout seuls vous faites une espa\"ce\n",
      "parce qu'enfin, bla\n"
     ]
    }
   ],
   "source": [
    "######## Remove character and create vocab\n",
    "\n",
    "import unidecode\n",
    "\n",
    "text = unidecode.unidecode(text)\n",
    "text = text.lower()\n",
    "\n",
    "text = text.replace(\"2\", \"\")\n",
    "text = text.replace(\"1\", \"\")\n",
    "text = text.replace(\"8\", \"\")\n",
    "text = text.replace(\"5\", \"\")\n",
    "text = text.replace(\">\", \"\")\n",
    "text = text.replace(\"<\", \"\")\n",
    "text = text.replace(\"!\", \"\")\n",
    "text = text.replace(\"?\", \"\")\n",
    "text = text.replace(\"-\", \"\")\n",
    "text = text.replace(\"$\", \"\")\n",
    "\n",
    "text = text.strip()\n",
    "\n",
    "vocab = set(text)\n",
    "print(len(vocab), vocab)\n",
    "\n",
    "print(text[:1000])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab_to_int {'/': 0, 'n': 1, '.': 2, 's': 3, 'i': 4, 'p': 5, 'q': 6, '(': 7, 'b': 8, '%': 9, 'z': 10, 'g': 11, 'm': 12, 'd': 13, '|': 14, ';': 15, 'j': 16, 'y': 17, 'l': 18, 't': 19, ')': 20, 'x': 21, 'u': 22, \"'\": 23, 'w': 24, ',': 25, '\\n': 26, 'v': 27, 'o': 28, '\"': 29, 'e': 30, ' ': 31, 'f': 32, 'h': 33, '0': 34, 'r': 35, 'c': 36, ':': 37, 'k': 38, 'a': 39}\n",
      "\n",
      "int_to_vocab {0: '/', 1: 'n', 2: '.', 3: 's', 4: 'i', 5: 'p', 6: 'q', 7: '(', 8: 'b', 9: '%', 10: 'z', 11: 'g', 12: 'm', 13: 'd', 14: '|', 15: ';', 16: 'j', 17: 'y', 18: 'l', 19: 't', 20: ')', 21: 'x', 22: 'u', 23: \"'\", 24: 'w', 25: ',', 26: '\\n', 27: 'v', 28: 'o', 29: '\"', 30: 'e', 31: ' ', 32: 'f', 33: 'h', 34: '0', 35: 'r', 36: 'c', 37: ':', 38: 'k', 39: 'a'}\n",
      "\n",
      "int for e: 30\n",
      "letter for 30: e\n"
     ]
    }
   ],
   "source": [
    "## Map each letter to int #####\n",
    "\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "vocab_to_int = {l:i for i,l in enumerate(vocab)}\n",
    "int_to_vocab = {i:l for i,l in enumerate(vocab)}\n",
    "\n",
    "print(\"vocab_to_int\", vocab_to_int)\n",
    "print()\n",
    "print(\"int_to_vocab\", int_to_vocab)\n",
    "\n",
    "print(\"\\nint for e:\", vocab_to_int[\"e\"])\n",
    "int_for_e = vocab_to_int[\"e\"]\n",
    "print(\"letter for %s: %s\" % (vocab_to_int[\"e\"], int_to_vocab[int_for_e]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5, 39, 35, 36, 30, 31, 6, 22, 30, 25, 31, 16, 39, 35, 11, 28, 1, 1, 39, 1, 19, 31, 27, 39, 39, 5, 35, 30, 3, 25, 31, 16, 30, 39, 1, 30, 31, 30, 19, 31, 27, 4, 11, 4, 18, 30, 25, 26, 30, 21, 5, 18, 28, 4, 19, 39, 1, 19, 31, 13, 4, 30, 22, 31, 6, 22, 4, 31, 35, 39, 39, 27, 30, 31, 39, 22, 31, 32, 28, 1, 13, 31, 13, 22, 31, 32, 4, 35, 12, 39, 12, 30, 1, 19, 25, 26, 27, 28, 22, 3]\n"
     ]
    }
   ],
   "source": [
    "encoded = [vocab_to_int[l] for l in text]\n",
    "encoded_sentence = encoded[:100]\n",
    "\n",
    "print(encoded_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['p', 'a', 'r', 'c', 'e', ' ', 'q', 'u', 'e', ',', ' ', 'j', 'a', 'r', 'g', 'o', 'n', 'n', 'a', 'n', 't', ' ', 'v', 'a', 'a', 'p', 'r', 'e', 's', ',', ' ', 'j', 'e', 'a', 'n', 'e', ' ', 'e', 't', ' ', 'v', 'i', 'g', 'i', 'l', 'e', ',', '\\n', 'e', 'x', 'p', 'l', 'o', 'i', 't', 'a', 'n', 't', ' ', 'd', 'i', 'e', 'u', ' ', 'q', 'u', 'i', ' ', 'r', 'a', 'a', 'v', 'e', ' ', 'a', 'u', ' ', 'f', 'o', 'n', 'd', ' ', 'd', 'u', ' ', 'f', 'i', 'r', 'm', 'a', 'm', 'e', 'n', 't', ',', '\\n', 'v', 'o', 'u', 's']\n"
     ]
    }
   ],
   "source": [
    "decoded_sentence = [int_to_vocab[i] for i in encoded_sentence]\n",
    "print(decoded_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parce que, jargonnant vaapres, jeane et vigile,\n",
      "exploitant dieu qui raave au fond du firmament,\n",
      "vous\n"
     ]
    }
   ],
   "source": [
    "decoded_sentence = \"\".join(decoded_sentence)\n",
    "print(decoded_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inputs [5, 39, 35, 36, 30, 31, 6, 22, 30, 25]\n",
      "Targets [39, 35, 36, 30, 31, 6, 22, 30, 25, 31]\n"
     ]
    }
   ],
   "source": [
    "inputs, targets = encoded, encoded[1:]\n",
    "\n",
    "print(\"Inputs\", inputs[:10])\n",
    "print(\"Targets\", targets[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 5. 39. 35. 36. 30.] [39. 35. 36. 30. 31.]\n",
      "[ 5. 39.  7. 36.  7.] [39. 35. 36. 30. 31.]\n"
     ]
    }
   ],
   "source": [
    "def gen_batch(inputs, targets, seq_len, batch_size, noise=0):\n",
    "    # Size of each chunk\n",
    "    chuck_size = (len(inputs) -1)  // batch_size\n",
    "    # Numbef of sequence per chunk\n",
    "    sequences_per_chunk = chuck_size // seq_len\n",
    "\n",
    "    for s in range(0, sequences_per_chunk):\n",
    "        batch_inputs = np.zeros((batch_size, seq_len))\n",
    "        batch_targets = np.zeros((batch_size, seq_len))\n",
    "        for b in range(0, batch_size):\n",
    "            fr = (b*chuck_size)+(s*seq_len)\n",
    "            to = fr+seq_len\n",
    "            batch_inputs[b] = inputs[fr:to]\n",
    "            batch_targets[b] = inputs[fr+1:to+1]\n",
    "            \n",
    "            if noise > 0:\n",
    "                noise_indices = np.random.choice(seq_len, noise)\n",
    "                batch_inputs[b][noise_indices] = np.random.randint(0, vocab_size)\n",
    "            \n",
    "        yield batch_inputs, batch_targets\n",
    "\n",
    "for batch_inputs, batch_targets in gen_batch(inputs, targets, 5, 32, noise=0):\n",
    "    print(batch_inputs[0], batch_targets[0])\n",
    "    break\n",
    "\n",
    "for batch_inputs, batch_targets in gen_batch(inputs, targets, 5, 32, noise=3):\n",
    "    print(batch_inputs[0], batch_targets[0])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 5. 39. 35. 36. 30. 31.  6. 22. 30. 25. 31. 16. 39. 35. 11. 28.  1.  1.\n",
      " 39.  1. 19. 31. 27. 39. 39.  5. 35. 30.  3. 25. 31. 16. 30. 39.  1. 30.\n",
      " 31. 30. 19. 31. 27.  4. 11.  4. 18. 30. 25. 26. 30. 21.] [39. 35. 36. 30. 31.  6. 22. 30. 25. 31. 16. 39. 35. 11. 28.  1.  1. 39.\n",
      "  1. 19. 31. 27. 39. 39.  5. 35. 30.  3. 25. 31. 16. 30. 39.  1. 30. 31.\n",
      " 30. 19. 31. 27.  4. 11.  4. 18. 30. 25. 26. 30. 21.  5.]\n",
      "[ 5. 39. 35. 23. 30. 31.  6. 22. 30. 25. 31. 16. 39. 35. 11. 28.  1.  1.\n",
      " 39.  1. 19. 31. 27. 39. 39.  5. 23. 30.  3. 25. 31. 16. 30. 39.  1. 30.\n",
      " 31. 30. 19. 31. 27.  4. 11.  4. 18. 30. 25. 26. 23. 21.] [39. 35. 36. 30. 31.  6. 22. 30. 25. 31. 16. 39. 35. 11. 28.  1.  1. 39.\n",
      "  1. 19. 31. 27. 39. 39.  5. 35. 30.  3. 25. 31. 16. 30. 39.  1. 30. 31.\n",
      " 30. 19. 31. 27.  4. 11.  4. 18. 30. 25. 26. 30. 21.  5.]\n"
     ]
    }
   ],
   "source": [
    "#### New buy mee\n",
    "######### new Mon genrateur  ########\n",
    "\n",
    "# def gen_batch(inputs, targets, seq_len, batch_size, noise=0):\n",
    "#     # Nombre total de séquences dans le jeu de données\n",
    "#     total_sequences = len(inputs) // seq_len\n",
    "#     # Nombre de lots par époque\n",
    "#     num_batches = total_sequences // batch_size\n",
    "\n",
    "#     for _ in range(num_batches):\n",
    "#         batch_inputs = np.zeros((batch_size, seq_len))\n",
    "#         batch_targets = np.zeros((batch_size, seq_len))\n",
    "#         for b in range(batch_size):\n",
    "#             start_idx = np.random.randint(0, len(inputs) - seq_len - 1)\n",
    "#             end_idx = start_idx + seq_len\n",
    "#             batch_inputs[b] = inputs[start_idx:end_idx]\n",
    "#             batch_targets[b] = targets[start_idx:end_idx]\n",
    "            \n",
    "#             if noise > 0:\n",
    "#                 noise_indices = np.random.choice(seq_len, noise)\n",
    "#                 batch_inputs[b][noise_indices] = np.random.randint(0, vocab_size, noise)\n",
    "            \n",
    "#         yield batch_inputs, batch_targets\n",
    "\n",
    "# # Utilisation du générateur\n",
    "# for batch_inputs, batch_targets in gen_batch(inputs, targets, 50, 64):\n",
    "#     print(batch_inputs.shape, batch_targets.shape)\n",
    "#     break\n",
    "\n",
    "# import numpy as np\n",
    "\n",
    "# def gen_batch(inputs, targets, seq_len, batch_size, noise=0):\n",
    "#     if isinstance(inputs, int) or isinstance(targets, int):\n",
    "#         raise ValueError(\"Les données d'entrée ou de cible ne sont pas correctement initialisées.\")\n",
    "    \n",
    "#     # Nombre total de séquences dans le jeu de données\n",
    "#     total_sequences = len(inputs) // seq_len\n",
    "#     # Nombre de lots par époque\n",
    "#     num_batches = total_sequences // batch_size\n",
    "\n",
    "#     for _ in range(num_batches):\n",
    "#         batch_inputs = np.zeros((batch_size, seq_len))\n",
    "#         batch_targets = np.zeros((batch_size, seq_len))\n",
    "#         for b in range(batch_size):\n",
    "#             start_idx = np.random.randint(0, len(inputs) - seq_len - 1)\n",
    "#             end_idx = start_idx + seq_len\n",
    "#             batch_inputs[b] = inputs[start_idx:end_idx]\n",
    "#             batch_targets[b] = targets[start_idx:end_idx]\n",
    "            \n",
    "#             if noise > 0:\n",
    "#                 noise_indices = np.random.choice(seq_len, noise)\n",
    "#                 batch_inputs[b][noise_indices] = np.random.randint(0, vocab_size, noise)\n",
    "            \n",
    "#         yield batch_inputs, batch_targets\n",
    "\n",
    "# # Utilisation du générateur\n",
    "# for batch_inputs, batch_targets in gen_batch(inputs, targets, 50, 64):\n",
    "#     print(batch_inputs.shape, batch_targets.shape)\n",
    "#     break\n",
    "\n",
    "\n",
    "\n",
    "def gen_batch(inputs, targets, seq_len, batch_size, noise=0):\n",
    "    # Taille de chaque chunk\n",
    "    chunk_size = (len(inputs) - 1) // batch_size\n",
    "    # Nombre de séquences par chunk\n",
    "    sequences_per_chunk = chunk_size // seq_len\n",
    "\n",
    "    for s in range(0, sequences_per_chunk):\n",
    "        batch_inputs = np.zeros((batch_size, seq_len))\n",
    "        batch_targets = np.zeros((batch_size, seq_len))\n",
    "        for b in range(0, batch_size):\n",
    "            fr = (b * chunk_size) + (s * seq_len)\n",
    "            to = fr + seq_len\n",
    "            batch_inputs[b] = inputs[fr:to]\n",
    "            batch_targets[b] = inputs[fr + 1:to + 1]\n",
    "\n",
    "            if noise > 0:\n",
    "                noise_indices = np.random.choice(seq_len, noise)\n",
    "                batch_inputs[b][noise_indices] = np.random.randint(0, vocab_size)\n",
    "\n",
    "        yield batch_inputs, batch_targets\n",
    "\n",
    "# Utilisation du générateur avec un lot de taille 64\n",
    "for batch_inputs, batch_targets in gen_batch(inputs, targets, 50, 64, noise=0):\n",
    "    print(batch_inputs[0], batch_targets[0])\n",
    "    break\n",
    "\n",
    "for batch_inputs, batch_targets in gen_batch(inputs, targets, 50, 64, noise=3):\n",
    "    print(batch_inputs[0], batch_targets[0])\n",
    "    break\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "############ create your own layer ###########\n",
    "\n",
    "class OneHot(tf.keras.layers.Layer):\n",
    "    def __init__(self, depth, **kwargs):\n",
    "        super(OneHot, self).__init__(**kwargs)\n",
    "        self.depth = depth\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        return tf.one_hot(tf.cast(x, tf.int32), self.depth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 50)\n",
      "1/1 [==============================] - 0s 394ms/step\n",
      "(32, 50, 40)\n",
      "Input letter is: 5.0\n",
      "One hot representation of the letter [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "################## Test if the layer work well ###################\n",
    "\n",
    "class RnnModel(tf.keras.Model):\n",
    "\n",
    "    def __init__(self, vocab_size):\n",
    "        super(RnnModel, self).__init__()\n",
    "        # Convolutions\n",
    "        self.one_hot = OneHot(len(vocab))\n",
    "\n",
    "    def call(self, inputs):\n",
    "        output = self.one_hot(inputs)\n",
    "        return output\n",
    "\n",
    "batch_inputs, batch_targets = next(gen_batch(inputs, targets, 50, 32))\n",
    "\n",
    "print(batch_inputs.shape)\n",
    "\n",
    "model = RnnModel(len(vocab))\n",
    "output = model.predict(batch_inputs)\n",
    "\n",
    "print(output.shape)\n",
    "\n",
    "#print(output)\n",
    "\n",
    "print(\"Input letter is:\", batch_inputs[0][0])\n",
    "print(\"One hot representation of the letter\", output[0][0])\n",
    "\n",
    "#assert(output[int(batch_inputs[0][0])]==1)\n",
    "\n",
    "\n",
    "#assert(output[int(batch_inputs[0][0])]==1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_20\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_21 (InputLayer)       [(64, None)]              0         \n",
      "                                                                 \n",
      " one_hot_36 (OneHot)         (64, None, 40)            0         \n",
      "                                                                 \n",
      " lstm_40 (LSTM)              (64, None, 128)           86528     \n",
      "                                                                 \n",
      " lstm_41 (LSTM)              (64, None, 128)           131584    \n",
      "                                                                 \n",
      " dense_40 (Dense)            (64, None, 128)           16512     \n",
      "                                                                 \n",
      " dense_41 (Dense)            (64, None, 40)            5160      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 239,784\n",
      "Trainable params: 239,784\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "vocab_size = len(vocab)\n",
    "\n",
    "### Creat the layers\n",
    "\n",
    "# Set the input of the model\n",
    "# tf_inputs = tf.keras.Input(shape=(None,), batch_size=64)\n",
    "\n",
    "tf_inputs = tf.keras.Input(shape=(None,), batch_size=64) \n",
    "# Convert each value of the  input into a one encoding vector\n",
    "one_hot = OneHot(len(vocab))(tf_inputs)\n",
    "# Stack LSTM cells\n",
    "rnn_layer1 = tf.keras.layers.LSTM(128, return_sequences=True, stateful=True)(one_hot)\n",
    "rnn_layer2 = tf.keras.layers.LSTM(128, return_sequences=True, stateful=True)(rnn_layer1)\n",
    "# Create the outputs of the model\n",
    "hidden_layer = tf.keras.layers.Dense(128, activation=\"relu\")(rnn_layer2)\n",
    "outputs = tf.keras.layers.Dense(vocab_size, activation=\"softmax\")(hidden_layer)\n",
    "\n",
    "### Setup the model\n",
    "model = tf.keras.Model(inputs=tf_inputs, outputs=outputs)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############# Chef if we can reset RNN Cells #####################\n",
    "\n",
    "\n",
    "# Star by resetting the cells of the RNN\n",
    "model.reset_states()\n",
    "\n",
    "# Get one batch\n",
    "batch_inputs, batch_targets = next(gen_batch(inputs, targets, 50, 64)) \n",
    "# Make a first prediction\n",
    "outputs = model.predict(batch_inputs)\n",
    "first_prediction = outputs[0][0]\n",
    "\n",
    "# Reset the states of the RNN states\n",
    "model.reset_states()\n",
    "\n",
    "# Make an other prediction to check the difference\n",
    "outputs = model.predict(batch_inputs)\n",
    "second_prediction = outputs[0][0]\n",
    "\n",
    "# Check if both prediction are equal\n",
    "assert(set(first_prediction)==set(second_prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
     ]
    }
   ],
   "source": [
    "########### Set the loss and the objective #############\n",
    "\n",
    "\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy()\n",
    "optimizer = tf.keras.optimizers.Adam(lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "########## some metrics to track the progress og training #######\n",
    "\n",
    "\n",
    "# Loss\n",
    "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "# Accuracy\n",
    "train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='train_accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "######### Set the train method method and the predict method in graph mode ###########\n",
    "\n",
    "\n",
    "@tf.function\n",
    "def train_step(inputs, targets):\n",
    "    with tf.GradientTape() as tape:\n",
    "        # Make a prediction on all the batch\n",
    "        predictions = model(inputs)\n",
    "        # Get the error/loss on these predictions\n",
    "        loss = loss_object(targets, predictions)\n",
    "    # Compute the gradient which respect to the loss\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    # Change the weights of the model\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "    # The metrics are accumulate over time. You don't need to average it yourself.\n",
    "    train_loss(loss)\n",
    "    train_accuracy(targets, predictions)\n",
    "\n",
    "@tf.function\n",
    "def predict(inputs):\n",
    "    # Make a prediction on all the batch\n",
    "    predictions = model(inputs)\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########### train de model #################\n",
    "\n",
    "model.reset_states()\n",
    "\n",
    "for epoch in range(4000):\n",
    "    for batch_inputs, batch_targets in gen_batch(inputs, targets, 100, 64, noise=13):\n",
    "        train_step(batch_inputs, batch_targets)\n",
    "    template = '\\r Epoch {}, Train Loss: {}, Train Accuracy: {}'\n",
    "    print(template.format(epoch, train_loss.result(), train_accuracy.result()*100), end=\"\")\n",
    "    model.reset_states()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############## Save the model #################\n",
    "\n",
    "\n",
    "import json\n",
    "model.save(\"model_rnn.h5\")\n",
    "\n",
    "with open(\"model_rnn_vocab_to_int\", \"w\") as f:\n",
    "    f.write(json.dumps(vocab_to_int))\n",
    "with open(\"model_rnn_int_to_vocab\", \"w\") as f:\n",
    "    f.write(json.dumps(int_to_vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################## Generating the text ##################\n",
    "\n",
    "import random\n",
    "\n",
    "model.reset_states()\n",
    "\n",
    "size_poetries = 300\n",
    "\n",
    "poetries = np.zeros((64, size_poetries, 1))\n",
    "sequences = np.zeros((64, 100))\n",
    "for b in range(64):\n",
    "    rd = np.random.randint(0, len(inputs) - 100)\n",
    "    sequences[b] = inputs[rd:rd+100]\n",
    "\n",
    "for i in range(size_poetries+1):\n",
    "    if i > 0:\n",
    "        poetries[:,i-1,:] = sequences\n",
    "    softmax = predict(sequences)\n",
    "    # Set the next sequences\n",
    "    sequences = np.zeros((64, 1))\n",
    "    for b in range(64):\n",
    "        argsort = np.argsort(softmax[b][0])\n",
    "        argsort = argsort[::-1]\n",
    "        # Select one of the strongest 4 proposals\n",
    "        sequences[b] = argsort[0]\n",
    "\n",
    "for b in range(64):\n",
    "    sentence = \"\".join([int_to_vocab[i[0]] for i in poetries[b]])\n",
    "    print(sentence)\n",
    "    print(\"\\n=====================\\n\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(\"model_rnn_vocab_to_int\", \"r\") as f:\n",
    "    vocab_to_int = json.loads(f.read())\n",
    "with open(\"model_rnn_int_to_vocab\", \"r\") as f:\n",
    "    int_to_vocab = json.loads(f.read())\n",
    "    int_to_vocab = {int(key):int_to_vocab[key] for key in int_to_vocab}\n",
    "\n",
    "model.load_weights(\"model_rnn.h5\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
