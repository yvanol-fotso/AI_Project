{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["########### c'est cette version qu'il faut installer car toute les version que j'installe ici ne marche pas j;ai pris celle qui marche sur mon pc\n","########## Ne pas executer deux fois #############\n","\n","## la commande %%capture pour supprimer les sorties des cellules de code\n","\n","%%capture\n","!pip install --upgrade tensorflow==2.12.0\n","!pip install transformers==4.37.1\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"HAbxReTMQWH4","outputId":"e4b96626-965a-452c-efa9-1fc4b468a905"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting tensorflow==2.12.0\n","  Downloading tensorflow-2.12.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (585.9 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m585.9/585.9 MB\u001b[0m \u001b[31m344.0 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.12.0) (1.4.0)\n","Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.12.0) (1.6.3)\n","Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.12.0) (24.3.6)\n","Collecting gast<=0.4.0,>=0.2.1 (from tensorflow==2.12.0)\n","  Downloading gast-0.4.0-py3-none-any.whl (9.8 kB)\n","Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.12.0) (0.2.0)\n","Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.12.0) (1.62.0)\n","Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.12.0) (3.9.0)\n","Requirement already satisfied: jax>=0.3.15 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.12.0) (0.4.23)\n","Collecting keras<2.13,>=2.12.0 (from tensorflow==2.12.0)\n","  Downloading keras-2.12.0-py2.py3-none-any.whl (1.7 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m67.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.12.0) (16.0.6)\n","Collecting numpy<1.24,>=1.22 (from tensorflow==2.12.0)\n","  Downloading numpy-1.23.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.1 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.1/17.1 MB\u001b[0m \u001b[31m34.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.12.0) (3.3.0)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.12.0) (23.2)\n","Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.12.0) (3.20.3)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.12.0) (67.7.2)\n","Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.12.0) (1.16.0)\n","Collecting tensorboard<2.13,>=2.12 (from tensorflow==2.12.0)\n","  Downloading tensorboard-2.12.3-py3-none-any.whl (5.6 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m56.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting tensorflow-estimator<2.13,>=2.12.0 (from tensorflow==2.12.0)\n","  Downloading tensorflow_estimator-2.12.0-py2.py3-none-any.whl (440 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m440.7/440.7 kB\u001b[0m \u001b[31m32.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.12.0) (2.4.0)\n","Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.12.0) (4.10.0)\n","Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.12.0) (1.14.1)\n","Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.12.0) (0.36.0)\n","Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow==2.12.0) (0.42.0)\n","Requirement already satisfied: ml-dtypes>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from jax>=0.3.15->tensorflow==2.12.0) (0.2.0)\n","Requirement already satisfied: scipy>=1.9 in /usr/local/lib/python3.10/dist-packages (from jax>=0.3.15->tensorflow==2.12.0) (1.11.4)\n","Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow==2.12.0) (2.27.0)\n","Collecting google-auth-oauthlib<1.1,>=0.5 (from tensorboard<2.13,>=2.12->tensorflow==2.12.0)\n","  Downloading google_auth_oauthlib-1.0.0-py2.py3-none-any.whl (18 kB)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow==2.12.0) (3.5.2)\n","Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow==2.12.0) (2.31.0)\n","Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow==2.12.0) (0.7.2)\n","Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow==2.12.0) (3.0.1)\n","Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow==2.12.0) (5.3.3)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow==2.12.0) (0.3.0)\n","Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow==2.12.0) (4.9)\n","Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow==2.12.0) (1.3.1)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow==2.12.0) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow==2.12.0) (3.6)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow==2.12.0) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow==2.12.0) (2024.2.2)\n","Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.13,>=2.12->tensorflow==2.12.0) (2.1.5)\n","Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow==2.12.0) (0.5.1)\n","Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow==2.12.0) (3.2.2)\n","Installing collected packages: tensorflow-estimator, numpy, keras, gast, google-auth-oauthlib, tensorboard, tensorflow\n","  Attempting uninstall: tensorflow-estimator\n","    Found existing installation: tensorflow-estimator 2.15.0\n","    Uninstalling tensorflow-estimator-2.15.0:\n","      Successfully uninstalled tensorflow-estimator-2.15.0\n","  Attempting uninstall: numpy\n","    Found existing installation: numpy 1.25.2\n","    Uninstalling numpy-1.25.2:\n","      Successfully uninstalled numpy-1.25.2\n","  Attempting uninstall: keras\n","    Found existing installation: keras 2.15.0\n","    Uninstalling keras-2.15.0:\n","      Successfully uninstalled keras-2.15.0\n","  Attempting uninstall: gast\n","    Found existing installation: gast 0.5.4\n","    Uninstalling gast-0.5.4:\n","      Successfully uninstalled gast-0.5.4\n","  Attempting uninstall: google-auth-oauthlib\n","    Found existing installation: google-auth-oauthlib 1.2.0\n","    Uninstalling google-auth-oauthlib-1.2.0:\n","      Successfully uninstalled google-auth-oauthlib-1.2.0\n","  Attempting uninstall: tensorboard\n","    Found existing installation: tensorboard 2.15.2\n","    Uninstalling tensorboard-2.15.2:\n","      Successfully uninstalled tensorboard-2.15.2\n","  Attempting uninstall: tensorflow\n","    Found existing installation: tensorflow 2.15.0\n","    Uninstalling tensorflow-2.15.0:\n","      Successfully uninstalled tensorflow-2.15.0\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","chex 0.1.85 requires numpy>=1.24.1, but you have numpy 1.23.5 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed gast-0.4.0 google-auth-oauthlib-1.0.0 keras-2.12.0 numpy-1.23.5 tensorboard-2.12.3 tensorflow-2.12.0 tensorflow-estimator-2.12.0\n"]},{"output_type":"display_data","data":{"application/vnd.colab-display-data+json":{"pip_warning":{"packages":["gast","keras","numpy","tensorboard","tensorflow"]},"id":"1cb04f4de3b04fbfb9b4933cfa706f20"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Collecting transformers==4.37.1\n","  Downloading transformers-4.37.1-py3-none-any.whl (8.4 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.4/8.4 MB\u001b[0m \u001b[31m29.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers==4.37.1) (3.13.1)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from transformers==4.37.1) (0.20.3)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.37.1) (1.23.5)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers==4.37.1) (23.2)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.37.1) (6.0.1)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.37.1) (2023.12.25)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers==4.37.1) (2.31.0)\n","Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers==4.37.1) (0.15.2)\n","Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.37.1) (0.4.2)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers==4.37.1) (4.66.2)\n","Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers==4.37.1) (2023.6.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers==4.37.1) (4.10.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.37.1) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.37.1) (3.6)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.37.1) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.37.1) (2024.2.2)\n","Installing collected packages: transformers\n","  Attempting uninstall: transformers\n","    Found existing installation: transformers 4.38.2\n","    Uninstalling transformers-4.38.2:\n","      Successfully uninstalled transformers-4.38.2\n","Successfully installed transformers-4.37.1\n"]},{"output_type":"display_data","data":{"application/vnd.colab-display-data+json":{"pip_warning":{"packages":["transformers"]},"id":"c63ddf5f06c54e51bb217056078edebc"}},"metadata":{}}]},{"cell_type":"code","source":["########### c'est cette version qu'il faut installer car toute les version que j'installe ici ne marche pas j;ai pris celle qui marche sur mon pc\n","\n","\n","import tensorflow as tf\n","import transformers\n","\n","print(\"Version de TensorFlow :\", tf.__version__)\n","print(\"Version de Transformers :\", transformers.__version__)"],"metadata":{"id":"ouhDck3uyilw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from google.colab import drive\n","import zipfile\n","import os\n","import pandas as pd\n","\n","import shutil\n","\n","\n","drive.mount('/content/drive')\n","\n","chemin_zip_heavy_attacks = \"/content/drive/My Drive/yvanolfotso/dataset/AttacksHeavy.zip\"\n","chemin_zip_heavy_benign = \"/content/drive/My Drive/yvanolfotso/dataset/BenignHeavy.zip\"\n","\n","chemin_zip_light_attacks = \"/content/drive/My Drive/yvanolfotso/dataset/AttacksLight.zip\"\n","chemin_zip_light_benign = \"/content/drive/My Drive/yvanolfotso/dataset/BenignLight.zip\"\n","\n","\n","# Fonction pour extraire mes fichiers zip\n","def extraire_zip(chemin_zip):\n","    with zipfile.ZipFile(chemin_zip, 'r') as zip_ref:\n","        zip_ref.extractall(\"/content/extraction_temp\")  # Extraction  dans mon répertoire temporaire\n","\n","# Fonction pour charger les fichiers CSV d'un type spécifique (stateful ou stateless)\n","def charger_concatener_donnees(sous_dossier, prefixe):\n","    # Lister tous les fichiers CSV dans le sous-dossier\n","    fichiers_csv = [f for f in os.listdir(f\"{sous_dossier}\") if f.startswith(prefixe) and f.endswith('.csv')]\n","    # Lire chaque fichier CSV et le stocker dans une liste de DataFrames\n","    dataframes = [pd.read_csv(f\"{sous_dossier}/{f}\") for f in fichiers_csv]\n","    # Concaténer les DataFrames en un seul\n","    return pd.concat(dataframes, ignore_index=True)\n","\n","\n","# Extraire les fichiers zip\n","extraire_zip(chemin_zip_heavy_attacks)\n","extraire_zip(chemin_zip_heavy_benign)\n","extraire_zip(chemin_zip_light_attacks)\n","extraire_zip(chemin_zip_light_benign)\n","\n","# Charger et concaténer les données de la même manière que vous l'avez fait auparavant\n","stateful_heavy_attack_data = charger_concatener_donnees(\"/content/extraction_temp/AttacksHeavy\", \"stateful\")\n","stateful_heavy_benign_data = charger_concatener_donnees(\"/content/extraction_temp/BenignHeavy\", \"stateful\")\n","stateless_heavy_attack_data = charger_concatener_donnees(\"/content/extraction_temp/AttacksHeavy\", \"stateless\")\n","stateless_heavy_benign_data = charger_concatener_donnees(\"/content/extraction_temp/BenignHeavy\", \"stateless\")\n","\n","stateful_light_attack_data = charger_concatener_donnees(\"/content/extraction_temp/AttacksLight\", \"stateful\")\n","stateful_light_benign_data = charger_concatener_donnees(\"/content/extraction_temp/BenignLight\", \"stateful\")\n","stateless_light_attack_data = charger_concatener_donnees(\"/content/extraction_temp/AttacksLight\", \"stateless\")\n","stateless_light_benign_data = charger_concatener_donnees(\"/content/extraction_temp/BenignLight\", \"stateless\")\n","\n","# Supprimer le répertoire temporaire après avoir terminé\n","if os.path.exists(\"/content/extraction_temp\"):\n","    shutil.rmtree(\"/content/extraction_temp\")\n","\n","\n","#### concatenation  sur axis = 0 ########\n","\n","print(\" Heavy attack\")\n","\n","heavy_attack = pd.concat([stateful_heavy_attack_data, stateless_heavy_attack_data], axis=0)\n","print(heavy_attack.shape)\n","\n","#### j'ajoute la classe / label ######\n","\n","heavy_attack['class'] = 'heavy_attacks'\n","print(heavy_attack.shape)\n","\n","print(\" \\n\")\n","print(\" Heavy Bengnin\")\n","\n","heavy_bengin = pd.concat([stateful_heavy_benign_data, stateless_heavy_benign_data], axis=0)\n","print(heavy_bengin.shape)\n","\n","#### j'ajoute la classe / label ######\n","\n","heavy_bengin['class'] = 'heavy_bengnin'\n","print(heavy_bengin.shape)\n","\n","print(\" \\n\")\n","print(\" Light attack\")\n","\n","light_attack = pd.concat([stateful_light_attack_data, stateless_light_attack_data], axis=0)\n","print(light_attack.shape)\n","\n","#### j'ajoute la classe / label ######\n","light_attack['class'] = 'light_attacks'\n","print(light_attack.shape)\n","\n","\n","print(\" \\n\")\n","print(\" Light Bengnin\")\n","\n","light_bengin = pd.concat([stateful_light_benign_data, stateless_light_benign_data], axis=0)\n","print(light_bengin.shape)\n","\n","#### j'ajoute la classe / label ######\n","light_bengin['class'] = 'light_bengnin'\n","print(light_bengin.shape)\n","\n","\n","################################### CONCATENATION Final des donnee sur axis = 0 #########################\n","\n","final_data = pd.concat([heavy_attack, heavy_bengin,light_attack,light_bengin], axis=0, ignore_index=True)\n","\n","# Suppression colonnes redondantes dans les données catégorielles\n","final_data = final_data.loc[:, ~final_data.columns.duplicated()]\n","\n","# Vérifier les dimensions du jeu de données final\n","print(\"final dataset size \\n\")\n","print(final_data.shape)\n"],"metadata":{"id":"i7v7Jn9DyMp6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import pandas as pd\n","from sklearn.utils import shuffle\n","from imblearn.over_sampling import RandomOverSampler\n","\n","# Identifier la classe majoritaire\n","major_class = final_data['class'].value_counts().idxmax()\n","major_count = final_data['class'].value_counts().max()\n","\n","# Sur-échantillonnage des classes minoritaires pour les ramener à la taille de la classe majoritaire\n","over_sampler = RandomOverSampler(sampling_strategy={class_: major_count for class_ in final_data['class'].unique() if class_ != major_class}, random_state=42)\n","X_over, y_over = over_sampler.fit_resample(final_data.drop(columns=['class']), final_data['class'])\n","\n","# Concaténer les données sur-échantillonnées avec la classe majoritaire\n","balanced_data = pd.concat([pd.DataFrame(X_over, columns=final_data.drop(columns=['class']).columns), pd.Series(y_over, name='class')], axis=1)\n","\n","# Mélanger les données\n","final_data = shuffle(balanced_data, random_state=42)\n","\n","# Afficher les classes équilibrées\n","print(final_data['class'].value_counts())\n"],"metadata":{"id":"tKxuLhm07gb0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["################## Les features ont deja été selectionné avec le code de feature selection dataset 3.  ####################\n","################## Mais actuellement il faut des jour pour pour que j'obtiens les resultat vue la taille des données d'OU POUR UN DEBUT J'UTILISE LES CARACTERISTIQUE SELCTIONNEE AVEC LE [GOA-GA] DU PAPIER DE 2024\n","\n","# X_numerical = final_data.select_dtypes(include=['int64', 'float64'])\n","# X_categorical = final_data.select_dtypes(exclude='number').drop('class', axis=1)\n","\n","X_numerical = final_data[['rr','A_frequency','FQDN_count','upper','lower','numeric','entropy','special', 'labels', 'labels_max','labels_average','len']]\n","\n","X_categorical = final_data[['rr_type','unique_ttl','timestamp', 'longest_word', 'sld']]\n","\n","y = final_data['class']\n","\n","print(X_numerical.shape)\n","print(X_categorical.shape)\n","\n","print(y.shape)\n","print(final_data.shape)\n","\n"],"metadata":{"id":"_HyGXC_YvX4x","colab":{"base_uri":"https://localhost:8080/"},"outputId":"12dcbf3e-5724-4e5f-977a-780c56d58686"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["(1327387, 12)\n","(1327387, 5)\n","(1327387,)\n","(1327387, 43)\n"]}]},{"cell_type":"code","source":["from sklearn.impute import SimpleImputer\n","\n","# Vérification des colonnes vides\n","if X_categorical.isnull().any().any():\n","    # Imputer les valeurs manquantes pour les caractéristiques catégorielles\n","    categorical_imputer = SimpleImputer(strategy='most_frequent')\n","    X_categorical_imputed = pd.DataFrame(categorical_imputer.fit_transform(X_categorical), columns=X_categorical.columns)\n","\n","    # Afficher le DataFrame avec les valeurs imputées\n","    print(\"Après imputation\")\n","    print(X_categorical_imputed)\n","\n","    total_size_categorical = X_categorical_imputed.shape\n","\n","    print(\"Taille totale des caractéristiques catégorielles après imputation :\", total_size_categorical)\n","\n","else:\n","    print(\"Pas de valeurs manquantes dans les caractéristiques catégorielles. Aucune imputation nécessaire.\")\n"],"metadata":{"id":"pkionIUP8BuG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["########### CONTINUER L'EXECUTION ICI ###########\n","\n","############ je continue ici ###################\n","from sklearn.impute import SimpleImputer\n","\n","# Imputer les valeurs manquantes pour les caractéristiques numériques\n","\n","# Imputer les valeurs manquantes pour les caractéristiques numériques\n","numerical_imputer = SimpleImputer(strategy='mean')\n","X_numerical_imputed = pd.DataFrame(numerical_imputer.fit_transform(X_numerical), columns=X_numerical.columns)\n","\n","\n","from sklearn.preprocessing import MinMaxScaler\n","import numpy as np\n","\n","#prepocessing des features numeriques soit  avec le  LabelEncoder soit le MinMaxScaler()\n","\n","# Sélection des fonctionnalités numériques\n","\n","# numeric_features = X_numerical_imputed[numeric_imputed_list] ## cas ou je veux selectionné certains features\n","\n","# Création d'un scaler\n","scaler = MinMaxScaler()\n","\n","# Ajustement du scaler aux données\n","scaler.fit(X_numerical_imputed)\n","\n","# Transformation des fonctionnalités numériques\n","scaled_numeric = scaler.transform(X_numerical_imputed)\n","\n","# Apres transformation Création d' un DataFrame à partir des valeurs transformées\n","\n","scaled_df = pd.DataFrame(scaled_numeric, columns=X_numerical_imputed.columns)\n","\n","\n","total_size = scaled_df.shape\n","\n","print(\"Taille totale des caracteristiques numeriques apres preprocessing :\", total_size)\n","\n"],"metadata":{"id":"mWEYjSu48Esy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from tensorflow.keras.preprocessing.text import Tokenizer\n","import pandas as pd\n","\n","\n","X_copy = X_categorical_imputed.copy()\n","\n","\n","# Appliquer une tokenisation à chaque colonne catégorielle\n","for feature in X_categorical_imputed.columns.tolist():\n","    X_copy[feature] = X_copy[feature].astype(str)\n","\n","\n","# Combinez les colonnes catégorielles dans une colonne 'combined_text'\n","\n","X_copy['combined_text'] = X_copy[X_categorical_imputed.columns.tolist()].apply(lambda row: ' '.join(row), axis=1)\n","\n","\n","# Tokenisation\n","tokenizer = Tokenizer(num_words=100, filters=' ', split=' ')\n","tokenizer.fit_on_texts(X_copy['combined_text'])\n","tokens = tokenizer.texts_to_sequences(X_copy['combined_text'])\n","\n","# Calcul de la longueur maximale du vecteur\n","max_sequence_length = max(len(seq) for seq in tokens)\n","\n","\n","### PAS BESOIN D'EXECUTER CE CECI  CAR ma machine ne dispose pas d'assez de ressources #####\n","\n","# Ajout des colonnes tokenisées au DataFrame\n","# for i in range(1, max_sequence_length + 1):\n","    # X_copy[f'token_{i}'] = [seq[i - 1] if len(seq) >= i else 0 for seq in tokens]\n","\n","# Suppression des colonnes originales et la colonne temporaire 'combined_text'\n","# X_copy.drop(columns=X_categorical_imputed.columns.tolist() + ['combined_text'], inplace=True)\n","\n","##################### End ##############################################\n","\n","# Afficher les tokens et les longueurs de séquence\n","# print(tokens)\n","\n","\n","# Affichage de la longueur maximale\n","print(f\"Longueur maximale du vecteur : {max_sequence_length}\")\n"],"metadata":{"id":"I3ifSeB68H6i"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Calculer la taille du vocabulaire réel\n","vocab_size = len(tokenizer.word_index) + 1  # Ajoutez 1 pour tenir compte du padding\n","\n","# Afficher la taille du vocabulaire\n","print(\"Taille réelle du vocabulaire:\", vocab_size)"],"metadata":{"id":"9EKFnX5k8M-n"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["mean_length = np.mean(max_sequence_length)\n","max_length = np.max(max_sequence_length)\n","min_length = np.min(max_sequence_length)\n","std_dev = np.std(max_sequence_length)\n","\n","# Visualisation de la distribution des longueurs des séquences\n","import matplotlib.pyplot as plt\n","plt.hist(max_sequence_length, bins=30)\n","plt.xlabel('Longueur de Séquence')\n","plt.ylabel('Nombre de Séquences')\n","plt.title('Distribution des Longueurs des Séquences')\n","plt.show()\n"],"metadata":{"id":"vUtJDRYw8QKC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Hyperparamètres et dimensions des données*\n","taille_num_features = scaled_df.shape[1]\n","taille_text_features = max_sequence_length # je donne la taille max parmi les longueur des sequences\n","vocab_size = vocab_size ### ici vu que j'utilise la tokenisation keras la taille du vocabulaires est en realaite le paramtre ['num_word] passer a mon tokeniser donc inutile cette ligne\n","embedding_dim = 128"],"metadata":{"id":"dPpuPugL8U0p"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from tensorflow.keras.layers import *\n","import tensorflow as tf\n","from tensorflow.keras.optimizers.legacy import Adam\n","\n","\n","\n","\n","# Entrées\n","input_num = tf.keras.layers.Input(shape=(taille_num_features,))\n","input_text = tf.keras.layers.Input(shape=(taille_text_features,))\n","\n","\n","# Branches du modèle\n","# Branche numérique - FFN\n","\n","num_branch = tf.keras.layers.Dense(256, activation='relu')(input_num)\n","num_branch = tf.keras.layers.Dense(128, activation='relu')(num_branch)\n","\n","\n","# Branche textuelle - Embedding + Transformer (Un block)\n","\n","text_branch = Embedding(vocab_size, embedding_dim, input_length=taille_text_features)(input_text)\n","\n","mask_inputs = masque_remplissage(input_text)\n","\n","out_seq = Encodeur(\n","            n_layers=5,\n","            d_model=128,\n","            num_heads=2,\n","            middle_units=256,\n","            max_seq_len=taille_text_features)([text_branch, mask_inputs])\n","# out_seq = GlobalAveragePooling1D()(out_seq)\n","out_seq = Dropout(0.2)(out_seq)\n","\n","# Fusion des branches\n","\n","flattened_text_branch = tf.keras.layers.Flatten()(text_branch) # je remodelise les dimension\n","merged = tf.keras.layers.concatenate([num_branch, flattened_text_branch])\n","\n","\n","# Couches supplémentaires après la fusion\n","merged = tf.keras.layers.Dropout(0.2)(merged)\n","merged = tf.keras.layers.Dense(64, activation='relu')(merged)\n","merged = tf.keras.layers.Dense(32, activation='relu')(merged)\n","merged = tf.keras.layers.Dense(16, activation='relu')(merged)\n","\n","output = tf.keras.layers.Dense(4, activation='softmax')(merged)\n","\n","\n","# Création et compilation du modèle\n","model = Model(inputs=[input_num, input_text], outputs=output)\n","\n","opt = Adam(learning_rate=0.001)\n","model.compile(optimizer=opt, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n","\n","print(model.summary())\n","\n","\n"],"metadata":{"id":"QTE_YpQr8dV7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from sklearn.preprocessing import MinMaxScaler, LabelEncoder\n","\n","# Entrées pour l'entraînement\n","data_num = scaled_df\n","data_text = pad_sequences(tokens, maxlen=max_length,padding='post')\n","\n","# Assurez-vous que les données numériques sont correctes\n","print(\"Shape of data_num:\", data_num.shape)\n","\n","# Assurez-vous que les données textuelles sont correctes après le rembourrage\n","print(\"Shape of data_text:\", data_text.shape)\n","\n","# Assurez-vous que les étiquettes sont correctes\n","print(\"Shape of labels:\", y.shape)\n","\n","# NB utiliser le resultat obtenu pour modifier les tailles ou dimension de mes entrées definie plus haut\n","\n","\n","# Encodage des étiquettes\n","label_encoder = LabelEncoder()\n","y_encoded = label_encoder.fit_transform(y)\n","\n","print(y_encoded)\n","\n","print(label_encoder.classes_)\n"],"metadata":{"id":"FZvChuJov5qJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn.model_selection import train_test_split\n","from tensorflow.keras.callbacks import ModelCheckpoint\n","\n","# Diviser les données : entraînement et test\n","X_train_num, X_test_num, X_train_cat, X_test_cat, y_train, y_test  = train_test_split(\n","    data_num,\n","    data_text,\n","    y_encoded,\n","    test_size=0.15,\n","    random_state=42\n",")\n","\n","# Définir le chemin pour sauvegarder les checkpoints\n","checkpoint_path = '/content/drive/My Drive/yvanolfotso/bestmodel/model_dataset1_1_checkpoint.h5'\n","\n","# Créer le rappel ModelCheckpoint pour sauvegarder le meilleur modèle basé sur la validation accuracy\n","checkpoint_callback = ModelCheckpoint(\n","    filepath=checkpoint_path,\n","    monitor='val_accuracy',\n","    verbose=1,\n","    save_best_only=True,\n","    mode='max',\n","    save_weights_only=False  # Sauvegardez le modèle complet, pas seulement les poids\n",")\n","\n","\n","# Diviser l'ensemble d'entraînement en sous-ensembles d'entraînement et de validation (15% pour la validation)\n","X_train_num, X_val_num, X_train_cat, X_val_cat, y_train, y_val = train_test_split(\n","    X_train_num, X_train_cat, y_train, test_size=0.15 / 0.85, random_state=42\n",")\n","\n","# Entraîner le modèle avec les données d'entraînement et de validation\n","history = model.fit(\n","    [X_train_num, X_train_cat], y_train,\n","    epochs=20,\n","    batch_size=64,\n","    validation_data=([X_val_num, X_val_cat], y_val)\n",")\n"],"metadata":{"id":"r0h60bxTmVI0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["############ evaluation sur les donnee de test ##############\n","\n","from sklearn.metrics import classification_report\n","\n","\n","# Évaluer le modèle sur l'ensemble de test\n","test_loss, test_accuracy = model.evaluate([X_test_num, X_test_cat], y_test)\n","\n","print(f'Loss sur l\\'ensemble de test: {test_loss}')\n","print(f'Accuracy sur l\\'ensemble de test: {test_accuracy}')\n","\n","\n","############### je do les prediction ####################\n","\n","\n","# Effectuer les prédictions sur les données de test\n","y_pred = model.predict([X_test_num, X_test_cat])\n","\n","# Convertir les indices des classes prédites en étiquettes\n","y_pred_classes = np.argmax(y_pred, axis=1)\n","\n","# Afficher le rapport de classification\n","class_report = classification_report(y_test, y_pred_classes)\n","print(\"Rapport de classification :\\n\", class_report)"],"metadata":{"id":"PVzNa8j5-y3y"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["######################################### recherche d'hyperpaaramtre ##################################"],"metadata":{"id":"XGvuH2Nn903M"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install keras-tuner\n"],"metadata":{"id":"f5PJ-mtsv_KJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["########################### recherche d'hyperparametre ################################################\n","\n","import keras_tuner as kt\n","from tensorflow.keras.models import Model\n","from tensorflow.keras.layers import *\n","from tensorflow.keras.optimizers.legacy import Adam\n","from sklearn.model_selection import train_test_split\n","\n","taille_num_features = taille_num_features\n","taille_text_features = taille_text_features\n","vocab_size = vocab_size\n","\n","\n","def build_model(hp):\n","\n","    input_num = Input(shape=(taille_num_features,))\n","    input_text = Input(shape=(taille_text_features,))\n","\n","    # Branche numérique - FFN\n","    num_branch = Dense(units=hp.Int('num_units_1', min_value=64, max_value=512, step=64), activation='relu')(input_num)\n","    num_branch = BatchNormalization()(num_branch)\n","    num_branch = Dropout(rate=hp.Float('num_dropout_1', min_value=0.1, max_value=0.5, step=0.1))(num_branch)\n","    num_branch = Dense(units=hp.Int('num_units_2', min_value=64, max_value=256, step=64), activation='relu')(num_branch)\n","    num_branch = BatchNormalization()(num_branch)\n","    num_branch = Dropout(rate=hp.Float('num_dropout_2', min_value=0.1, max_value=0.5, step=0.1))(num_branch)\n","\n","    # Branche textuelle - Embedding + Transformer\n","    embedding_dim = hp.Choice('embedding_dim', values=[128, 256, 512])\n","    text_branch = Embedding(vocab_size, embedding_dim, input_length=taille_text_features)(input_text)\n","    mask_inputs = masque_remplissage(input_text)\n","    out_seq = Encodeur(\n","                n_layers=hp.Int('n_layers', 1, 6, step=1),\n","                d_model=embedding_dim,\n","                num_heads=hp.Choice('num_heads', values=[4, 8, 16]),\n","                middle_units=hp.Int('middle_units', 128, 512, step=128),\n","                max_seq_len=taille_text_features)([text_branch, mask_inputs])\n","    out_seq = GlobalAveragePooling1D()(out_seq)\n","    out_seq = Dropout(rate=hp.Float('transformer_dropout', 0.1, 0.5, step=0.1))(out_seq)\n","\n","    merged = concatenate([num_branch, out_seq])\n","\n","    # Couches supplémentaires après la fusion\n","    merged = Dense(units=hp.Int('merged_units_1', 64, 256, step=64), activation='relu')(merged)\n","    merged = BatchNormalization()(merged)\n","    merged = Dropout(rate=hp.Float('merged_dropout_1', 0.1, 0.5, step=0.1))(merged)\n","    merged = Dense(units=hp.Int('merged_units_2', 32, 128, step=32), activation='relu')(merged)\n","    merged = BatchNormalization()(merged)\n","    merged = Dropout(rate=hp.Float('merged_dropout_2', 0.1, 0.5, step=0.1))(merged)\n","    merged = Dense(32, activation='relu')(merged)\n","\n","    output = Dense(4, activation='softmax')(merged)\n","\n","    model = Model(inputs=[input_num, input_text], outputs=output)\n","\n","    opt = Adam(learning_rate=hp.Float('learning_rate', 1e-4, 1e-2, sampling='log'))\n","    model.compile(optimizer=opt, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n","\n","    return model\n","\n","# Définir la recherche d'hyperparamètres\n","tuner = kt.Hyperband(\n","    build_model,\n","    objective='val_accuracy',\n","    max_epochs=10,\n","    factor=3,\n","    directory='hyperband',\n","    project_name='text_numerical_model_tuning'\n",")\n","\n","# Diviser les données en ensembles d'entraînement et de validation\n","X_train_numerical, X_val_numerical, X_train_categorical, X_val_categorical, y_train, y_val = train_test_split(\n","    data_num, data_text, y_encoded, test_size=0.25, random_state=42)\n","\n","# Préparation des  données pour le tuner\n","train_data = [X_train_numerical, X_train_categorical]\n","val_data = [X_val_numerical, X_val_categorical]\n","\n","# Lancer la recherche d'hyperparamètres\n","tuner.search(train_data, y_train, epochs=10, validation_data=(val_data, y_val), batch_size=32)\n","\n","# résultats\n","best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n","print(f\"\"\"\n","Les meilleurs hyperparamètres sont :\n","- embedding_dim : {best_hps.get('embedding_dim')}\n","- num_units_1 : {best_hps.get('num_units_1')}\n","- num_dropout_1 : {best_hps.get('num_dropout_1')}\n","- num_units_2 : {best_hps.get('num_units_2')}\n","- num_dropout_2 : {best_hps.get('num_dropout_2')}\n","- n_layers : {best_hps.get('n_layers')}\n","- num_heads : {best_hps.get('num_heads')}\n","- middle_units : {best_hps.get('middle_units')}\n","- transformer_dropout : {best_hps.get('transformer_dropout')}\n","- merged_units_1 : {best_hps.get('merged_units_1')}\n","- merged_dropout_1 : {best_hps.get('merged_dropout_1')}\n","- merged_units_2 : {best_hps.get('merged_units_2')}\n","- merged_dropout_2 : {best_hps.get('merged_dropout_2')}\n","- learning_rate : {best_hps.get('learning_rate')}\n","\"\"\")\n"],"metadata":{"id":"ebIhdQJQ97mS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Affichage des courbes d'apprentissage et de validation\n","plt.figure(figsize=(12, 6))\n","\n","# Plot de la perte d'entraînement et de la perte de validation\n","plt.subplot(1, 2, 1)\n","plt.plot(history.history['loss'], label='Training loss')\n","plt.plot(history.history['val_loss'], label='Validation loss')\n","plt.title('Courbe de Perte')\n","plt.xlabel('Epochs')\n","plt.ylabel('Loss')\n","plt.legend()\n","\n","\n","# Plot de la précision d'entraînement et de la précision de validation\n","plt.subplot(1, 2, 2)\n","plt.plot(history.history['accuracy'], label='Training accuracy')\n","plt.plot(history.history['val_accuracy'], label='Validation accuracy')\n","plt.title('Courbe de Précision')\n","plt.xlabel('Epochs')\n","plt.ylabel('Précision')\n","plt.legend()\n","\n","# Afficher les deux sous-plots\n","plt.tight_layout()\n","plt.show()"],"metadata":{"id":"cVjmqpHOwEbl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn.metrics import accuracy_score\n","\n","# Convertir les indices des classes prédites en étiquettes\n","y_true_classes = np.argmax(y_test, axis=1) if y_test.ndim > 1 else y_test\n","y_pred_classes = np.argmax(y_pred, axis=1) if y_pred.ndim > 1 else y_pred\n","\n","# Calculer l'accuracy pour chaque classe\n","accuracies = []\n","for class_label in range(4):  # Il y a 4 classes numérotées de 0 à 3\n","    y_true_class = (y_true_classes == class_label).astype(int)\n","    y_pred_class = (y_pred_classes == class_label).astype(int)\n","    class_accuracy = accuracy_score(y_true_class, y_pred_class)\n","    accuracies.append(class_accuracy)\n","\n","# Afficher les accuracies pour chaque classe\n","for class_label, accuracy in enumerate(accuracies):\n","    print(f\"Accuracy for class {class_label}: {accuracy}\")\n"],"metadata":{"id":"-S0WIGP8wSwm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Extraction of training metrics\n","training_loss = history.history['loss']\n","training_accuracy = history.history['accuracy']\n","validation_loss = history.history['val_loss']\n","validation_accuracy = history.history['val_accuracy']\n","\n","# Plotting loss and accuracy curves separately\n","epochs = range(1, len(training_loss) + 1)\n","\n","# Plotting training and validation curves\n","plt.figure(figsize=(12, 6))\n","\n","# Plotting training and validation loss\n","plt.subplot(1, 2, 1)\n","plt.plot(epochs, training_loss, label='Training loss')\n","plt.plot(epochs, validation_loss, label='Validation loss')\n","plt.title('Loss Curve')\n","plt.xlabel('Epochs')\n","plt.ylabel('Loss')\n","plt.legend()\n","\n","# Plotting training and validation accuracy\n","plt.subplot(1, 2, 2)\n","plt.plot(epochs, training_accuracy, label='Training accuracy')\n","plt.plot(epochs, validation_accuracy, label='Validation accuracy')\n","plt.title('Accuracy Curve')\n","plt.xlabel('Epochs')\n","plt.ylabel('Accuracy')\n","plt.legend()\n","\n","# Displaying both subplots\n","plt.tight_layout()\n","plt.show()\n"],"metadata":{"id":"h_KIT0-XwVbG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import matplotlib.pyplot as plt\n","import seaborn as sns\n","from sklearn.metrics import confusion_matrix\n","\n","# Matrice de confusion\n","conf_matrix = confusion_matrix(y_test, y_pred_classes)\n","class_names = ['heavy_attacks', 'heavy_benign', 'light_attacks', 'light_benign']\n","\n","df_cm = pd.DataFrame(conf_matrix, index=class_names, columns=class_names)\n","\n","# Afficher la matrice de confusion avec seaborn\n","plt.figure(figsize=(8, 6))\n","sns.heatmap(df_cm, annot=True, fmt='g', cmap='Blues', cbar=False)\n","plt.title('Matrice de Confusion')\n","plt.xlabel('Prédictions')\n","plt.ylabel('Vraies valeurs')\n","plt.show()\n"],"metadata":{"id":"Z6OvY93uwYEO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Vérifier la forme des tableaux labels_test et y_pred_classes\n","print(\"Shape of labels_test:\", y_test.shape)\n","print(\"Shape of y_pred_classes:\", y_pred_classes.shape)\n","\n","from sklearn.preprocessing import LabelBinarizer\n","\n","# Binariser labels_test\n","lb = LabelBinarizer()\n","labels_test_binary = lb.fit_transform(labels_test)\n","\n","\n","from sklearn.metrics import roc_curve, auc\n","import matplotlib.pyplot as plt\n","\n","# Calcul des courbes ROC et AUC pour chaque classe\n","fpr = dict()\n","tpr = dict()\n","roc_auc = dict()\n","for i in range(4):\n","    fpr[i], tpr[i], _ = roc_curve(labels_test_binary[:, i], y_pred[:, i])\n","    roc_auc[i] = auc(fpr[i], tpr[i])\n","\n","# Plotter les courbes ROC pour chaque classe\n","plt.figure(figsize=(8, 6))\n","for i in range(4):\n","    plt.plot(fpr[i], tpr[i], lw=2, label='Class %d (AUC = %0.2f)' % (i, roc_auc[i]))\n","\n","plt.plot([0, 1], [0, 1], color='black', lw=2, linestyle='--')\n","plt.xlabel('Taux de faux positifs (FPR)')\n","plt.ylabel('Taux de vrais positifs (TPR)')\n","plt.title('Courbes ROC pour chaque classe')\n","plt.legend(loc=\"lower right\")\n","plt.show()\n"],"metadata":{"id":"VlgpEv9Wwad2"},"execution_count":null,"outputs":[]}]}