{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["########### c'est cette version qu'il faut installer car toute les version que j'installe ici ne marche pas j;ai pris celle qui marche sur mon pc\n","########## Ne pas executer deux fois #############\n","\n","## la commande %%capture pour supprimer les sorties des cellules de code\n","\n","%%capture\n","!pip install --upgrade tensorflow==2.12.0\n","!pip install transformers==4.37.1\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"HAbxReTMQWH4","executionInfo":{"status":"ok","timestamp":1710324313653,"user_tz":0,"elapsed":294759,"user":{"displayName":"Aurelien Kamdem","userId":"15537056127158048299"}},"outputId":"e4b96626-965a-452c-efa9-1fc4b468a905"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting tensorflow==2.12.0\n","  Downloading tensorflow-2.12.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (585.9 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m585.9/585.9 MB\u001b[0m \u001b[31m344.0 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.12.0) (1.4.0)\n","Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.12.0) (1.6.3)\n","Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.12.0) (24.3.6)\n","Collecting gast<=0.4.0,>=0.2.1 (from tensorflow==2.12.0)\n","  Downloading gast-0.4.0-py3-none-any.whl (9.8 kB)\n","Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.12.0) (0.2.0)\n","Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.12.0) (1.62.0)\n","Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.12.0) (3.9.0)\n","Requirement already satisfied: jax>=0.3.15 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.12.0) (0.4.23)\n","Collecting keras<2.13,>=2.12.0 (from tensorflow==2.12.0)\n","  Downloading keras-2.12.0-py2.py3-none-any.whl (1.7 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m67.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.12.0) (16.0.6)\n","Collecting numpy<1.24,>=1.22 (from tensorflow==2.12.0)\n","  Downloading numpy-1.23.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.1 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.1/17.1 MB\u001b[0m \u001b[31m34.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.12.0) (3.3.0)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.12.0) (23.2)\n","Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.12.0) (3.20.3)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.12.0) (67.7.2)\n","Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.12.0) (1.16.0)\n","Collecting tensorboard<2.13,>=2.12 (from tensorflow==2.12.0)\n","  Downloading tensorboard-2.12.3-py3-none-any.whl (5.6 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m56.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting tensorflow-estimator<2.13,>=2.12.0 (from tensorflow==2.12.0)\n","  Downloading tensorflow_estimator-2.12.0-py2.py3-none-any.whl (440 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m440.7/440.7 kB\u001b[0m \u001b[31m32.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.12.0) (2.4.0)\n","Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.12.0) (4.10.0)\n","Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.12.0) (1.14.1)\n","Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.12.0) (0.36.0)\n","Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow==2.12.0) (0.42.0)\n","Requirement already satisfied: ml-dtypes>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from jax>=0.3.15->tensorflow==2.12.0) (0.2.0)\n","Requirement already satisfied: scipy>=1.9 in /usr/local/lib/python3.10/dist-packages (from jax>=0.3.15->tensorflow==2.12.0) (1.11.4)\n","Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow==2.12.0) (2.27.0)\n","Collecting google-auth-oauthlib<1.1,>=0.5 (from tensorboard<2.13,>=2.12->tensorflow==2.12.0)\n","  Downloading google_auth_oauthlib-1.0.0-py2.py3-none-any.whl (18 kB)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow==2.12.0) (3.5.2)\n","Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow==2.12.0) (2.31.0)\n","Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow==2.12.0) (0.7.2)\n","Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow==2.12.0) (3.0.1)\n","Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow==2.12.0) (5.3.3)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow==2.12.0) (0.3.0)\n","Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow==2.12.0) (4.9)\n","Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow==2.12.0) (1.3.1)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow==2.12.0) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow==2.12.0) (3.6)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow==2.12.0) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow==2.12.0) (2024.2.2)\n","Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.13,>=2.12->tensorflow==2.12.0) (2.1.5)\n","Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow==2.12.0) (0.5.1)\n","Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow==2.12.0) (3.2.2)\n","Installing collected packages: tensorflow-estimator, numpy, keras, gast, google-auth-oauthlib, tensorboard, tensorflow\n","  Attempting uninstall: tensorflow-estimator\n","    Found existing installation: tensorflow-estimator 2.15.0\n","    Uninstalling tensorflow-estimator-2.15.0:\n","      Successfully uninstalled tensorflow-estimator-2.15.0\n","  Attempting uninstall: numpy\n","    Found existing installation: numpy 1.25.2\n","    Uninstalling numpy-1.25.2:\n","      Successfully uninstalled numpy-1.25.2\n","  Attempting uninstall: keras\n","    Found existing installation: keras 2.15.0\n","    Uninstalling keras-2.15.0:\n","      Successfully uninstalled keras-2.15.0\n","  Attempting uninstall: gast\n","    Found existing installation: gast 0.5.4\n","    Uninstalling gast-0.5.4:\n","      Successfully uninstalled gast-0.5.4\n","  Attempting uninstall: google-auth-oauthlib\n","    Found existing installation: google-auth-oauthlib 1.2.0\n","    Uninstalling google-auth-oauthlib-1.2.0:\n","      Successfully uninstalled google-auth-oauthlib-1.2.0\n","  Attempting uninstall: tensorboard\n","    Found existing installation: tensorboard 2.15.2\n","    Uninstalling tensorboard-2.15.2:\n","      Successfully uninstalled tensorboard-2.15.2\n","  Attempting uninstall: tensorflow\n","    Found existing installation: tensorflow 2.15.0\n","    Uninstalling tensorflow-2.15.0:\n","      Successfully uninstalled tensorflow-2.15.0\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","chex 0.1.85 requires numpy>=1.24.1, but you have numpy 1.23.5 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed gast-0.4.0 google-auth-oauthlib-1.0.0 keras-2.12.0 numpy-1.23.5 tensorboard-2.12.3 tensorflow-2.12.0 tensorflow-estimator-2.12.0\n"]},{"output_type":"display_data","data":{"application/vnd.colab-display-data+json":{"pip_warning":{"packages":["gast","keras","numpy","tensorboard","tensorflow"]},"id":"1cb04f4de3b04fbfb9b4933cfa706f20"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Collecting transformers==4.37.1\n","  Downloading transformers-4.37.1-py3-none-any.whl (8.4 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.4/8.4 MB\u001b[0m \u001b[31m29.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers==4.37.1) (3.13.1)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from transformers==4.37.1) (0.20.3)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.37.1) (1.23.5)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers==4.37.1) (23.2)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.37.1) (6.0.1)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.37.1) (2023.12.25)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers==4.37.1) (2.31.0)\n","Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers==4.37.1) (0.15.2)\n","Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.37.1) (0.4.2)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers==4.37.1) (4.66.2)\n","Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers==4.37.1) (2023.6.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers==4.37.1) (4.10.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.37.1) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.37.1) (3.6)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.37.1) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.37.1) (2024.2.2)\n","Installing collected packages: transformers\n","  Attempting uninstall: transformers\n","    Found existing installation: transformers 4.38.2\n","    Uninstalling transformers-4.38.2:\n","      Successfully uninstalled transformers-4.38.2\n","Successfully installed transformers-4.37.1\n"]},{"output_type":"display_data","data":{"application/vnd.colab-display-data+json":{"pip_warning":{"packages":["transformers"]},"id":"c63ddf5f06c54e51bb217056078edebc"}},"metadata":{}}]},{"cell_type":"code","source":["########### c'est cette version qu'il faut installer car toute les version que j'installe ici ne marche pas j;ai pris celle qui marche sur mon pc\n","\n","\n","import tensorflow as tf\n","import transformers\n","\n","print(\"Version de TensorFlow :\", tf.__version__)\n","print(\"Version de Transformers :\", transformers.__version__)"],"metadata":{"id":"ouhDck3uyilw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from google.colab import drive\n","import zipfile\n","import os\n","import pandas as pd\n","\n","import shutil\n","\n","\n","drive.mount('/content/drive')\n","\n","chemin_zip_heavy_attacks = \"/content/drive/My Drive/yvanolfotso/dataset/AttacksHeavy.zip\"\n","chemin_zip_heavy_benign = \"/content/drive/My Drive/yvanolfotso/dataset/BenignHeavy.zip\"\n","\n","chemin_zip_light_attacks = \"/content/drive/My Drive/yvanolfotso/dataset/AttacksLight.zip\"\n","chemin_zip_light_benign = \"/content/drive/My Drive/yvanolfotso/dataset/BenignLight.zip\"\n","\n","\n","# Fonction pour extraire mes fichiers zip\n","def extraire_zip(chemin_zip):\n","    with zipfile.ZipFile(chemin_zip, 'r') as zip_ref:\n","        zip_ref.extractall(\"/content/extraction_temp\")  # Extraction  dans mon répertoire temporaire\n","\n","# Fonction pour charger les fichiers CSV d'un type spécifique (stateful ou stateless)\n","def charger_concatener_donnees(sous_dossier, prefixe):\n","    # Lister tous les fichiers CSV dans le sous-dossier\n","    fichiers_csv = [f for f in os.listdir(f\"{sous_dossier}\") if f.startswith(prefixe) and f.endswith('.csv')]\n","    # Lire chaque fichier CSV et le stocker dans une liste de DataFrames\n","    dataframes = [pd.read_csv(f\"{sous_dossier}/{f}\") for f in fichiers_csv]\n","    # Concaténer les DataFrames en un seul\n","    return pd.concat(dataframes, ignore_index=True)\n","\n","\n","# Extraire les fichiers zip\n","extraire_zip(chemin_zip_heavy_attacks)\n","extraire_zip(chemin_zip_heavy_benign)\n","extraire_zip(chemin_zip_light_attacks)\n","extraire_zip(chemin_zip_light_benign)\n","\n","# Charger et concaténer les données de la même manière que vous l'avez fait auparavant\n","stateful_heavy_attack_data = charger_concatener_donnees(\"/content/extraction_temp/AttacksHeavy\", \"stateful\")\n","stateful_heavy_benign_data = charger_concatener_donnees(\"/content/extraction_temp/BenignHeavy\", \"stateful\")\n","stateless_heavy_attack_data = charger_concatener_donnees(\"/content/extraction_temp/AttacksHeavy\", \"stateless\")\n","stateless_heavy_benign_data = charger_concatener_donnees(\"/content/extraction_temp/BenignHeavy\", \"stateless\")\n","\n","stateful_light_attack_data = charger_concatener_donnees(\"/content/extraction_temp/AttacksLight\", \"stateful\")\n","stateful_light_benign_data = charger_concatener_donnees(\"/content/extraction_temp/BenignLight\", \"stateful\")\n","stateless_light_attack_data = charger_concatener_donnees(\"/content/extraction_temp/AttacksLight\", \"stateless\")\n","stateless_light_benign_data = charger_concatener_donnees(\"/content/extraction_temp/BenignLight\", \"stateless\")\n","\n","# Supprimer le répertoire temporaire après avoir terminé\n","if os.path.exists(\"/content/extraction_temp\"):\n","    shutil.rmtree(\"/content/extraction_temp\")\n","\n","\n","#### concatenation  sur axis = 0 ########\n","\n","print(\" Heavy attack\")\n","\n","heavy_attack = pd.concat([stateful_heavy_attack_data, stateless_heavy_attack_data], axis=0)\n","print(heavy_attack.shape)\n","\n","#### j'ajoute la classe / label ######\n","\n","heavy_attack['class'] = 'heavy_attacks'\n","print(heavy_attack.shape)\n","\n","print(\" \\n\")\n","print(\" Heavy Bengnin\")\n","\n","heavy_bengin = pd.concat([stateful_heavy_benign_data, stateless_heavy_benign_data], axis=0)\n","print(heavy_bengin.shape)\n","\n","#### j'ajoute la classe / label ######\n","\n","heavy_bengin['class'] = 'heavy_bengnin'\n","print(heavy_bengin.shape)\n","\n","print(\" \\n\")\n","print(\" Light attack\")\n","\n","light_attack = pd.concat([stateful_light_attack_data, stateless_light_attack_data], axis=0)\n","print(light_attack.shape)\n","\n","#### j'ajoute la classe / label ######\n","light_attack['class'] = 'light_attacks'\n","print(light_attack.shape)\n","\n","\n","print(\" \\n\")\n","print(\" Light Bengnin\")\n","\n","light_bengin = pd.concat([stateful_light_benign_data, stateless_light_benign_data], axis=0)\n","print(light_bengin.shape)\n","\n","#### j'ajoute la classe / label ######\n","light_bengin['class'] = 'light_bengnin'\n","print(light_bengin.shape)\n","\n","\n","################################### CONCATENATION Final des donnee sur axis = 0 #########################\n","\n","final_data = pd.concat([heavy_attack, heavy_bengin,light_attack,light_bengin], axis=0, ignore_index=True)\n","\n","# Suppression colonnes redondantes dans les données catégorielles\n","final_data = final_data.loc[:, ~final_data.columns.duplicated()]\n","\n","# Vérifier les dimensions du jeu de données final\n","print(\"final dataset size \\n\")\n","print(final_data.shape)\n"],"metadata":{"id":"i7v7Jn9DyMp6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["################## Les features ont deja été selectionné avec le code de feature selection dataset 3.  ####################\n","################## Mais actuellement il faut des jour pour pour que j'obtiens les resultat vue la taille des données d'OU POUR UN DEBUT J'UTILISE LES CARACTERISTIQUE SELCTIONNEE AVEC LE [GOA-GA] DU PAPIER DE 2024\n","\n","# X_numerical = final_data.select_dtypes(include=['int64', 'float64'])\n","# X_categorical = final_data.select_dtypes(exclude='number').drop('class', axis=1)\n","\n","X_numerical = final_data[['rr','A_frequency','FQDN_count','upper','lower','numeric','entropy','special', 'labels', 'labels_max','labels_average','len']]\n","\n","X_categorical = final_data[['rr_type','unique_ttl','timestamp', 'longest_word', 'sld']]\n","\n","y = final_data['class']\n","\n","print(X_numerical.shape)\n","print(X_categorical.shape)\n","\n","print(y.shape)\n","print(final_data.shape)\n","\n"],"metadata":{"id":"_HyGXC_YvX4x","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1710324417288,"user_tz":0,"elapsed":4661,"user":{"displayName":"Aurelien Kamdem","userId":"15537056127158048299"}},"outputId":"12dcbf3e-5724-4e5f-977a-780c56d58686"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["(1327387, 12)\n","(1327387, 5)\n","(1327387,)\n","(1327387, 43)\n"]}]},{"cell_type":"code","source":["# Exemple de transformation des données numériques en texte\n","def preprocess_numeric_data(numeric_data):\n","    return ' '.join(map(str, numeric_data))\n","\n","# Exemple de transformation des données catégoriques en texte\n","def preprocess_categorical_data(categorical_data):\n","    return ' '.join(categorical_data)"],"metadata":{"id":"6UAVLbdPvYxe"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn.impute import SimpleImputer\n","\n","# Préparer les données numériques et catégoriques pour DistilBERT\n","numeric_data_as_text = X_numerical.apply(preprocess_numeric_data, axis=1)\n","#categorical_data_as_text = X_categorical.apply(preprocess_categorical_data, axis=1) ### comme c'est deja les datas categroiques sa va seulement add les errors\n","\n","# Imputation des valeurs manquantes pour les caractéristiques numériques\n","numerical_imputer = SimpleImputer(strategy='mean')\n","X_numerical_imputed = pd.DataFrame(numerical_imputer.fit_transform(X_numerical), columns=X_numerical.columns)\n","\n","# Imputation des valeurs manquantes pour les caractéristiques catégorielles\n","categorical_imputer = SimpleImputer(strategy='most_frequent')\n","X_categorical_imputed = pd.DataFrame(categorical_imputer.fit_transform(X_categorical), columns=X_categorical.columns)\n","\n","# Fusion des données numériques et catégoriques imputées en une seule représentation textuelle\n","\n","#combined_data_as_text = numeric_data_as_text + ' ' + categorical_data_as_text\n","\n","combined_data_as_text = numeric_data_as_text + ' ' + X_categorical\n"],"metadata":{"id":"w2INIY43lw3t"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from transformers import DistilBertTokenizer, DistilBertModel,DistilBertConfig\n","\n","config = DistilBertConfig(\n","    sinusoidal_pos_embds=True,\n","    num_labels=1\n",")\n","# Branche textuelle - DistilBERT\n","distilbert_model = TFDistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased',config=config)\n","\n","# Tokenisation avec DistilBERTTokenizer\n","tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n","tokenized_inputs = tokenizer(combined_data_as_text.tolist(), padding=True, truncation=True, return_tensors='tf')\n"],"metadata":{"id":"B5LEZZWsvfNJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import matplotlib.pyplot as plt\n","\n","# Obtenir les longueurs de séquence\n","sequence_lengths = [len(token_ids) for token_ids in tokenized_inputs['input_ids'].numpy()]\n","\n","# Afficher l'histogramme des longueurs de séquence\n","plt.hist(sequence_lengths, bins=20)\n","plt.title('Histogramme des longueurs de séquence')\n","plt.xlabel('Longueur de séquence')\n","plt.ylabel('Fréquence')\n","plt.show()"],"metadata":{"id":"mCzkjjHDviAz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# calcul longueur maximale\n","max_sequence_length = max(sequence_lengths)\n","\n","# get l'indice de la plus longue séquence\n","indice_plus_longue_sequence = sequence_lengths.index(max_sequence_length)\n","\n","# Longueur de la plus longue séquence\n","longueur_plus_longue_sequence = sequence_lengths[indice_plus_longue_sequence]\n","\n","print(f\"Longueur maximale du vecteur : {max_sequence_length}\")\n","print(f\"Longueur de la plus longue séquence : {longueur_plus_longue_sequence}\")\n"],"metadata":{"id":"x84AU_USvn72"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Obtenir la longueur maximale\n","max_sequence_length = max(sequence_lengths)\n","\n","# Obtenir l'indice de la plus longue séquence\n","indice_plus_longue_sequence = sequence_lengths.index(max_sequence_length)\n","\n","# Longueur de la plus longue séquence\n","longueur_plus_longue_sequence = sequence_lengths[indice_plus_longue_sequence]\n","\n","print(f\"Longueur maximale du vecteur : {max_sequence_length}\")\n","print(f\"Longueur de la plus longue séquence : {longueur_plus_longue_sequence}\")\n"],"metadata":{"id":"quLkv0_bvozy","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1710324929338,"user_tz":0,"elapsed":361,"user":{"displayName":"Aurelien Kamdem","userId":"15537056127158048299"}},"outputId":"edb8c5ce-5870-4ad5-81b7-29b811d2aae7"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Longueur maximale du vecteur : 62\n","Longueur de la plus longue séquence : 62\n"]}]},{"cell_type":"code","source":["from transformers import TFDistilBertForSequenceClassification,TFDistilBertModel\n","from tensorflow.keras import layers, Model\n","# from tensorflow.keras.optimizers import Adam\n","from tensorflow.keras.optimizers.legacy import Adam\n","\n","\n","\n","# TFDistilBertModel\n","input_ids = layers.Input(shape=(tokenized_inputs['input_ids'].shape[1],), dtype=tf.int32, name=\"input_ids\")\n","attention_mask = layers.Input(shape=(tokenized_inputs['attention_mask'].shape[1],), dtype=tf.int32, name=\"attention_mask\")\n","\n","distilbert_model = TFDistilBertModel.from_pretrained('distilbert-base-uncased')\n","distilbert_output = distilbert_model(input_ids, attention_mask=attention_mask)\n","text_output = distilbert_output.last_hidden_state[:, 0, :]  # Accéder à la sortie des couches cachées\n","\n","# Ajouter des couches supplémentaires au besoin\n","x = layers.Dropout(0.5)(text_output)\n","x = layers.Dense(16, activation='relu')(x)\n","output = layers.Dense(4, activation='softmax')(x)  # Par exemple, 4 classes pour votre cible y\n","\n","# Création et compilation du modèle\n","model = Model(inputs=[input_ids, attention_mask], outputs=output)\n","opt = Adam(learning_rate=0.001)\n","model.compile(optimizer=opt, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n","\n","# Affichage du résumé du modèle pour vérifier l'architecture\n","model.summary()\n","\n","print(model.summary())\n"],"metadata":{"id":"nyFIvxAqvrmR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn.preprocessing import  LabelEncoder\n","\n","# Encodage des étiquettes\n","label_encoder = LabelEncoder()\n","y_encoded = label_encoder.fit_transform(y)\n","\n","print(\"Classes d'origine associées aux classes encodées:\", label_encoder.classes_)\n","print(y_encoded)"],"metadata":{"id":"FZvChuJov5qJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn.model_selection import train_test_split\n","from tensorflow.keras.callbacks import ModelCheckpoint\n","\n","# Diviser les données : entraînement et test\n","X_train, X_test, y_train, y_test = train_test_split(\n","    tokenized_inputs['input_ids'],\n","    y,\n","    test_size=0.2,\n","    random_state=42\n",")\n","\n","# Définir le chemin pour sauvegarder les checkpoints\n","checkpoint_path = '/content/drive/My Drive/yvanolfotso/bestmodel/model_dataset1_1_checkpoint.h5'\n","\n","# Créer le rappel ModelCheckpoint pour sauvegarder le meilleur modèle basé sur la validation accuracy\n","checkpoint_callback = ModelCheckpoint(\n","    filepath=checkpoint_path,\n","    monitor='val_accuracy',\n","    verbose=1,\n","    save_best_only=True,\n","    mode='max',\n","    save_weights_only=False  # Sauvegardez le modèle complet, pas seulement les poids\n",")\n","\n","# Entraîner le modèle\n","history = model.fit(\n","    x={'input_ids': X_train, 'attention_mask': tokenized_inputs['attention_mask']},\n","    y=y_train,\n","    epochs=10,\n","    batch_size=64,\n","    validation_data=({'input_ids': X_test, 'attention_mask': tokenized_inputs['attention_mask']}, y_test),\n","    callbacks=[checkpoint_callback]\n",")\n"],"metadata":{"id":"r0h60bxTmVI0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install keras-tuner\n"],"metadata":{"id":"f5PJ-mtsv_KJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report\n","import numpy as np\n","\n","# Évaluation du modèle sur les données de test\n","test_loss, test_accuracy = model.evaluate(\n","    {'input_ids': X_test, 'attention_mask': tokenized_inputs['attention_mask'][X_test.index]},\n","    y_test,\n","    verbose=1\n",")\n","\n","print(f\"Test Loss: {test_loss}\")\n","print(f\"Test Accuracy: {test_accuracy}\")\n","\n","# Prédictions sur les données de test\n","y_pred_probs = model.predict({'input_ids': X_test, 'attention_mask': tokenized_inputs['attention_mask'][X_test.index]})\n","y_pred = np.argmax(y_pred_probs, axis=1)\n","\n","# Calcul des métriques\n","accuracy = accuracy_score(y_test, y_pred)\n","precision = precision_score(y_test, y_pred, average='weighted')\n","recall = recall_score(y_test, y_pred, average='weighted')\n","f1 = f1_score(y_test, y_pred, average='weighted')\n","\n","print(f\"Accuracy: {accuracy}\")\n","print(f\"Precision: {precision}\")\n","print(f\"Recall: {recall}\")\n","print(f\"F1-score: {f1}\")\n","\n","# Rapport de classification complet\n","report = classification_report(y_test, y_pred)\n","print(report)\n"],"metadata":{"id":"MT2uPxd7mcoS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Affichage des courbes d'apprentissage et de validation\n","plt.figure(figsize=(12, 6))\n","\n","# Plot de la perte d'entraînement et de la perte de validation\n","plt.subplot(1, 2, 1)\n","plt.plot(history.history['loss'], label='Training loss')\n","plt.plot(history.history['val_loss'], label='Validation loss')\n","plt.title('Courbe de Perte')\n","plt.xlabel('Epochs')\n","plt.ylabel('Loss')\n","plt.legend()\n","\n","\n","# Plot de la précision d'entraînement et de la précision de validation\n","plt.subplot(1, 2, 2)\n","plt.plot(history.history['accuracy'], label='Training accuracy')\n","plt.plot(history.history['val_accuracy'], label='Validation accuracy')\n","plt.title('Courbe de Précision')\n","plt.xlabel('Epochs')\n","plt.ylabel('Précision')\n","plt.legend()\n","\n","# Afficher les deux sous-plots\n","plt.tight_layout()\n","plt.show()"],"metadata":{"id":"cVjmqpHOwEbl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn.metrics import accuracy_score\n","\n","# Convertir les indices des classes prédites en étiquettes\n","y_true_classes = np.argmax(y_test, axis=1) if y_test.ndim > 1 else y_test\n","y_pred_classes = np.argmax(y_pred, axis=1) if y_pred.ndim > 1 else y_pred\n","\n","# Calculer l'accuracy pour chaque classe\n","accuracies = []\n","for class_label in range(4):  # Il y a 4 classes numérotées de 0 à 3\n","    y_true_class = (y_true_classes == class_label).astype(int)\n","    y_pred_class = (y_pred_classes == class_label).astype(int)\n","    class_accuracy = accuracy_score(y_true_class, y_pred_class)\n","    accuracies.append(class_accuracy)\n","\n","# Afficher les accuracies pour chaque classe\n","for class_label, accuracy in enumerate(accuracies):\n","    print(f\"Accuracy for class {class_label}: {accuracy}\")\n"],"metadata":{"id":"-S0WIGP8wSwm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Extraction of training metrics\n","training_loss = history.history['loss']\n","training_accuracy = history.history['accuracy']\n","validation_loss = history.history['val_loss']\n","validation_accuracy = history.history['val_accuracy']\n","\n","# Plotting loss and accuracy curves separately\n","epochs = range(1, len(training_loss) + 1)\n","\n","# Plotting training and validation curves\n","plt.figure(figsize=(12, 6))\n","\n","# Plotting training and validation loss\n","plt.subplot(1, 2, 1)\n","plt.plot(epochs, training_loss, label='Training loss')\n","plt.plot(epochs, validation_loss, label='Validation loss')\n","plt.title('Loss Curve')\n","plt.xlabel('Epochs')\n","plt.ylabel('Loss')\n","plt.legend()\n","\n","# Plotting training and validation accuracy\n","plt.subplot(1, 2, 2)\n","plt.plot(epochs, training_accuracy, label='Training accuracy')\n","plt.plot(epochs, validation_accuracy, label='Validation accuracy')\n","plt.title('Accuracy Curve')\n","plt.xlabel('Epochs')\n","plt.ylabel('Accuracy')\n","plt.legend()\n","\n","# Displaying both subplots\n","plt.tight_layout()\n","plt.show()\n"],"metadata":{"id":"h_KIT0-XwVbG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import matplotlib.pyplot as plt\n","import seaborn as sns\n","from sklearn.metrics import confusion_matrix\n","\n","# Matrice de confusion\n","conf_matrix = confusion_matrix(y_test, y_pred_classes)\n","class_names = ['heavy_attacks', 'heavy_benign', 'light_attacks', 'light_benign']\n","\n","df_cm = pd.DataFrame(conf_matrix, index=class_names, columns=class_names)\n","\n","# Afficher la matrice de confusion avec seaborn\n","plt.figure(figsize=(8, 6))\n","sns.heatmap(df_cm, annot=True, fmt='g', cmap='Blues', cbar=False)\n","plt.title('Matrice de Confusion')\n","plt.xlabel('Prédictions')\n","plt.ylabel('Vraies valeurs')\n","plt.show()\n"],"metadata":{"id":"Z6OvY93uwYEO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Vérifier la forme des tableaux labels_test et y_pred_classes\n","print(\"Shape of labels_test:\", y_test.shape)\n","print(\"Shape of y_pred_classes:\", y_pred_classes.shape)\n","\n","from sklearn.preprocessing import LabelBinarizer\n","\n","# Binariser labels_test\n","lb = LabelBinarizer()\n","labels_test_binary = lb.fit_transform(labels_test)\n","\n","\n","from sklearn.metrics import roc_curve, auc\n","import matplotlib.pyplot as plt\n","\n","# Calcul des courbes ROC et AUC pour chaque classe\n","fpr = dict()\n","tpr = dict()\n","roc_auc = dict()\n","for i in range(4):\n","    fpr[i], tpr[i], _ = roc_curve(labels_test_binary[:, i], y_pred[:, i])\n","    roc_auc[i] = auc(fpr[i], tpr[i])\n","\n","# Plotter les courbes ROC pour chaque classe\n","plt.figure(figsize=(8, 6))\n","for i in range(4):\n","    plt.plot(fpr[i], tpr[i], lw=2, label='Class %d (AUC = %0.2f)' % (i, roc_auc[i]))\n","\n","plt.plot([0, 1], [0, 1], color='black', lw=2, linestyle='--')\n","plt.xlabel('Taux de faux positifs (FPR)')\n","plt.ylabel('Taux de vrais positifs (TPR)')\n","plt.title('Courbes ROC pour chaque classe')\n","plt.legend(loc=\"lower right\")\n","plt.show()\n"],"metadata":{"id":"VlgpEv9Wwad2"},"execution_count":null,"outputs":[]}]}