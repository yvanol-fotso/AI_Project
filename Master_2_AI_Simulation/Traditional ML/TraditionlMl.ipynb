{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score, precision_score, recall_score, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "################ Bon Code ###############\n",
    "#### charger et lire un dataset [.zip] sur coolab ####\n",
    "\n",
    "import zipfile\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "import shutil\n",
    "\n",
    "# # Chemins vers les fichiers zip quand je suis sur google Coolab\n",
    "\n",
    "chemin_zip = \"Bell_DNS Dataset.zip\"\n",
    "\n",
    "\n",
    "# Fonction pour extraire les fichiers zip\n",
    "def extraire_zip(chemin_zip):\n",
    "    with zipfile.ZipFile(chemin_zip, 'r') as zip_ref:\n",
    "        zip_ref.extractall(\"extraction_temp\")  # Extraire les fichiers zip dans un répertoire temporaire\n",
    "\n",
    "# Fonction pour charger les fichiers CSV d'un type spécifique (stateful ou stateless)\n",
    "def charger_concatener_donnees(sous_dossier, prefixe):\n",
    "    # Lister tous les fichiers CSV dans le sous-dossier\n",
    "    fichiers_csv = [f for f in os.listdir(f\"extraction_temp/{sous_dossier}\") if f.startswith(prefixe) and f.endswith('.csv')]\n",
    "    # Lire chaque fichier CSV et le stocker dans une liste de DataFrames\n",
    "    dataframes = [pd.read_csv(f\"extraction_temp/{sous_dossier}/{f}\") for f in fichiers_csv]\n",
    "    # Concaténer les DataFrames en un seul\n",
    "    return pd.concat(dataframes, ignore_index=True)\n",
    "\n",
    "# Extraire les fichiers zip\n",
    "extraire_zip(chemin_zip)\n",
    "\n",
    "\n",
    "\n",
    "benin_data = charger_concatener_donnees(\"Bell_DNS Dataset\", \"features_domain_benign\")\n",
    "malware_data = charger_concatener_donnees(\"Bell_DNS Dataset\", \"features-domain_Malware\")\n",
    "\n",
    "phishing_data = charger_concatener_donnees(\"Bell_DNS Dataset\", \"features-domain_phishing\")\n",
    "spam_data = charger_concatener_donnees(\"Bell_DNS Dataset\", \"features-domain_Spam\")\n",
    "\n",
    "# Supprimer le répertoire temporaire après avoir terminé\n",
    "\n",
    "# Vérifier si le répertoire temporaire existe\n",
    "if os.path.exists(\"extraction_temp\"):\n",
    "    # Supprimer le répertoire temporaire et son contenu\n",
    "    shutil.rmtree(\"extraction_temp\")\n",
    "\n",
    "\n",
    "# Maintenant, vous avez vos données prêtes à être utilisées\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Taille de la classe Benin après équilibrage : 23716\n",
      "Taille de la classe Malicioux après équilibrage : 22929\n",
      "\n",
      "\n",
      "Taille de l'ensemble de données final : (46645, 36)\n",
      "Nbre de classe: class\n",
      "Benin        23716\n",
      "Malicioux    22929\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "# Ajout de la colonne 'Class' pour chaque classe\n",
    "spam_data['class'] = 'Malicioux'\n",
    "malware_data['class'] = 'Malicioux'\n",
    "phishing_data['class'] = 'Malicioux'\n",
    "benin_data['class'] = 'Benin'\n",
    "\n",
    "\n",
    "# Concaténation des données\n",
    "all_data = pd.concat([benin_data, spam_data, malware_data, phishing_data])\n",
    "\n",
    "# Diviser les données en caractéristiques (X) et les étiquettes de classe (y)\n",
    "X = all_data.drop(columns=['class'])\n",
    "y = all_data['class']\n",
    "\n",
    "# Instancier le sous-échantillonneur aléatoire\n",
    "rus = RandomUnderSampler(sampling_strategy={'Benin': 23716, 'Malicioux': 22929})\n",
    "\n",
    "# Appliquer le sous-échantillonneur\n",
    "X_resampled, y_resampled = rus.fit_resample(X, y)\n",
    "\n",
    "# Recréer un DataFrame avec les données équilibrées\n",
    "balanced_data = pd.DataFrame(X_resampled, columns=X.columns)\n",
    "balanced_data['class'] = y_resampled\n",
    "\n",
    "# Vérifier la taille des classes après équilibrage\n",
    "print(\"Taille de la classe Benin après équilibrage :\", balanced_data[balanced_data['class'] == 'Benin'].shape[0])\n",
    "print(\"Taille de la classe Malicioux après équilibrage :\", balanced_data[balanced_data['class'] == 'Malicioux'].shape[0])\n",
    "\n",
    "# Taille de l'ensemble de données final\n",
    "print(\"\\n\")\n",
    "print(\"Taille de l'ensemble de données final :\", balanced_data.shape)\n",
    "\n",
    "print(\"Nbre de classe:\", balanced_data['class'].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(46645, 7)\n",
      "(46645, 16)\n",
      "<class 'pandas.core.series.Series'>\n",
      "Index: 46645 entries, 20966 to 596\n",
      "Series name: class\n",
      "Non-Null Count  Dtype \n",
      "--------------  ----- \n",
      "46645 non-null  object\n",
      "dtypes: object(1)\n",
      "memory usage: 728.8+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "X_numerical_balanced = balanced_data[['Page_Rank','entropy','len','numeric_percentage','dec_32','oc_8','oc_32']]\n",
    "X_categorical_balanced = balanced_data[['char_distribution','Registrant_Name','Domain_Name',\n",
    "                        'distance_from_bad_words','Creation_Date_Time','sld','1gram','2gram','3gram',\n",
    "                        'shortened','obfuscate_at_sign','Country','Organization','State','Emails','Domain_Age']]\n",
    "y_balanced = balanced_data['class']\n",
    "\n",
    "\n",
    "\n",
    "print(X_numerical_balanced.shape)\n",
    "print(X_categorical_balanced.shape)\n",
    "print(y_balanced.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "########### CONTINUER L'EXECUTION ICI ###########\n",
    "\n",
    "############ je continue ici ###################\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Imputer les valeurs manquantes pour les caractéristiques numériques\n",
    "\n",
    "# Imputer les valeurs manquantes pour les caractéristiques numériques\n",
    "numerical_imputer = SimpleImputer(strategy='mean')\n",
    "X_numerical_balanced_imputed = pd.DataFrame(numerical_imputer.fit_transform(X_numerical_balanced), columns=X_numerical_balanced.columns)\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Imputer les valeurs manquantes pour les caractéristiques catégorielles\n",
    "\n",
    "categorical_imputer = SimpleImputer(strategy='most_frequent')\n",
    "categorical_balanced_imputed = pd.DataFrame(categorical_imputer.fit_transform(X_categorical_balanced),columns=X_categorical_balanced.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Taille totale des caracteristiques numeriques apres preprocessing : (46645, 7)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import numpy as np\n",
    "\n",
    "#prepocessing des features numeriques soit  avec le  LabelEncoder soit le MinMaxScaler()\n",
    "\n",
    "# Sélection des fonctionnalités numériques\n",
    "\n",
    "# numeric_features = X_numerical_imputed[numeric_imputed_list] ## cas ou je veux selectionné certains features\n",
    "\n",
    "# Création d'un scaler\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# Ajustement du scaler aux données\n",
    "scaler.fit(X_numerical_balanced_imputed)\n",
    "\n",
    "# Transformation des fonctionnalités numériques\n",
    "scaled_numeric_balanced = scaler.transform(X_numerical_balanced_imputed)\n",
    "\n",
    "# Apres transformation Création d' un DataFrame à partir des valeurs transformées\n",
    "\n",
    "scaled_df_balanced = pd.DataFrame(scaled_numeric_balanced, columns=X_numerical_balanced_imputed.columns)\n",
    "\n",
    "\n",
    "total_size_balanced = scaled_df_balanced.shape\n",
    "\n",
    "print(\"Taille totale des caracteristiques numeriques apres preprocessing :\", total_size_balanced)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "char_distribution: [<class 'str'>]\n",
      "Registrant_Name: [<class 'str'>]\n",
      "Domain_Name: [<class 'str'>]\n",
      "distance_from_bad_words: [<class 'str'>]\n",
      "Creation_Date_Time: [<class 'str'>]\n",
      "sld: [<class 'str'>]\n",
      "1gram: [<class 'str'>]\n",
      "2gram: [<class 'str'>]\n",
      "3gram: [<class 'str'>]\n",
      "shortened: [<class 'str'>]\n",
      "obfuscate_at_sign: [<class 'str'>]\n",
      "Country: [<class 'str'>]\n",
      "Organization: [<class 'str'>]\n",
      "State: [<class 'str'>]\n",
      "Emails: [<class 'str'>]\n",
      "Domain_Age: [<class 'str'>]\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 61.1 GiB for an array with shape (46645, 175920) and data type float64",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[28], line 20\u001b[0m\n\u001b[0;32m     17\u001b[0m X_categorical_encoded \u001b[38;5;241m=\u001b[39m encoder\u001b[38;5;241m.\u001b[39mfit_transform(categorical_balanced_imputed)\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# Convertir en DataFrame\u001b[39;00m\n\u001b[1;32m---> 20\u001b[0m X_categorical_encoded_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(\u001b[43mX_categorical_encoded\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtoarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m, columns\u001b[38;5;241m=\u001b[39mencoder\u001b[38;5;241m.\u001b[39mget_feature_names_out(categorical_balanced_imputed\u001b[38;5;241m.\u001b[39mcolumns))\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28mprint\u001b[39m(X_categorical_encoded_df)\n",
      "File \u001b[1;32mc:\\python install\\lib\\site-packages\\scipy\\sparse\\_compressed.py:1051\u001b[0m, in \u001b[0;36m_cs_matrix.toarray\u001b[1;34m(self, order, out)\u001b[0m\n\u001b[0;32m   1049\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m out \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m order \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1050\u001b[0m     order \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_swap(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcf\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m-> 1051\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_process_toarray_args\u001b[49m\u001b[43m(\u001b[49m\u001b[43morder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1052\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (out\u001b[38;5;241m.\u001b[39mflags\u001b[38;5;241m.\u001b[39mc_contiguous \u001b[38;5;129;01mor\u001b[39;00m out\u001b[38;5;241m.\u001b[39mflags\u001b[38;5;241m.\u001b[39mf_contiguous):\n\u001b[0;32m   1053\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mOutput array must be C or F contiguous\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\python install\\lib\\site-packages\\scipy\\sparse\\_base.py:1298\u001b[0m, in \u001b[0;36mspmatrix._process_toarray_args\u001b[1;34m(self, order, out)\u001b[0m\n\u001b[0;32m   1296\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m out\n\u001b[0;32m   1297\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1298\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzeros\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morder\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 61.1 GiB for an array with shape (46645, 175920) and data type float64"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# Convertir toutes les valeurs des colonnes mixtes en chaînes de caractères\n",
    "mixed_columns = ['char_distribution', 'Registrant_Name', 'Domain_Name', 'distance_from_bad_words',\n",
    "                 'Creation_Date_Time', 'sld', '1gram', '2gram', '3gram', 'Country', 'Organization',\n",
    "                 'State', 'Emails', 'Domain_Age', 'shortened', 'obfuscate_at_sign']\n",
    "\n",
    "for col in mixed_columns:\n",
    "    categorical_balanced_imputed[col] = categorical_balanced_imputed[col].astype(str)\n",
    "\n",
    "# Afficher les types de données de chaque colonne pour vérifier\n",
    "for col in categorical_balanced_imputed.columns:\n",
    "    print(f\"{col}: {categorical_balanced_imputed[col].apply(type).unique()}\")\n",
    "\n",
    "# Réencoder les caractéristiques catégoriques\n",
    "encoder = OneHotEncoder()\n",
    "X_categorical_encoded = encoder.fit_transform(categorical_balanced_imputed)\n",
    "\n",
    "# Convertir en DataFrame\n",
    "X_categorical_encoded_df = pd.DataFrame(X_categorical_encoded.toarray(), columns=encoder.get_feature_names_out(categorical_balanced_imputed.columns))\n",
    "\n",
    "print(X_categorical_encoded_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Encoder les étiquettes de classe\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(y_balanced)\n",
    "\n",
    "# Concaténer les données numériques et catégorielles prétraitées\n",
    "X_combined = pd.concat([X_numerical_balanced_imputed, X_categorical_encoded_df], axis=1)\n",
    "\n",
    "# Diviser les données en ensembles d'entraînement et de test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_combined, y_encoded, test_size=0.2, random_state=42)\n",
    "\n",
    "# Définir les hyperparamètres à rechercher\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 150],\n",
    "    'max_depth': [None, 10, 20],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "\n",
    "# Créer un objet GridSearchCV pour la recherche d'hyperparamètres\n",
    "grid_search = GridSearchCV(RandomForestClassifier(random_state=42), param_grid, cv=5, scoring='accuracy')\n",
    "\n",
    "# Effectuer la recherche d'hyperparamètres\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Afficher les meilleurs hyperparamètres trouvés\n",
    "print(\"Meilleurs hyperparamètres :\")\n",
    "print(grid_search.best_params_)\n",
    "\n",
    "# Entraîner le modèle RandomForestClassifier avec les meilleurs hyperparamètres\n",
    "best_rf_classifier = grid_search.best_estimator_\n",
    "best_rf_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Prédire sur l'ensemble de test\n",
    "y_pred = best_rf_classifier.predict(X_test)\n",
    "\n",
    "# Convertir les prédictions décodées\n",
    "y_pred_decoded = label_encoder.inverse_transform(y_pred)\n",
    "\n",
    "# Évaluer le modèle\n",
    "print(\"\\nRapport de classification :\")\n",
    "print(classification_report(y_test, y_pred_decoded))\n",
    "print(\"\\nMatrice de confusion :\")\n",
    "print(confusion_matrix(y_test, y_pred_decoded))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "# Prédire les probabilités des classes positives\n",
    "y_prob = best_rf_classifier.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Calculer les valeurs de FPR (False Positive Rate) et TPR (True Positive Rate)\n",
    "fpr, tpr, _ = roc_curve(y_test, y_prob)\n",
    "\n",
    "# Calculer l'AUC (Area Under the Curve)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "# Tracer la courbe ROC\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(fpr, tpr, color='darkorange', lw=2, label='Courbe ROC (AUC = %0.2f)' % roc_auc)\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('Taux de faux positifs (FPR)')\n",
    "plt.ylabel('Taux de vrais positifs (TPR)')\n",
    "plt.title('Courbe ROC')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()\n",
    "\n",
    "# Matrice de confusion\n",
    "plt.figure(figsize=(8, 6))\n",
    "cm = confusion_matrix(y_test, y_pred_decoded)\n",
    "plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "plt.title('Matrice de confusion')\n",
    "plt.colorbar()\n",
    "plt.xticks([0, 1], ['Classe 0', 'Classe 1'])\n",
    "plt.yticks([0, 1], ['Classe 0', 'Classe 1'])\n",
    "plt.xlabel('Prédictions')\n",
    "plt.ylabel('Vraies valeurs')\n",
    "for i in range(2):\n",
    "    for j in range(2):\n",
    "        plt.text(j, i, str(cm[i, j]), horizontalalignment=\"center\", color=\"white\" if cm[i, j] > (cm.max() / 2) else \"black\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
