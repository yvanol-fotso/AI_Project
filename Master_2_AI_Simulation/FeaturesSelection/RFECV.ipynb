{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "# Chargement des fichiers CSV\n",
    "benin_data = pd.read_csv(\"../data/dataBell_DNS Dataset/features_domain_benign_csv.csv\")\n",
    "malware_data = pd.read_csv(\"../data/Bell_DNS Dataset/features-domain_Malware.csv\")\n",
    "phishing_data = pd.read_csv(\"../data/Bell_DNS Dataset/features-domain_phishing.csv\")\n",
    "spam_data = pd.read_csv(\"../data/dataBell_DNS Dataset/features-domain_Spam.csv\")\n",
    "\n",
    "# Ajout de la colonne 'Class' \n",
    "benin_data['Class'] = 'Benin'\n",
    "malware_data['Class'] = 'Malware'\n",
    "phishing_data['Class'] = 'Phishing'\n",
    "spam_data['Class'] = 'Spam'\n",
    "\n",
    "\n",
    "# Concaténation de tous les ensembles de données\n",
    "all_data = pd.concat([benin_data, malware_data, phishing_data, spam_data], ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spécifier la colonne cible je doit le faire avant l'encodage\n",
    "y = all_data['Class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 ... 3 3 3]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler, LabelEncoder\n",
    "\n",
    "# Encodage de la cible en labels numériques\n",
    "label_encoder = LabelEncoder()\n",
    "# y_encoded = label_encoder.fit_transform(y)\n",
    "y= label_encoder.fit_transform(y)\n",
    "\n",
    "\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing value : {}        subdomain  Alexa_Rank  Page_Rank\n",
      "0            0.0         1.0        NaN\n",
      "1            0.0         1.0        NaN\n",
      "2            1.0         1.0        NaN\n",
      "3            1.0         1.0        NaN\n",
      "4            0.0         7.0        NaN\n",
      "...          ...         ...        ...\n",
      "50154        0.0        -1.0        NaN\n",
      "50155        0.0        -1.0        NaN\n",
      "50156        0.0        -1.0        NaN\n",
      "50157        0.0        -1.0        NaN\n",
      "50158        0.0        -1.0        NaN\n",
      "\n",
      "[50159 rows x 3 columns]\n",
      "Missing value : {}        subdomain  Alexa_Rank  Page_Rank\n",
      "0            0.0         1.0       -1.0\n",
      "1            0.0         1.0       -1.0\n",
      "2            1.0         1.0       -1.0\n",
      "3            1.0         1.0       -1.0\n",
      "4            0.0         7.0       -1.0\n",
      "...          ...         ...        ...\n",
      "50154        0.0        -1.0       -1.0\n",
      "50155        0.0        -1.0       -1.0\n",
      "50156        0.0        -1.0       -1.0\n",
      "50157        0.0        -1.0       -1.0\n",
      "50158        0.0        -1.0       -1.0\n",
      "\n",
      "[50159 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Concaténation de tous les ensembles de données\n",
    "all_data = pd.concat([benin_data, malware_data, phishing_data, spam_data], ignore_index=True)\n",
    "\n",
    "# Convertir les variables catégorielles en variables numériques\n",
    "all_data_encoded = pd.get_dummies(all_data, columns=['Class'], drop_first=True)\n",
    "\n",
    "# Exclure les colonnes non numériques\n",
    "X = all_data_encoded.select_dtypes(include='number')\n",
    "\n",
    "# Vérifier les colonnes avec des valeurs manquantes\n",
    "missing_columns = X.columns[X.isnull().any()]\n",
    "\n",
    "print(\"Missing value : {}\", X[missing_columns])\n",
    "\n",
    "# Imputation des valeurs manquantes\n",
    "if not missing_columns.empty:\n",
    "    imputer = SimpleImputer(strategy='mean')  # Vous pouvez également utiliser 'median' ou 'most_frequent'\n",
    "    X[missing_columns] = imputer.fit_transform(X[missing_columns])\n",
    "\n",
    "\n",
    "\n",
    "#apres remplissage avec la valuer moyenne    \n",
    "print(\"Missing value : {}\", X[missing_columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre total de caractéristiques numeriques (car les autres ont été ignorer): 13\n"
     ]
    }
   ],
   "source": [
    "num_features = X.shape[1]\n",
    "print(\"Nombre total de caractéristiques numeriques (car les autres ont été ignorer):\", num_features)\n",
    "\n",
    "\n",
    "#donc dans notre fonction il faut passer un nombre de caracteristiques inferieur ou egale a 13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor #Comme estimateur pour la sélection des caractéristiques.\n",
    "from sklearn.model_selection import StratifiedKFold #Pour effectuer une validation croisée stratifiée.\n",
    "from sklearn.feature_selection import RFECV #La classe qui effectue la sélection de caractéristiques\n",
    "\n",
    "\n",
    "\n",
    "def rfe_cross_validation(X, y, feature_number):\n",
    "    # Créer une instance de RFECV avec l'estimateur précédemment produit \n",
    "    rfe_cv = RFECV(estimator=DecisionTreeRegressor(), step=1, cv=StratifiedKFold(10),\n",
    "                   verbose=1, min_features_to_select=feature_number, n_jobs=4)\n",
    "\n",
    "    # Ajuster RFECV sur les données\n",
    "    rfe_cv.fit(X, y)\n",
    "\n",
    "    # Transformer X pour ne conserver que les caractéristiques sélectionnées\n",
    "    X_selected = rfe_cv.transform(X)\n",
    "\n",
    "    # Créer un DataFrame avec les noms des caractéristiques et leurs importances\n",
    "    feature_set = pd.DataFrame()\n",
    "    # feature_set['attr'] = X.columns\n",
    "    feature_set['attr'] = X.columns[rfe_cv.support_]  # Utiliser seulement les caractéristiques sélectionnées pour ne pas avoir un probleme de hors page/marge comme la ligne precedente\n",
    "    feature_set['importance'] = rfe_cv.estimator_.feature_importances_\n",
    "\n",
    "    # Trier par ordre décroissant d'importance\n",
    "    feature_set = feature_set.sort_values(by='importance', ascending=False)\n",
    "\n",
    "    # Retourner les noms des caractéristiques les plus importantes\n",
    "    return feature_set['attr'].head(feature_number).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting estimator with 13 features.\n",
      "Fitting estimator with 12 features.\n",
      "Fitting estimator with 11 features.\n",
      "Caractéristiques importantes (RFECV):\n",
      "['Alexa_Rank' 'entropy' 'len' 'numeric_percentage' 'subdomain'\n",
      " 'puny_coded']\n"
     ]
    }
   ],
   "source": [
    "# Appel de la fonction et impression des features importants\n",
    "important_features_rfe = rfe_cross_validation(X, y, feature_number=6)\n",
    "print(\"Caractéristiques importantes (RFECV):\")\n",
    "print(important_features_rfe)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
