{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "# Chargement des fichiers CSV\n",
    "benin_data = pd.read_csv(\"../data/dataBell_DNS Dataset/features_domain_benign_csv.csv\")\n",
    "malware_data = pd.read_csv(\"../data/Bell_DNS Dataset/features-domain_Malware.csv\")\n",
    "phishing_data = pd.read_csv(\"../data/Bell_DNS Dataset/features-domain_phishing.csv\")\n",
    "spam_data = pd.read_csv(\"../data/dataBell_DNS Dataset/features-domain_Spam.csv\")\n",
    "\n",
    "# Ajout de la colonne 'Class' \n",
    "benin_data['Class'] = 'Benin'\n",
    "malware_data['Class'] = 'Malware'\n",
    "phishing_data['Class'] = 'Phishing'\n",
    "spam_data['Class'] = 'Spam'\n",
    "\n",
    "\n",
    "# Concaténation de tous les ensembles de données\n",
    "all_data = pd.concat([benin_data, malware_data, phishing_data, spam_data], ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spécifier la colonne cible je doit le faire avant l'encodage\n",
    "y = all_data['Class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing value : {}        subdomain  Alexa_Rank  Page_Rank\n",
      "0            0.0         1.0        NaN\n",
      "1            0.0         1.0        NaN\n",
      "2            1.0         1.0        NaN\n",
      "3            1.0         1.0        NaN\n",
      "4            0.0         7.0        NaN\n",
      "...          ...         ...        ...\n",
      "50154        0.0        -1.0        NaN\n",
      "50155        0.0        -1.0        NaN\n",
      "50156        0.0        -1.0        NaN\n",
      "50157        0.0        -1.0        NaN\n",
      "50158        0.0        -1.0        NaN\n",
      "\n",
      "[50159 rows x 3 columns]\n",
      "Missing value : {}        subdomain  Alexa_Rank  Page_Rank\n",
      "0            0.0         1.0       -1.0\n",
      "1            0.0         1.0       -1.0\n",
      "2            1.0         1.0       -1.0\n",
      "3            1.0         1.0       -1.0\n",
      "4            0.0         7.0       -1.0\n",
      "...          ...         ...        ...\n",
      "50154        0.0        -1.0       -1.0\n",
      "50155        0.0        -1.0       -1.0\n",
      "50156        0.0        -1.0       -1.0\n",
      "50157        0.0        -1.0       -1.0\n",
      "50158        0.0        -1.0       -1.0\n",
      "\n",
      "[50159 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Concaténation de tous les ensembles de données\n",
    "all_data = pd.concat([benin_data, malware_data, phishing_data, spam_data], ignore_index=True)\n",
    "\n",
    "# Convertir les variables catégorielles en variables numériques\n",
    "all_data_encoded = pd.get_dummies(all_data, columns=['Class'], drop_first=True)\n",
    "\n",
    "# Exclure les colonnes non numériques\n",
    "X = all_data_encoded.select_dtypes(include='number')\n",
    "\n",
    "# Vérifier les colonnes avec des valeurs manquantes\n",
    "missing_columns = X.columns[X.isnull().any()]\n",
    "\n",
    "print(\"Missing value : {}\", X[missing_columns])\n",
    "\n",
    "# Imputation des valeurs manquantes\n",
    "if not missing_columns.empty:\n",
    "    imputer = SimpleImputer(strategy='mean')  # Vous pouvez également utiliser 'median' ou 'most_frequent'\n",
    "    X[missing_columns] = imputer.fit_transform(X[missing_columns])\n",
    "\n",
    "\n",
    "\n",
    "#apres remplissage avec la valuer moyenne    \n",
    "print(\"Missing value : {}\", X[missing_columns])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre total de caractéristiques numeriques (car les autres ont été ignorer): 13\n"
     ]
    }
   ],
   "source": [
    "num_features = X.shape[1]\n",
    "print(\"Nombre total de caractéristiques numeriques (car les autres ont été ignorer):\", num_features)\n",
    "\n",
    "\n",
    "#donc dans notre fonction il faut passer un nombre de caracteristiques inferieur ou egale a 13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "subdomain             2.468767e-01\n",
       "len                   5.041374e+01\n",
       "numeric_percentage    4.878897e+01\n",
       "entropy               3.723222e-01\n",
       "dec_8                 0.000000e+00\n",
       "dec_32                6.559620e-03\n",
       "oc_8                  0.000000e+00\n",
       "oc_32                 7.974164e-05\n",
       "hex_8                 0.000000e+00\n",
       "hex_32                0.000000e+00\n",
       "puny_coded            4.682994e-03\n",
       "Alexa_Rank            3.907441e+12\n",
       "Page_Rank             0.000000e+00\n",
       "dtype: float64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##### j'affiche les variance \n",
    "\n",
    "X.var()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features importants (RFE):\n",
      "Index(['subdomain', 'len', 'numeric_percentage', 'entropy', 'dec_32', 'oc_32',\n",
      "       'hex_32', 'puny_coded', 'Alexa_Rank', 'Page_Rank'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.ensemble import RandomForestClassifier  # Utilisez l'estimateur de votre choix\n",
    "\n",
    "\n",
    "# Utiliser l'estimateur de votre choix (Random Forest ici)\n",
    "estimator = RandomForestClassifier()\n",
    "\n",
    "# Utiliser RFE pour sélectionner les caractéristiques\n",
    "rfe = RFE(estimator, n_features_to_select=10)  # Sélectionner 10 caractéristiques, ajustez selon vos besoins\n",
    "fit = rfe.fit(X, y)\n",
    "\n",
    "# Afficher les caractéristiques sélectionnées\n",
    "print(\"Features importants (RFE):\")\n",
    "print(X.columns[fit.support_])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#liste des eventuelle Estimateur qu'on peut utiliser avec RFE il suffit juste de changer le parametre [estimator par celui qu'on veut esperimenter ou use]\n",
    "\n",
    "# 1)\n",
    "from sklearn.svm import SVC\n",
    "estimator = SVC(kernel=\"linear\")\n",
    "\n",
    "#2)\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "estimator = LogisticRegression()\n",
    "\n",
    "\n",
    "# 3)\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "estimator = KNeighborsClassifier()\n",
    "\n",
    "\n",
    "# 4)\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "estimator = DecisionTreeClassifier()\n",
    "\n",
    "\n",
    "# 5)\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "estimator = GradientBoostingClassifier()\n",
    "\n",
    "\n",
    "# 6)\n",
    "\n",
    "\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "estimator = MLPClassifier()\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
