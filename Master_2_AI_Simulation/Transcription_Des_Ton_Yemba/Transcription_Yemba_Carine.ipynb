{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Hum_BVATvYqL"
      },
      "outputs": [],
      "source": [
        "# Importation des librairies\n",
        "import os\n",
        "import random\n",
        "import torch\n",
        "import soundfile as sf\n",
        "from transformers import Wav2Vec2ForCTC, Wav2Vec2Processor\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from IPython.display import Audio, display"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ThCpt-SsxCns",
        "outputId": "46bc7e78-54d8-4f71-ed41-bf765d922494"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Chemin vers le dossier contenant les fichiers audio et la transcription\n",
        "DATA_DIR = \"/content/drive/MyDrive/datasets/data\"\n",
        "AUDIO_FILES = [os.path.join(DATA_DIR, f) for f in os.listdir(DATA_DIR) if f.endswith('.wav')]\n",
        "TRANSCRIPTION_FILE = os.path.join(DATA_DIR, \"transcription.txt\")"
      ],
      "metadata": {
        "id": "r9UMyy5ivhVT"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Chargement du tokenizer et du modèle pré-entraîné\n",
        "processor = Wav2Vec2Processor.from_pretrained(\"facebook/wav2vec2-large-960h\")\n",
        "model = Wav2Vec2ForCTC.from_pretrained(\"facebook/wav2vec2-large-960h\")"
      ],
      "metadata": {
        "id": "zcYZUktSvkIM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "00468404-d224-470a-f440-1c9af4a82bf1"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "Some weights of the model checkpoint at facebook/wav2vec2-large-960h were not used when initializing Wav2Vec2ForCTC: ['wav2vec2.encoder.pos_conv_embed.conv.weight_g', 'wav2vec2.encoder.pos_conv_embed.conv.weight_v']\n",
            "- This IS expected if you are initializing Wav2Vec2ForCTC from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing Wav2Vec2ForCTC from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at facebook/wav2vec2-large-960h and are newly initialized: ['wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original1', 'wav2vec2.masked_spec_embed']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torchaudio\n",
        "\n",
        "def resample_audio(file_path, target_sample_rate=16000):\n",
        "    audio, sample_rate = torchaudio.load(file_path)\n",
        "    if sample_rate != target_sample_rate:\n",
        "        resampler = torchaudio.transforms.Resample(orig_freq=sample_rate, new_freq=target_sample_rate)\n",
        "        audio = resampler(audio)\n",
        "    return audio.squeeze()  #S'assurer que l'audio est mono pour l'utiliser\n",
        "\n",
        "# Fonction pour lire et rééchantillonner les fichiers audio\n",
        "def read_and_resample_audio(file_path):\n",
        "    audio = resample_audio(file_path)\n",
        "    return audio"
      ],
      "metadata": {
        "id": "bXEKl8AvYtI_"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fonction pour lire les fichiers audio\n",
        "def read_audio(file_path):\n",
        "    audio, sample_rate = sf.read(file_path)\n",
        "    # Convertir en mono si l'audio est stéréo\n",
        "    if audio.ndim > 1:\n",
        "        audio = audio.mean(axis=1)\n",
        "    return audio\n",
        "\n",
        "# Fonction pour charger les transcriptions\n",
        "def load_transcriptions(transcription_file):\n",
        "    with open(transcription_file, \"r\", encoding=\"utf-8\") as f:\n",
        "        transcriptions = f.read().splitlines()\n",
        "    return {line.split()[0]: \" \".join(line.split()[1:]) for line in transcriptions}"
      ],
      "metadata": {
        "id": "DEPZYZ0KvnCj"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Prétraitement des données audio\n",
        "'''\n",
        "def preprocess_audio(audio_files):\n",
        "    audio_data = []\n",
        "    for file_path in audio_files:\n",
        "        data = read_and_resample_audio(file_path)\n",
        "        audio_data.append(data)\n",
        "    return audio_data  # Retourne une liste de tableaux NumPy\n",
        "'''\n",
        "# Prétraitement des données audio\n",
        "def preprocess_audio(audio_files):\n",
        "    audio_data = []\n",
        "    for file_path in audio_files:\n",
        "        # Lire et rééchantillonner l'audio\n",
        "        data = read_and_resample_audio(file_path)\n",
        "        # S'assurer que l'audio est mono en supprimant la première dimension si elle est de taille 2\n",
        "        if data.shape[0] == 2:\n",
        "            data = data.mean(axis=0)  # Convertir en mono si nécessaire\n",
        "        # Ajouter une dimension de canal à la fin si elle n'existe pas\n",
        "        if data.ndim == 1:\n",
        "            data = data.unsqueeze(1)\n",
        "        audio_data.append(data)\n",
        "    return audio_data  # Retourne une liste de tenseurs avec la forme attendue\n",
        "\n",
        "# Prétraitement des transcriptions\n",
        "def preprocess_transcriptions(transcriptions):\n",
        "    return [processor.tokenizer.encode(transcription, add_special_tokens=True) for transcription in transcriptions.values()]"
      ],
      "metadata": {
        "id": "hlpO4JfUvquT"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Charger les transcriptions\n",
        "transcriptions = load_transcriptions(TRANSCRIPTION_FILE)\n",
        "\n",
        "# Prétraiter les données audio et les transcriptions\n",
        "input_values = preprocess_audio(AUDIO_FILES)\n",
        "labels = preprocess_transcriptions(transcriptions)"
      ],
      "metadata": {
        "id": "VQNcZSs8vxcy"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Verification des dimensions des audios apres Prétraitement\n",
        "for i, v in enumerate(input_values):\n",
        "    print(f\"Forme des données audio {i}: {v.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IhsEEMOFJTDG",
        "outputId": "e248dd56-309a-4cf1-a50b-9abead2e159a"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Forme des données audio 0: torch.Size([10550, 1])\n",
            "Forme des données audio 1: torch.Size([11031, 1])\n",
            "Forme des données audio 2: torch.Size([9386, 1])\n",
            "Forme des données audio 3: torch.Size([11031, 1])\n",
            "Forme des données audio 4: torch.Size([12515, 1])\n",
            "Forme des données audio 5: torch.Size([11874, 1])\n",
            "Forme des données audio 6: torch.Size([11793, 1])\n",
            "Forme des données audio 7: torch.Size([9627, 1])\n",
            "Forme des données audio 8: torch.Size([9948, 1])\n",
            "Forme des données audio 9: torch.Size([10269, 1])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "# Recalcul des longueurs des séquences d'entrée avant le padding\n",
        "input_lengths = [len(input) for input in input_values]\n",
        "\n",
        "# Recalcul des longueurs des cibles avant le padding\n",
        "target_lengths = [len(target) for target in labels]\n",
        "'''"
      ],
      "metadata": {
        "id": "AMV2CDS_czJ-",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "9325f5f9-bb4e-4313-b06c-6e9c947aa9ce"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\n# Recalcul des longueurs des séquences d'entrée avant le padding\\ninput_lengths = [len(input) for input in input_values]\\n\\n# Recalcul des longueurs des cibles avant le padding\\ntarget_lengths = [len(target) for target in labels]\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Convertir input_values et labels en tenseurs PyTorch\n",
        "input_values = [torch.tensor(v, dtype=torch.float32) for v in input_values]\n",
        "#input_values = [torch.tensor(dtype=torch.float32, v) for v in input_values]\n",
        "labels = [torch.tensor(l, dtype=torch.int32) for l in labels]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R_hi2OT68-Ep",
        "outputId": "a171b44d-df01-4ee8-8154-89c0a2d394e5"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-11-47278c278ff9>:2: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  input_values = [torch.tensor(v, dtype=torch.float32) for v in input_values]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculez les longueurs des séquences d'entrée et sorties avant le padding\n",
        "input_lengths = [v.size(0) for v in input_values]\n",
        "target_lengths = [len(target) for target in labels]"
      ],
      "metadata": {
        "id": "7xyNlpfrrmPD"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Vérification des longueurs des séquences audio avant le padding\n",
        "print(\"Longueurs des séquences audio avant le padding:\")\n",
        "input_lengths = []\n",
        "for i, v in enumerate(input_values):\n",
        "    print(f\"Audio {i}: Longueur {v.size(0)}\")\n",
        "    input_lengths.append(v.size(0))"
      ],
      "metadata": {
        "id": "_O42TTcWagsN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a85e036c-8eb6-4edd-c91a-1a08f9490636"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Longueurs des séquences audio avant le padding:\n",
            "Audio 0: Longueur 10550\n",
            "Audio 1: Longueur 11031\n",
            "Audio 2: Longueur 9386\n",
            "Audio 3: Longueur 11031\n",
            "Audio 4: Longueur 12515\n",
            "Audio 5: Longueur 11874\n",
            "Audio 6: Longueur 11793\n",
            "Audio 7: Longueur 9627\n",
            "Audio 8: Longueur 9948\n",
            "Audio 9: Longueur 10269\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Vérification des longueurs des étiquettes avant le padding\n",
        "print(\"Longueurs des étiquettes avant le padding:\")\n",
        "target_lengths = []\n",
        "for i, l in enumerate(labels):\n",
        "    print(f\"Étiquette {i}: Longueur {l.size(0)}\")\n",
        "    target_lengths.append(l.size(0))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8OzoaYIXd_Vt",
        "outputId": "b7258a32-57dd-428e-bfdc-64e000997a43"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Longueurs des étiquettes avant le padding:\n",
            "Étiquette 0: Longueur 3\n",
            "Étiquette 1: Longueur 4\n",
            "Étiquette 2: Longueur 4\n",
            "Étiquette 3: Longueur 3\n",
            "Étiquette 4: Longueur 4\n",
            "Étiquette 5: Longueur 6\n",
            "Étiquette 6: Longueur 7\n",
            "Étiquette 7: Longueur 4\n",
            "Étiquette 8: Longueur 5\n",
            "Étiquette 9: Longueur 4\n",
            "Étiquette 10: Longueur 5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# S'assurer que chaque tenseur audio a deux dimensions\n",
        "input_values = [v.unsqueeze(1) if v.dim() == 1 else v for v in input_values]\n",
        "\n",
        "# Vérifier les dimensions des tenseurs\n",
        "for i, v in enumerate(input_values):\n",
        "    print(f\"Audio {i}: Dimensions {v.size()}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "geYWexXhfune",
        "outputId": "1047ba88-76c9-48ee-89bd-68b7881b42b9"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Audio 0: Dimensions torch.Size([10550, 1])\n",
            "Audio 1: Dimensions torch.Size([11031, 1])\n",
            "Audio 2: Dimensions torch.Size([9386, 1])\n",
            "Audio 3: Dimensions torch.Size([11031, 1])\n",
            "Audio 4: Dimensions torch.Size([12515, 1])\n",
            "Audio 5: Dimensions torch.Size([11874, 1])\n",
            "Audio 6: Dimensions torch.Size([11793, 1])\n",
            "Audio 7: Dimensions torch.Size([9627, 1])\n",
            "Audio 8: Dimensions torch.Size([9948, 1])\n",
            "Audio 9: Dimensions torch.Size([10269, 1])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Padding des séquences audio pour qu'elles aient toutes la même longueur\n",
        "input_values_padded = torch.nn.utils.rnn.pad_sequence([v.squeeze(0) for v in input_values], batch_first=True)"
      ],
      "metadata": {
        "id": "DQGJ1bV9BiZl"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Application du padding aux étiquettes\n",
        "labels_padded = torch.nn.utils.rnn.pad_sequence(labels, batch_first=True, padding_value=processor.tokenizer.pad_token_id)"
      ],
      "metadata": {
        "id": "0SIcoHxDbKzA"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Création d'un Dataset personnalisé\n",
        "class AudioData(Dataset):\n",
        "    def __init__(self, inputs, labels):\n",
        "        self.inputs = inputs\n",
        "        self.labels = labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.inputs)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.inputs[idx], self.labels[idx]\n",
        "\n",
        "# Création du DataLoader\n",
        "train_dataset = AudioData(input_values_padded, labels_padded)\n",
        "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)"
      ],
      "metadata": {
        "id": "QQ1NHHcnBnkn"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialisation du processor qui contient le tokenizer\n",
        "processor = Wav2Vec2Processor.from_pretrained(\"facebook/wav2vec2-large-960h\")"
      ],
      "metadata": {
        "id": "jPO3SHroDEJC"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Définition de la fonction de perte et de l'optimiseur\n",
        "loss_fn = torch.nn.CTCLoss(blank=processor.tokenizer.pad_token_id)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=3e-5)"
      ],
      "metadata": {
        "id": "s-y4vKp6BtKW"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Boucle d'entraînement\n",
        "model.train()\n",
        "for epoch in range(10):  # Nombre d'époques\n",
        "    for batch_idx, (inputs, targets) in enumerate(train_loader):\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Suppression de la dimension supplémentaire avant de passer les données au modèle\n",
        "        inputs = inputs.squeeze(-1)\n",
        "\n",
        "        outputs = model(inputs).logits\n",
        "\n",
        "        # Calcul des longueurs des séquences d'entrée pour le lot actuel\n",
        "        current_batch_size = inputs.size(0)\n",
        "        input_lengths_tensor = torch.tensor(input_lengths[batch_idx * current_batch_size : (batch_idx + 1) * current_batch_size], dtype=torch.int64)\n",
        "\n",
        "        # Calcul des longueurs des cibles pour le lot actuel\n",
        "        target_lengths_tensor = torch.tensor(target_lengths[batch_idx * current_batch_size : (batch_idx + 1) * current_batch_size], dtype=torch.int64)\n",
        "\n",
        "        # Aplatir les cibles en un vecteur 1D\n",
        "        targets_flat = torch.cat([t for t in targets]).to(outputs.device)\n",
        "\n",
        "        # Calcul de la perte en utilisant la fonction de perte CTC\n",
        "        loss = loss_fn(outputs.log_softmax(2), targets_flat, input_lengths_tensor, target_lengths_tensor)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        print(f\"Epoch {epoch} Batch {batch_idx} Loss: {loss.item()}\")\n"
      ],
      "metadata": {
        "id": "hJuFBLe6v3g0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 390
        },
        "outputId": "739080ac-5f52-4197-e91b-7566432c22ae"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "input_lengths must be of size batch_size",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-21-a65865ada7a9>\u001b[0m in \u001b[0;36m<cell line: 32>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0;31m# Calcul de la perte en utilisant la fonction de perte CTC\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_softmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets_flat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_lengths_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_lengths_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1532\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1533\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1534\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1539\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1540\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1542\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1543\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/loss.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, log_probs, targets, input_lengths, target_lengths)\u001b[0m\n\u001b[1;32m   1783\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1784\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_probs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_lengths\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_lengths\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1785\u001b[0;31m         return F.ctc_loss(log_probs, targets, input_lengths, target_lengths, self.blank, self.reduction,\n\u001b[0m\u001b[1;32m   1786\u001b[0m                           self.zero_infinity)\n\u001b[1;32m   1787\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mctc_loss\u001b[0;34m(log_probs, targets, input_lengths, target_lengths, blank, reduction, zero_infinity)\u001b[0m\n\u001b[1;32m   2685\u001b[0m             \u001b[0mblank\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mblank\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mzero_infinity\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mzero_infinity\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2686\u001b[0m         )\n\u001b[0;32m-> 2687\u001b[0;31m     return torch.ctc_loss(\n\u001b[0m\u001b[1;32m   2688\u001b[0m         \u001b[0mlog_probs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_lengths\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_lengths\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mblank\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_enum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mzero_infinity\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2689\u001b[0m     )\n",
            "\u001b[0;31mRuntimeError\u001b[0m: input_lengths must be of size batch_size"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Fonction pour jouer un fichier audio au hasard avec sa transcription\n",
        "def play_random_audio(transcriptions, audio_files):\n",
        "    random_index = random.randint(0, len(audio_files) - 1)\n",
        "    random_audio_path = audio_files[random_index]\n",
        "    random_audio_id = os.path.basename(random_audio_path).split('.')[0]\n",
        "    audio_data = read_audio(random_audio_path)\n",
        "    print(f\"Transcription pour {random_audio_id}: {transcriptions[random_audio_id]}\")\n",
        "    display(Audio(data=audio_data, rate=16000))\n",
        "\n",
        "# Jouer un fichier audio au hasard\n",
        "play_random_audio(transcriptions, AUDIO_FILES)"
      ],
      "metadata": {
        "id": "x-Iv2PXCv6Rs"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}